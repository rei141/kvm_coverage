<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.<br>2.<br>3.<br>4.<br>5.<br>6.<br>7.<br>8.<br>9.<br>10.<br>11.<br>12.<br>13.<br>14.<br>15.<br>16.<br>17.<br>18.<br>19.<br>20.<br>21.<br>22.<br>23.<br>24.<br>25.<br>26.<br>27.<br>28.<br>29.<br>30.<br>31.<br>32.<br>33.<br>34.<br>35.<br>36.<br>37.<br>38.<br>39.<br>40.<br>41.<br>42.<br>43.<br>44.<br>45.<br>46.<br>47.<br>48.<br>49.<br>50.<br>51.<br>52.<br>53.<br>54.<br>55.<br>56.<br>57.<br>58.<br>59.<br>60.<br>61.<br>62.<br>63.<br>64.<br>65.<br>66.<br>67.<br>68.<br>69.<br>70.<br>71.<br>72.<br>73.<br>74.<br>75.<br>76.<br>77.<br>78.<br>79.<br>80.<br>81.<br>82.<br>83.<br>84.<br>85.<br>86.<br>87.<br>88.<br>89.<br>90.<br>91.<br>92.<br>93.<br>94.<br>95.<br>96.<br>97.<br>98.<br>99.<br>100.<br>101.<br>102.<br>103.<br>104.<br>105.<br>106.<br>107.<br>108.<br>109.<br>110.<br>111.<br>112.<br>113.<br>114.<br>115.<br>116.<br>117.<br>118.<br>119.<br>120.<br>121.<br>122.<br>123.<br>124.<br>125.<br>126.<br>127.<br>128.<br>129.<br>130.<br>131.<br>132.<br>133.<br>134.<br>135.<br>136.<br>137.<br>138.<br>139.<br>140.<br>141.<br>142.<br>143.<br>144.<br>145.<br>146.<br>147.<br>148.<br>149.<br>150.<br>151.<br>152.<br>153.<br>154.<br>155.<br>156.<br>157.<br>158.<br>159.<br>160.<br>161.<br>162.<br>163.<br>164.<br>165.<br>166.<br>167.<br>168.<br>169.<br>170.<br>171.<br>172.<br>173.<br>174.<br>175.<br>176.<br>177.<br>178.<br>179.<br>180.<br>181.<br>182.<br>183.<br>184.<br>185.<br>186.<br>187.<br>188.<br>189.<br>190.<br>191.<br>192.<br>193.<br>194.<br>195.<br>196.<br>197.<br>198.<br>199.<br>200.<br>201.<br>202.<br>203.<br>204.<br>205.<br>206.<br>207.<br>208.<br>209.<br>210.<br>211.<br>212.<br>213.<br>214.<br>215.<br>216.<br>217.<br>218.<br>219.<br>220.<br>221.<br>222.<br>223.<br>224.<br>225.<br>226.<br>227.<br>228.<br>229.<br>230.<br>231.<br>232.<br>233.<br>234.<br>235.<br>236.<br>237.<br>238.<br>239.<br>240.<br>241.<br>242.<br>243.<br>244.<br>245.<br>246.<br>247.<br>248.<br>249.<br>250.<br>251.<br>252.<br>253.<br>254.<br>255.<br>256.<br>257.<br>258.<br>259.<br>260.<br>261.<br>262.<br>263.<br>264.<br>265.<br>266.<br>267.<br>268.<br>269.<br>270.<br>271.<br>272.<br>273.<br>274.<br>275.<br>276.<br>277.<br>278.<br>279.<br>280.<br>281.<br>282.<br>283.<br>284.<br>285.<br>286.<br>287.<br>288.<br>289.<br>290.<br>291.<br>292.<br>293.<br>294.<br>295.<br>296.<br>297.<br>298.<br>299.<br>300.<br>301.<br>302.<br>303.<br>304.<br>305.<br>306.<br>307.<br>308.<br>309.<br>310.<br>311.<br>312.<br>313.<br>314.<br>315.<br>316.<br>317.<br>318.<br>319.<br>320.<br>321.<br>322.<br>323.<br>324.<br>325.<br>326.<br>327.<br>328.<br>329.<br>330.<br>331.<br>332.<br>333.<br>334.<br>335.<br>336.<br>337.<br>338.<br>339.<br>340.<br>341.<br>342.<br>343.<br>344.<br>345.<br>346.<br>347.<br>348.<br>349.<br>350.<br>351.<br>352.<br>353.<br>354.<br>355.<br>356.<br>357.<br>358.<br>359.<br>360.<br>361.<br>362.<br>363.<br>364.<br>365.<br>366.<br>367.<br>368.<br>369.<br>370.<br>371.<br>372.<br>373.<br>374.<br>375.<br>376.<br>377.<br>378.<br>379.<br>380.<br>381.<br>382.<br>383.<br>384.<br>385.<br>386.<br>387.<br>388.<br>389.<br>390.<br>391.<br>392.<br>393.<br>394.<br>395.<br>396.<br>397.<br>398.<br>399.<br>400.<br>401.<br>402.<br>403.<br>404.<br>405.<br>406.<br>407.<br>408.<br>409.<br>410.<br>411.<br>412.<br>413.<br>414.<br>415.<br>416.<br>417.<br>418.<br>419.<br>420.<br>421.<br>422.<br>423.<br>424.<br>425.<br>426.<br>427.<br>428.<br>429.<br>430.<br>431.<br>432.<br>433.<br>434.<br>435.<br>436.<br>437.<br>438.<br>439.<br>440.<br>441.<br>442.<br>443.<br>444.<br>445.<br>446.<br>447.<br>448.<br>449.<br>450.<br>451.<br>452.<br>453.<br>454.<br>455.<br>456.<br>457.<br>458.<br>459.<br>460.<br>461.<br>462.<br>463.<br>464.<br>465.<br>466.<br>467.<br>468.<br>469.<br>470.<br>471.<br>472.<br>473.<br>474.<br>475.<br>476.<br>477.<br>478.<br>479.<br>480.<br>481.<br>482.<br>483.<br>484.<br>485.<br>486.<br>487.<br>488.<br>489.<br>490.<br>491.<br>492.<br>493.<br>494.<br>495.<br>496.<br>497.<br>498.<br>499.<br>500.<br>501.<br>502.<br>503.<br>504.<br>505.<br>506.<br>507.<br>508.<br>509.<br>510.<br>511.<br>512.<br>513.<br>514.<br>515.<br>516.<br>517.<br>518.<br>519.<br>520.<br>521.<br>522.<br>523.<br>524.<br>525.<br>526.<br>527.<br>528.<br>529.<br>530.<br>531.<br>532.<br>533.<br>534.<br>535.<br>536.<br>537.<br>538.<br>539.<br>540.<br>541.<br>542.<br>543.<br>544.<br>545.<br>546.<br>547.<br>548.<br>549.<br>550.<br>551.<br>552.<br>553.<br>554.<br>555.<br>556.<br>557.<br>558.<br>559.<br>560.<br>561.<br>562.<br>563.<br>564.<br>565.<br>566.<br>567.<br>568.<br>569.<br>570.<br>571.<br>572.<br>573.<br>574.<br>575.<br>576.<br>577.<br>578.<br>579.<br>580.<br>581.<br>582.<br>583.<br>584.<br>585.<br>586.<br>587.<br>588.<br>589.<br>590.<br>591.<br>592.<br>593.<br>594.<br>595.<br>596.<br>597.<br>598.<br>599.<br>600.<br>601.<br>602.<br>603.<br>604.<br>605.<br>606.<br>607.<br>608.<br>609.<br>610.<br>611.<br>612.<br>613.<br>614.<br>615.<br>616.<br>617.<br>618.<br>619.<br>620.<br>621.<br>622.<br>623.<br>624.<br>625.<br>626.<br>627.<br>628.<br>629.<br>630.<br>631.<br>632.<br>633.<br>634.<br>635.<br>636.<br>637.<br>638.<br>639.<br>640.<br>641.<br>642.<br>643.<br>644.<br>645.<br>646.<br>647.<br>648.<br>649.<br>650.<br>651.<br>652.<br>653.<br>654.<br>655.<br>656.<br>657.<br>658.<br>659.<br>660.<br>661.<br>662.<br>663.<br>664.<br>665.<br>666.<br>667.<br>668.<br>669.<br>670.<br>671.<br>672.<br>673.<br>674.<br>675.<br>676.<br>677.<br>678.<br>679.<br>680.<br>681.<br>682.<br>683.<br>684.<br>685.<br>686.<br>687.<br>688.<br>689.<br>690.<br>691.<br>692.<br>693.<br>694.<br>695.<br>696.<br>697.<br>698.<br>699.<br>700.<br>701.<br>702.<br>703.<br>704.<br>705.<br>706.<br>707.<br>708.<br>709.<br>710.<br>711.<br>712.<br>713.<br>714.<br>715.<br>716.<br>717.<br>718.<br>719.<br>720.<br>721.<br>722.<br>723.<br>724.<br>725.<br>726.<br>727.<br>728.<br>729.<br>730.<br>731.<br>732.<br>733.<br>734.<br>735.<br>736.<br>737.<br>738.<br>739.<br>740.<br>741.<br>742.<br>743.<br>744.<br>745.<br>746.<br>747.<br>748.<br>749.<br>750.<br>751.<br>752.<br>753.<br>754.<br>755.<br>756.<br>757.<br>758.<br>759.<br>760.<br>761.<br>762.<br>763.<br>764.<br>765.<br>766.<br>767.<br>768.<br>769.<br>770.<br>771.<br>772.<br>773.<br>774.<br>775.<br>776.<br>777.<br>778.<br>779.<br>780.<br>781.<br>782.<br>783.<br>784.<br>785.<br>786.<br>787.<br>788.<br>789.<br>790.<br>791.<br>792.<br>793.<br>794.<br>795.<br>796.<br>797.<br>798.<br>799.<br>800.<br>801.<br>802.<br>803.<br>804.<br>805.<br>806.<br>807.<br>808.<br>809.<br>810.<br>811.<br>812.<br>813.<br>814.<br>815.<br>816.<br>817.<br>818.<br>819.<br>820.<br>821.<br>822.<br>823.<br>824.<br>825.<br>826.<br>827.<br>828.<br>829.<br>830.<br>831.<br>832.<br>833.<br>834.<br>835.<br>836.<br>837.<br>838.<br>839.<br>840.<br>841.<br>842.<br>843.<br>844.<br>845.<br>846.<br>847.<br>848.<br>849.<br>850.<br>851.<br>852.<br>853.<br>854.<br>855.<br>856.<br>857.<br>858.<br>859.<br>860.<br>861.<br>862.<br>863.<br>864.<br>865.<br>866.<br>867.<br>868.<br>869.<br>870.<br>871.<br>872.<br>873.<br>874.<br>875.<br>876.<br>877.<br>878.<br>879.<br>880.<br>881.<br>882.<br>883.<br>884.<br>885.<br>886.<br>887.<br>888.<br>889.<br>890.<br>891.<br>892.<br>893.<br>894.<br>895.<br>896.<br>897.<br>898.<br>899.<br>900.<br>901.<br>902.<br>903.<br>904.<br>905.<br>906.<br>907.<br>908.<br>909.<br>910.<br>911.<br>912.<br>913.<br>914.<br>915.<br>916.<br>917.<br>918.<br>919.<br>920.<br>921.<br>922.<br>923.<br>924.<br>925.<br>926.<br>927.<br>928.<br>929.<br>930.<br>931.<br>932.<br>933.<br>934.<br>935.<br>936.<br>937.<br>938.<br>939.<br>940.<br>941.<br>942.<br>943.<br>944.<br>945.<br>946.<br>947.<br>948.<br>949.<br>950.<br>951.<br>952.<br>953.<br>954.<br>955.<br>956.<br>957.<br>958.<br>959.<br>960.<br>961.<br>962.<br>963.<br>964.<br>965.<br>966.<br>967.<br>968.<br>969.<br>970.<br>971.<br>972.<br>973.<br>974.<br>975.<br>976.<br>977.<br>978.<br>979.<br>980.<br>981.<br>982.<br>983.<br>984.<br>985.<br>986.<br>987.<br>988.<br>989.<br>990.<br>991.<br>992.<br>993.<br>994.<br>995.<br>996.<br>997.<br>998.<br>999.<br>1000.<br>1001.<br>1002.<br>1003.<br>1004.<br>1005.<br>1006.<br>1007.<br>1008.<br>1009.<br>1010.<br>1011.<br>1012.<br>1013.<br>1014.<br>1015.<br>1016.<br>1017.<br>1018.<br>1019.<br>1020.<br>1021.<br>1022.<br>1023.<br>1024.<br>1025.<br>1026.<br>1027.<br>1028.<br>1029.<br>1030.<br>1031.<br>1032.<br>1033.<br>1034.<br>1035.<br>1036.<br>1037.<br>1038.<br>1039.<br>1040.<br>1041.<br>1042.<br>1043.<br>1044.<br>1045.<br>1046.<br>1047.<br>1048.<br>1049.<br>1050.<br>1051.<br>1052.<br>1053.<br>1054.<br>1055.<br>1056.<br>1057.<br>1058.<br>1059.<br>1060.<br>1061.<br>1062.<br>1063.<br>1064.<br>1065.<br>1066.<br>1067.<br>1068.<br>1069.<br>1070.<br>1071.<br>1072.<br>1073.<br>1074.<br>1075.<br>1076.<br>1077.<br>1078.<br>1079.<br>1080.<br>1081.<br>1082.<br>1083.<br>1084.<br>1085.<br>1086.<br>1087.<br>1088.<br>1089.<br>1090.<br>1091.<br>1092.<br>1093.<br>1094.<br>1095.<br>1096.<br>1097.<br>1098.<br>1099.<br>1100.<br>1101.<br>1102.<br>1103.<br>1104.<br>1105.<br>1106.<br>1107.<br>1108.<br>1109.<br>1110.<br>1111.<br>1112.<br>1113.<br>1114.<br>1115.<br>1116.<br>1117.<br>1118.<br>1119.<br>1120.<br>1121.<br>1122.<br>1123.<br>1124.<br>1125.<br>1126.<br>1127.<br>1128.<br>1129.<br>1130.<br>1131.<br>1132.<br>1133.<br>1134.<br>1135.<br>1136.<br>1137.<br>1138.<br>1139.<br>1140.<br>1141.<br>1142.<br>1143.<br>1144.<br>1145.<br>1146.<br>1147.<br>1148.<br>1149.<br>1150.<br>1151.<br>1152.<br>1153.<br>1154.<br>1155.<br>1156.<br>1157.<br>1158.<br>1159.<br>1160.<br>1161.<br>1162.<br>1163.<br>1164.<br>1165.<br>1166.<br>1167.<br>1168.<br>1169.<br>1170.<br>1171.<br>1172.<br>1173.<br>1174.<br>1175.<br>1176.<br>1177.<br>1178.<br>1179.<br>1180.<br>1181.<br>1182.<br>1183.<br>1184.<br>1185.<br>1186.<br>1187.<br>1188.<br>1189.<br>1190.<br>1191.<br>1192.<br>1193.<br>1194.<br>1195.<br>1196.<br>1197.<br>1198.<br>1199.<br>1200.<br>1201.<br>1202.<br>1203.<br>1204.<br>1205.<br>1206.<br>1207.<br>1208.<br>1209.<br>1210.<br>1211.<br>1212.<br>1213.<br>1214.<br>1215.<br>1216.<br>1217.<br>1218.<br>1219.<br>1220.<br>1221.<br>1222.<br>1223.<br>1224.<br>1225.<br>1226.<br>1227.<br>1228.<br>1229.<br>1230.<br>1231.<br>1232.<br>1233.<br>1234.<br>1235.<br>1236.<br>1237.<br>1238.<br>1239.<br>1240.<br>1241.<br>1242.<br>1243.<br>1244.<br>1245.<br>1246.<br>1247.<br>1248.<br>1249.<br>1250.<br>1251.<br>1252.<br>1253.<br>1254.<br>1255.<br>1256.<br>1257.<br>1258.<br>1259.<br>1260.<br>1261.<br>1262.<br>1263.<br>1264.<br>1265.<br>1266.<br>1267.<br>1268.<br>1269.<br>1270.<br>1271.<br>1272.<br>1273.<br>1274.<br>1275.<br>1276.<br>1277.<br>1278.<br>1279.<br>1280.<br>1281.<br>1282.<br>1283.<br>1284.<br>1285.<br>1286.<br>1287.<br>1288.<br>1289.<br>1290.<br>1291.<br>1292.<br>1293.<br>1294.<br>1295.<br>1296.<br>1297.<br>1298.<br>1299.<br>1300.<br>1301.<br>1302.<br>1303.<br>1304.<br>1305.<br>1306.<br>1307.<br>1308.<br>1309.<br>1310.<br>1311.<br>1312.<br>1313.<br>1314.<br>1315.<br>1316.<br>1317.<br>1318.<br>1319.<br>1320.<br>1321.<br>1322.<br>1323.<br>1324.<br>1325.<br>1326.<br>1327.<br>1328.<br>1329.<br>1330.<br>1331.<br>1332.<br>1333.<br>1334.<br>1335.<br>1336.<br>1337.<br>1338.<br>1339.<br>1340.<br>1341.<br>1342.<br>1343.<br>1344.<br>1345.<br>1346.<br>1347.<br>1348.<br>1349.<br>1350.<br>1351.<br>1352.<br>1353.<br>1354.<br>1355.<br>1356.<br>1357.<br>1358.<br>1359.<br>1360.<br>1361.<br>1362.<br>1363.<br>1364.<br>1365.<br>1366.<br>1367.<br>1368.<br>1369.<br>1370.<br>1371.<br>1372.<br>1373.<br>1374.<br>1375.<br>1376.<br>1377.<br>1378.<br>1379.<br>1380.<br>1381.<br>1382.<br>1383.<br>1384.<br>1385.<br>1386.<br>1387.<br>1388.<br>1389.<br>1390.<br>1391.<br>1392.<br>1393.<br>1394.<br>1395.<br>1396.<br>1397.<br>1398.<br>1399.<br>1400.<br>1401.<br>1402.<br>1403.<br>1404.<br>1405.<br>1406.<br>1407.<br>1408.<br>1409.<br>1410.<br>1411.<br>1412.<br>1413.<br>1414.<br>1415.<br>1416.<br>1417.<br>1418.<br>1419.<br>1420.<br>1421.<br>1422.<br>1423.<br>1424.<br>1425.<br>1426.<br>1427.<br>1428.<br>1429.<br>1430.<br>1431.<br>1432.<br>1433.<br>1434.<br>1435.<br>1436.<br>1437.<br>1438.<br>1439.<br>1440.<br>1441.<br>1442.<br>1443.<br>1444.<br>1445.<br>1446.<br>1447.<br>1448.<br>1449.<br>1450.<br>1451.<br>1452.<br>1453.<br>1454.<br>1455.<br>1456.<br>1457.<br>1458.<br>1459.<br>1460.<br>1461.<br>1462.<br>1463.<br>1464.<br>1465.<br>1466.<br>1467.<br>1468.<br>1469.<br>1470.<br>1471.<br>1472.<br>1473.<br>1474.<br>1475.<br>1476.<br>1477.<br>1478.<br>1479.<br>1480.<br>1481.<br>1482.<br>1483.<br>1484.<br>1485.<br>1486.<br>1487.<br>1488.<br>1489.<br>1490.<br>1491.<br>1492.<br>1493.<br>1494.<br>1495.<br>1496.<br>1497.<br>1498.<br>1499.<br>1500.<br>1501.<br>1502.<br>1503.<br>1504.<br>1505.<br>1506.<br>1507.<br>1508.<br>1509.<br>1510.<br>1511.<br>1512.<br>1513.<br>1514.<br>1515.<br>1516.<br>1517.<br>1518.<br>1519.<br>1520.<br>1521.<br>1522.<br>1523.<br>1524.<br>1525.<br>1526.<br>1527.<br>1528.<br>1529.<br>1530.<br>1531.<br>1532.<br>1533.<br>1534.<br>1535.<br>1536.<br>1537.<br>1538.<br>1539.<br>1540.<br>1541.<br>1542.<br>1543.<br>1544.<br>1545.<br>1546.<br>1547.<br>1548.<br>1549.<br>1550.<br>1551.<br>1552.<br>1553.<br>1554.<br>1555.<br>1556.<br>1557.<br>1558.<br>1559.<br>1560.<br>1561.<br>1562.<br>1563.<br>1564.<br>1565.<br>1566.<br>1567.<br>1568.<br>1569.<br>1570.<br>1571.<br>1572.<br>1573.<br>1574.<br>1575.<br>1576.<br>1577.<br>1578.<br>1579.<br>1580.<br>1581.<br>1582.<br>1583.<br>1584.<br>1585.<br>1586.<br>1587.<br>1588.<br>1589.<br>1590.<br>1591.<br>1592.<br>1593.<br>1594.<br>1595.<br>1596.<br>1597.<br>1598.<br>1599.<br>1600.<br>1601.<br>1602.<br>1603.<br>1604.<br>1605.<br>1606.<br>1607.<br>1608.<br>1609.<br>1610.<br>1611.<br>1612.<br>1613.<br>1614.<br>1615.<br>1616.<br>1617.<br>1618.<br>1619.<br>1620.<br>1621.<br>1622.<br>1623.<br>1624.<br>1625.<br>1626.<br>1627.<br>1628.<br>1629.<br>1630.<br>1631.<br>1632.<br>1633.<br>1634.<br>1635.<br>1636.<br>1637.<br>1638.<br>1639.<br>1640.<br>1641.<br>1642.<br>1643.<br>1644.<br>1645.<br>1646.<br>1647.<br>1648.<br>1649.<br>1650.<br>1651.<br>1652.<br>1653.<br>1654.<br>1655.<br>1656.<br>1657.<br>1658.<br>1659.<br>1660.<br>1661.<br>1662.<br>1663.<br>1664.<br>1665.<br>1666.<br>1667.<br>1668.<br>1669.<br>1670.<br>1671.<br>1672.<br>1673.<br>1674.<br>1675.<br>1676.<br>1677.<br>1678.<br>1679.<br>1680.<br>1681.<br>1682.<br>1683.<br>1684.<br>1685.<br>1686.<br>1687.<br>1688.<br>1689.<br>1690.<br>1691.<br>1692.<br>1693.<br>1694.<br>1695.<br>1696.<br>1697.<br>1698.<br>1699.<br>1700.<br>1701.<br>1702.<br>1703.<br>1704.<br>1705.<br>1706.<br>1707.<br>1708.<br>1709.<br>1710.<br>1711.<br>1712.<br>1713.<br>1714.<br>1715.<br>1716.<br>1717.<br>1718.<br>1719.<br>1720.<br>1721.<br>1722.<br>1723.<br>1724.<br>1725.<br>1726.<br>1727.<br>1728.<br>1729.<br>1730.<br>1731.<br>1732.<br>1733.<br>1734.<br>1735.<br>1736.<br>1737.<br>1738.<br>1739.<br>1740.<br>1741.<br>1742.<br>1743.<br>1744.<br>1745.<br>1746.<br>1747.<br>1748.<br>1749.<br>1750.<br>1751.<br>1752.<br>1753.<br>1754.<br>1755.<br>1756.<br>1757.<br>1758.<br>1759.<br>1760.<br>1761.<br>1762.<br>1763.<br>1764.<br>1765.<br>1766.<br>1767.<br>1768.<br>1769.<br>1770.<br>1771.<br>1772.<br>1773.<br>1774.<br>1775.<br>1776.<br>1777.<br>1778.<br>1779.<br>1780.<br>1781.<br>1782.<br>1783.<br>1784.<br>1785.<br>1786.<br>1787.<br>1788.<br>1789.<br>1790.<br>1791.<br>1792.<br>1793.<br>1794.<br>1795.<br>1796.<br>1797.<br>1798.<br>1799.<br>1800.<br>1801.<br>1802.<br>1803.<br>1804.<br>1805.<br>1806.<br>1807.<br>1808.<br>1809.<br>1810.<br>1811.<br>1812.<br>1813.<br>1814.<br>1815.<br>1816.<br>1817.<br>1818.<br>1819.<br>1820.<br>1821.<br>1822.<br>1823.<br>1824.<br>1825.<br>1826.<br>1827.<br>1828.<br>1829.<br>1830.<br>1831.<br>1832.<br>1833.<br>1834.<br>1835.<br>1836.<br>1837.<br>1838.<br>1839.<br>1840.<br>1841.<br>1842.<br>1843.<br>1844.<br>1845.<br>1846.<br>1847.<br>1848.<br>1849.<br>1850.<br>1851.<br>1852.<br>1853.<br>1854.<br>1855.<br>1856.<br>1857.<br>1858.<br>1859.<br>1860.<br>1861.<br>1862.<br>1863.<br>1864.<br>1865.<br>1866.<br>1867.<br>1868.<br>1869.<br>1870.<br>1871.<br>1872.<br>1873.<br>1874.<br>1875.<br>1876.<br>1877.<br>1878.<br>1879.<br>1880.<br>1881.<br>1882.<br>1883.<br>1884.<br>1885.<br>1886.<br>1887.<br>1888.<br>1889.<br>1890.<br>1891.<br>1892.<br>1893.<br>1894.<br>1895.<br>1896.<br>1897.<br>1898.<br>1899.<br>1900.<br>1901.<br>1902.<br>1903.<br>1904.<br>1905.<br>1906.<br>1907.<br>1908.<br>1909.<br>1910.<br>1911.<br>1912.<br>1913.<br>1914.<br>1915.<br>1916.<br>1917.<br>1918.<br>1919.<br>1920.<br>1921.<br>1922.<br>1923.<br>1924.<br>1925.<br>1926.<br>1927.<br>1928.<br>1929.<br>1930.<br>1931.<br>1932.<br>1933.<br>1934.<br>1935.<br>1936.<br>1937.<br>1938.<br>1939.<br>1940.<br>1941.<br>1942.<br>1943.<br>1944.<br>1945.<br>1946.<br>1947.<br>1948.<br>1949.<br>1950.<br>1951.<br>1952.<br>1953.<br>1954.<br>1955.<br>1956.<br>1957.<br>1958.<br>1959.<br>1960.<br>1961.<br>1962.<br>1963.<br>1964.<br>1965.<br>1966.<br>1967.<br>1968.<br>1969.<br>1970.<br>1971.<br>1972.<br>1973.<br>1974.<br>1975.<br>1976.<br>1977.<br>1978.<br>1979.<br>1980.<br>1981.<br>1982.<br>1983.<br>1984.<br>1985.<br>1986.<br>1987.<br>1988.<br>1989.<br>1990.<br>1991.<br>1992.<br>1993.<br>1994.<br>1995.<br>1996.<br>1997.<br>1998.<br>1999.<br>2000.<br>2001.<br>2002.<br>2003.<br>2004.<br>2005.<br>2006.<br>2007.<br>2008.<br>2009.<br>2010.<br>2011.<br>2012.<br>2013.<br>2014.<br>2015.<br>2016.<br>2017.<br>2018.<br>2019.<br>2020.<br>2021.<br>2022.<br>2023.<br>2024.<br>2025.<br>2026.<br>2027.<br>2028.<br>2029.<br>2030.<br>2031.<br>2032.<br>2033.<br>2034.<br>2035.<br>2036.<br>2037.<br>2038.<br>2039.<br>2040.<br>2041.<br>2042.<br>2043.<br>2044.<br>2045.<br>2046.<br>2047.<br>2048.<br>2049.<br>2050.<br>2051.<br>2052.<br>2053.<br>2054.<br>2055.<br>2056.<br>2057.<br>2058.<br>2059.<br>2060.<br>2061.<br>2062.<br>2063.<br>2064.<br>2065.<br>2066.<br>2067.<br>2068.<br>2069.<br>2070.<br>2071.<br>2072.<br>2073.<br>2074.<br>2075.<br>2076.<br>2077.<br>2078.<br>2079.<br>2080.<br>2081.<br>2082.<br>2083.<br>2084.<br>2085.<br>2086.<br>2087.<br>2088.<br>2089.<br>2090.<br>2091.<br>2092.<br>2093.<br>2094.<br>2095.<br>2096.<br>2097.<br>2098.<br>2099.<br>2100.<br>2101.<br>2102.<br>2103.<br>2104.<br>2105.<br>2106.<br>2107.<br>2108.<br>2109.<br>2110.<br>2111.<br>2112.<br>2113.<br>2114.<br>2115.<br>2116.<br>2117.<br>2118.<br>2119.<br>2120.<br>2121.<br>2122.<br>2123.<br>2124.<br>2125.<br>2126.<br>2127.<br>2128.<br>2129.<br>2130.<br>2131.<br>2132.<br>2133.<br>2134.<br>2135.<br>2136.<br>2137.<br>2138.<br>2139.<br>2140.<br>2141.<br>2142.<br>2143.<br>2144.<br>2145.<br>2146.<br>2147.<br>2148.<br>2149.<br>2150.<br>2151.<br>2152.<br>2153.<br>2154.<br>2155.<br>2156.<br>2157.<br>2158.<br>2159.<br>2160.<br>2161.<br>2162.<br>2163.<br>2164.<br>2165.<br>2166.<br>2167.<br>2168.<br>2169.<br>2170.<br>2171.<br>2172.<br>2173.<br>2174.<br>2175.<br>2176.<br>2177.<br>2178.<br>2179.<br>2180.<br>2181.<br>2182.<br>2183.<br>2184.<br>2185.<br>2186.<br>2187.<br>2188.<br>2189.<br>2190.<br>2191.<br>2192.<br>2193.<br>2194.<br>2195.<br>2196.<br>2197.<br>2198.<br>2199.<br>2200.<br>2201.<br>2202.<br>2203.<br>2204.<br>2205.<br>2206.<br>2207.<br>2208.<br>2209.<br>2210.<br>2211.<br>2212.<br>2213.<br>2214.<br>2215.<br>2216.<br>2217.<br>2218.<br>2219.<br>2220.<br>2221.<br>2222.<br>2223.<br>2224.<br>2225.<br>2226.<br>2227.<br>2228.<br>2229.<br>2230.<br>2231.<br>2232.<br>2233.<br>2234.<br>2235.<br>2236.<br>2237.<br>2238.<br>2239.<br>2240.<br>2241.<br>2242.<br>2243.<br>2244.<br>2245.<br>2246.<br>2247.<br>2248.<br>2249.<br>2250.<br>2251.<br>2252.<br>2253.<br>2254.<br>2255.<br>2256.<br>2257.<br>2258.<br>2259.<br>2260.<br>2261.<br>2262.<br>2263.<br>2264.<br>2265.<br>2266.<br>2267.<br>2268.<br>2269.<br>2270.<br>2271.<br>2272.<br>2273.<br>2274.<br>2275.<br>2276.<br>2277.<br>2278.<br>2279.<br>2280.<br>2281.<br>2282.<br>2283.<br>2284.<br>2285.<br>2286.<br>2287.<br>2288.<br>2289.<br>2290.<br>2291.<br>2292.<br>2293.<br>2294.<br>2295.<br>2296.<br>2297.<br>2298.<br>2299.<br>2300.<br>2301.<br>2302.<br>2303.<br>2304.<br>2305.<br>2306.<br>2307.<br>2308.<br>2309.<br>2310.<br>2311.<br>2312.<br>2313.<br>2314.<br>2315.<br>2316.<br>2317.<br>2318.<br>2319.<br>2320.<br>2321.<br>2322.<br>2323.<br>2324.<br>2325.<br>2326.<br>2327.<br>2328.<br>2329.<br>2330.<br>2331.<br>2332.<br>2333.<br>2334.<br>2335.<br>2336.<br>2337.<br>2338.<br>2339.<br>2340.<br>2341.<br>2342.<br>2343.<br>2344.<br>2345.<br>2346.<br>2347.<br>2348.<br>2349.<br>2350.<br>2351.<br>2352.<br>2353.<br>2354.<br>2355.<br>2356.<br>2357.<br>2358.<br>2359.<br>2360.<br>2361.<br>2362.<br>2363.<br>2364.<br>2365.<br>2366.<br>2367.<br>2368.<br>2369.<br>2370.<br>2371.<br>2372.<br>2373.<br>2374.<br>2375.<br>2376.<br>2377.<br>2378.<br>2379.<br>2380.<br>2381.<br>2382.<br>2383.<br>2384.<br>2385.<br>2386.<br>2387.<br>2388.<br>2389.<br>2390.<br>2391.<br>2392.<br>2393.<br>2394.<br>2395.<br>2396.<br>2397.<br>2398.<br>2399.<br>2400.<br>2401.<br>2402.<br>2403.<br>2404.<br>2405.<br>2406.<br>2407.<br>2408.<br>2409.<br>2410.<br>2411.<br>2412.<br>2413.<br>2414.<br>2415.<br>2416.<br>2417.<br>2418.<br>2419.<br>2420.<br>2421.<br>2422.<br>2423.<br>2424.<br>2425.<br>2426.<br>2427.<br>2428.<br>2429.<br>2430.<br>2431.<br>2432.<br>2433.<br>2434.<br>2435.<br>2436.<br>2437.<br>2438.<br>2439.<br>2440.<br>2441.<br>2442.<br>2443.<br>2444.<br>2445.<br>2446.<br>2447.<br>2448.<br>2449.<br>2450.<br>2451.<br>2452.<br>2453.<br>2454.<br>2455.<br>2456.<br>2457.<br>2458.<br>2459.<br>2460.<br>2461.<br>2462.<br>2463.<br>2464.<br>2465.<br>2466.<br>2467.<br>2468.<br>2469.<br>2470.<br>2471.<br>2472.<br>2473.<br>2474.<br>2475.<br>2476.<br>2477.<br>2478.<br>2479.<br>2480.<br>2481.<br>2482.<br>2483.<br>2484.<br>2485.<br>2486.<br>2487.<br>2488.<br>2489.<br>2490.<br>2491.<br>2492.<br>2493.<br>2494.<br>2495.<br>2496.<br>2497.<br>2498.<br>2499.<br>2500.<br>2501.<br>2502.<br>2503.<br>2504.<br>2505.<br>2506.<br>2507.<br>2508.<br>2509.<br>2510.<br>2511.<br>2512.<br>2513.<br>2514.<br>2515.<br>2516.<br>2517.<br>2518.<br>2519.<br>2520.<br>2521.<br>2522.<br>2523.<br>2524.<br>2525.<br>2526.<br>2527.<br>2528.<br>2529.<br>2530.<br>2531.<br>2532.<br>2533.<br>2534.<br>2535.<br>2536.<br>2537.<br>2538.<br>2539.<br>2540.<br>2541.<br>2542.<br>2543.<br>2544.<br>2545.<br>2546.<br>2547.<br>2548.<br>2549.<br>2550.<br>2551.<br>2552.<br>2553.<br>2554.<br>2555.<br>2556.<br>2557.<br>2558.<br>2559.<br>2560.<br>2561.<br>2562.<br>2563.<br>2564.<br>2565.<br>2566.<br>2567.<br>2568.<br>2569.<br>2570.<br>2571.<br>2572.<br>2573.<br>2574.<br>2575.<br>2576.<br>2577.<br>2578.<br>2579.<br>2580.<br>2581.<br>2582.<br>2583.<br>2584.<br>2585.<br>2586.<br>2587.<br>2588.<br>2589.<br>2590.<br>2591.<br>2592.<br>2593.<br>2594.<br>2595.<br>2596.<br>2597.<br>2598.<br>2599.<br>2600.<br>2601.<br>2602.<br>2603.<br>2604.<br>2605.<br>2606.<br>2607.<br>2608.<br>2609.<br>2610.<br>2611.<br>2612.<br>2613.<br>2614.<br>2615.<br>2616.<br>2617.<br>2618.<br>2619.<br>2620.<br>2621.<br>2622.<br>2623.<br>2624.<br>2625.<br>2626.<br>2627.<br>2628.<br>2629.<br>2630.<br>2631.<br>2632.<br>2633.<br>2634.<br>2635.<br>2636.<br>2637.<br>2638.<br>2639.<br>2640.<br>2641.<br>2642.<br>2643.<br>2644.<br>2645.<br>2646.<br>2647.<br>2648.<br>2649.<br>2650.<br>2651.<br>2652.<br>2653.<br>2654.<br>2655.<br>2656.<br>2657.<br>2658.<br>2659.<br>2660.<br>2661.<br>2662.<br>2663.<br>2664.<br>2665.<br>2666.<br>2667.<br>2668.<br>2669.<br>2670.<br>2671.<br>2672.<br>2673.<br>2674.<br>2675.<br>2676.<br>2677.<br>2678.<br>2679.<br>2680.<br>2681.<br>2682.<br>2683.<br>2684.<br>2685.<br>2686.<br>2687.<br>2688.<br>2689.<br>2690.<br>2691.<br>2692.<br>2693.<br>2694.<br>2695.<br>2696.<br>2697.<br>2698.<br>2699.<br>2700.<br>2701.<br>2702.<br>2703.<br>2704.<br>2705.<br>2706.<br>2707.<br>2708.<br>2709.<br>2710.<br>2711.<br>2712.<br>2713.<br>2714.<br>2715.<br>2716.<br>2717.<br>2718.<br>2719.<br>2720.<br>2721.<br>2722.<br>2723.<br>2724.<br>2725.<br>2726.<br>2727.<br>2728.<br>2729.<br>2730.<br>2731.<br>2732.<br>2733.<br>2734.<br>2735.<br>2736.<br>2737.<br>2738.<br>2739.<br>2740.<br>2741.<br>2742.<br>2743.<br>2744.<br>2745.<br>2746.<br>2747.<br>2748.<br>2749.<br>2750.<br>2751.<br>2752.<br>2753.<br>2754.<br>2755.<br>2756.<br>2757.<br>2758.<br>2759.<br>2760.<br>2761.<br>2762.<br>2763.<br>2764.<br>2765.<br>2766.<br>2767.<br>2768.<br>2769.<br>2770.<br>2771.<br>2772.<br>2773.<br>2774.<br>2775.<br>2776.<br>2777.<br>2778.<br>2779.<br>2780.<br>2781.<br>2782.<br>2783.<br>2784.<br>2785.<br>2786.<br>2787.<br>2788.<br>2789.<br>2790.<br>2791.<br>2792.<br>2793.<br>2794.<br>2795.<br>2796.<br>2797.<br>2798.<br>2799.<br>2800.<br>2801.<br>2802.<br>2803.<br>2804.<br>2805.<br>2806.<br>2807.<br>2808.<br>2809.<br>2810.<br>2811.<br>2812.<br>2813.<br>2814.<br>2815.<br>2816.<br>2817.<br>2818.<br>2819.<br>2820.<br>2821.<br>2822.<br>2823.<br>2824.<br>2825.<br>2826.<br>2827.<br>2828.<br>2829.<br>2830.<br>2831.<br>2832.<br>2833.<br>2834.<br>2835.<br>2836.<br>2837.<br>2838.<br>2839.<br>2840.<br>2841.<br>2842.<br>2843.<br>2844.<br>2845.<br>2846.<br>2847.<br>2848.<br>2849.<br>2850.<br>2851.<br>2852.<br>2853.<br>2854.<br>2855.<br>2856.<br>2857.<br>2858.<br>2859.<br>2860.<br>2861.<br>2862.<br>2863.<br>2864.<br>2865.<br>2866.<br>2867.<br>2868.<br>2869.<br>2870.<br>2871.<br>2872.<br>2873.<br>2874.<br>2875.<br>2876.<br>2877.<br>2878.<br>2879.<br>2880.<br>2881.<br>2882.<br>2883.<br>2884.<br>2885.<br>2886.<br>2887.<br>2888.<br>2889.<br>2890.<br>2891.<br>2892.<br>2893.<br>2894.<br>2895.<br>2896.<br>2897.<br>2898.<br>2899.<br>2900.<br>2901.<br>2902.<br>2903.<br>2904.<br>2905.<br>2906.<br>2907.<br>2908.<br>2909.<br>2910.<br>2911.<br>2912.<br>2913.<br>2914.<br>2915.<br>2916.<br>2917.<br>2918.<br>2919.<br>2920.<br>2921.<br>2922.<br>2923.<br>2924.<br>2925.<br>2926.<br>2927.<br>2928.<br>2929.<br>2930.<br>2931.<br>2932.<br>2933.<br>2934.<br>2935.<br>2936.<br>2937.<br>2938.<br>2939.<br>2940.<br>2941.<br>2942.<br>2943.<br>2944.<br>2945.<br>2946.<br>2947.<br>2948.<br>2949.<br>2950.<br>2951.<br>2952.<br>2953.<br>2954.<br>2955.<br>2956.<br>2957.<br>2958.<br>2959.<br>2960.<br>2961.<br>2962.<br>2963.<br>2964.<br>2965.<br>2966.<br>2967.<br>2968.<br>2969.<br>2970.<br>2971.<br>2972.<br>2973.<br>2974.<br>2975.<br>2976.<br>2977.<br>2978.<br>2979.<br>2980.<br>2981.<br>2982.<br>2983.<br>2984.<br>2985.<br>2986.<br>2987.<br>2988.<br>2989.<br>2990.<br>2991.<br>2992.<br>2993.<br>2994.<br>2995.<br>2996.<br>2997.<br>2998.<br>2999.<br>3000.<br>3001.<br>3002.<br>3003.<br>3004.<br>3005.<br>3006.<br>3007.<br>3008.<br>3009.<br>3010.<br>3011.<br>3012.<br>3013.<br>3014.<br>3015.<br>3016.<br>3017.<br>3018.<br>3019.<br>3020.<br>3021.<br>3022.<br>3023.<br>3024.<br>3025.<br>3026.<br>3027.<br>3028.<br>3029.<br>3030.<br>3031.<br>3032.<br>3033.<br>3034.<br>3035.<br>3036.<br>3037.<br>3038.<br>3039.<br>3040.<br>3041.<br>3042.<br>3043.<br>3044.<br>3045.<br>3046.<br>3047.<br>3048.<br>3049.<br>3050.<br>3051.<br>3052.<br>3053.<br>3054.<br>3055.<br>3056.<br>3057.<br>3058.<br>3059.<br>3060.<br>3061.<br>3062.<br>3063.<br>3064.<br>3065.<br>3066.<br>3067.<br>3068.<br>3069.<br>3070.<br>3071.<br>3072.<br>3073.<br>3074.<br>3075.<br>3076.<br>3077.<br>3078.<br>3079.<br>3080.<br>3081.<br>3082.<br>3083.<br>3084.<br>3085.<br>3086.<br>3087.<br>3088.<br>3089.<br>3090.<br>3091.<br>3092.<br>3093.<br>3094.<br>3095.<br>3096.<br>3097.<br>3098.<br>3099.<br>3100.<br>3101.<br>3102.<br>3103.<br>3104.<br>3105.<br>3106.<br>3107.<br>3108.<br>3109.<br>3110.<br>3111.<br>3112.<br>3113.<br>3114.<br>3115.<br>3116.<br>3117.<br>3118.<br>3119.<br>3120.<br>3121.<br>3122.<br>3123.<br>3124.<br>3125.<br>3126.<br>3127.<br>3128.<br>3129.<br>3130.<br>3131.<br>3132.<br>3133.<br>3134.<br>3135.<br>3136.<br>3137.<br>3138.<br>3139.<br>3140.<br>3141.<br>3142.<br>3143.<br>3144.<br>3145.<br>3146.<br>3147.<br>3148.<br>3149.<br>3150.<br>3151.<br>3152.<br>3153.<br>3154.<br>3155.<br>3156.<br>3157.<br>3158.<br>3159.<br>3160.<br>3161.<br>3162.<br>3163.<br>3164.<br>3165.<br>3166.<br>3167.<br>3168.<br>3169.<br>3170.<br>3171.<br>3172.<br>3173.<br>3174.<br>3175.<br>3176.<br>3177.<br>3178.<br>3179.<br>3180.<br>3181.<br>3182.<br>3183.<br>3184.<br>3185.<br>3186.<br>3187.<br>3188.<br>3189.<br>3190.<br>3191.<br>3192.<br>3193.<br>3194.<br>3195.<br>3196.<br>3197.<br>3198.<br>3199.<br>3200.<br>3201.<br>3202.<br>3203.<br>3204.<br>3205.<br>3206.<br>3207.<br>3208.<br>3209.<br>3210.<br>3211.<br>3212.<br>3213.<br>3214.<br>3215.<br>3216.<br>3217.<br>3218.<br>3219.<br>3220.<br>3221.<br>3222.<br>3223.<br>3224.<br>3225.<br>3226.<br>3227.<br>3228.<br>3229.<br>3230.<br>3231.<br>3232.<br>3233.<br>3234.<br>3235.<br>3236.<br>3237.<br>3238.<br>3239.<br>3240.<br>3241.<br>3242.<br>3243.<br>3244.<br>3245.<br>3246.<br>3247.<br>3248.<br>3249.<br>3250.<br>3251.<br>3252.<br>3253.<br>3254.<br>3255.<br>3256.<br>3257.<br>3258.<br>3259.<br>3260.<br>3261.<br>3262.<br>3263.<br>3264.<br>3265.<br>3266.<br>3267.<br>3268.<br>3269.<br>3270.<br>3271.<br>3272.<br>3273.<br>3274.<br>3275.<br>3276.<br>3277.<br>3278.<br>3279.<br>3280.<br>3281.<br>3282.<br>3283.<br>3284.<br>3285.<br>3286.<br>3287.<br>3288.<br>3289.<br>3290.<br>3291.<br>3292.<br>3293.<br>3294.<br>3295.<br>3296.<br>3297.<br>3298.<br>3299.<br>3300.<br>3301.<br>3302.<br>3303.<br>3304.<br>3305.<br>3306.<br>3307.<br>3308.<br>3309.<br>3310.<br>3311.<br>3312.<br>3313.<br>3314.<br>3315.<br>3316.<br>3317.<br>3318.<br>3319.<br>3320.<br>3321.<br>3322.<br>3323.<br>3324.<br>3325.<br>3326.<br>3327.<br>3328.<br>3329.<br>3330.<br>3331.<br>3332.<br>3333.<br>3334.<br>3335.<br>3336.<br>3337.<br>3338.<br>3339.<br>3340.<br>3341.<br>3342.<br>3343.<br>3344.<br>3345.<br>3346.<br>3347.<br>3348.<br>3349.<br>3350.<br>3351.<br>3352.<br>3353.<br>3354.<br>3355.<br>3356.<br>3357.<br>3358.<br>3359.<br>3360.<br>3361.<br>3362.<br>3363.<br>3364.<br>3365.<br>3366.<br>3367.<br>3368.<br>3369.<br>3370.<br>3371.<br>3372.<br>3373.<br>3374.<br>3375.<br>3376.<br>3377.<br>3378.<br>3379.<br>3380.<br>3381.<br>3382.<br>3383.<br>3384.<br>3385.<br>3386.<br>3387.<br>3388.<br>3389.<br>3390.<br>3391.<br>3392.<br>3393.<br>3394.<br>3395.<br>3396.<br>3397.<br>3398.<br>3399.<br>3400.<br>3401.<br>3402.<br>3403.<br>3404.<br>3405.<br>3406.<br>3407.<br>3408.<br>3409.<br>3410.<br>3411.<br>3412.<br>3413.<br>3414.<br>3415.<br>3416.<br>3417.<br>3418.<br>3419.<br>3420.<br>3421.<br>3422.<br>3423.<br>3424.<br>3425.<br>3426.<br>3427.<br>3428.<br>3429.<br>3430.<br>3431.<br>3432.<br>3433.<br>3434.<br>3435.<br>3436.<br>3437.<br>3438.<br>3439.<br>3440.<br>3441.<br>3442.<br>3443.<br>3444.<br>3445.<br>3446.<br>3447.<br>3448.<br>3449.<br>3450.<br>3451.<br>3452.<br>3453.<br>3454.<br>3455.<br>3456.<br>3457.<br>3458.<br>3459.<br>3460.<br>3461.<br>3462.<br>3463.<br>3464.<br>3465.<br>3466.<br>3467.<br>3468.<br>3469.<br>3470.<br>3471.<br>3472.<br>3473.<br>3474.<br>3475.<br>3476.<br>3477.<br>3478.<br>3479.<br>3480.<br>3481.<br>3482.<br>3483.<br>3484.<br>3485.<br>3486.<br>3487.<br>3488.<br>3489.<br>3490.<br>3491.<br>3492.<br>3493.<br>3494.<br>3495.<br>3496.<br>3497.<br>3498.<br>3499.<br>3500.<br>3501.<br>3502.<br>3503.<br>3504.<br>3505.<br>3506.<br>3507.<br>3508.<br>3509.<br>3510.<br>3511.<br>3512.<br>3513.<br>3514.<br>3515.<br>3516.<br>3517.<br>3518.<br>3519.<br>3520.<br>3521.<br>3522.<br>3523.<br>3524.<br>3525.<br>3526.<br>3527.<br>3528.<br>3529.<br>3530.<br>3531.<br>3532.<br>3533.<br>3534.<br>3535.<br>3536.<br>3537.<br>3538.<br>3539.<br>3540.<br>3541.<br>3542.<br>3543.<br>3544.<br>3545.<br>3546.<br>3547.<br>3548.<br>3549.<br>3550.<br>3551.<br>3552.<br>3553.<br>3554.<br>3555.<br>3556.<br>3557.<br>3558.<br>3559.<br>3560.<br>3561.<br>3562.<br>3563.<br>3564.<br>3565.<br>3566.<br>3567.<br>3568.<br>3569.<br>3570.<br>3571.<br>3572.<br>3573.<br>3574.<br>3575.<br>3576.<br>3577.<br>3578.<br>3579.<br>3580.<br>3581.<br>3582.<br>3583.<br>3584.<br>3585.<br>3586.<br>3587.<br>3588.<br>3589.<br>3590.<br>3591.<br>3592.<br>3593.<br>3594.<br>3595.<br>3596.<br>3597.<br>3598.<br>3599.<br>3600.<br>3601.<br>3602.<br>3603.<br>3604.<br>3605.<br>3606.<br>3607.<br>3608.<br>3609.<br>3610.<br>3611.<br>3612.<br>3613.<br>3614.<br>3615.<br>3616.<br>3617.<br>3618.<br>3619.<br>3620.<br>3621.<br>3622.<br>3623.<br>3624.<br>3625.<br>3626.<br>3627.<br>3628.<br>3629.<br>3630.<br>3631.<br>3632.<br>3633.<br>3634.<br>3635.<br>3636.<br>3637.<br>3638.<br>3639.<br>3640.<br>3641.<br>3642.<br>3643.<br>3644.<br>3645.<br>3646.<br>3647.<br>3648.<br>3649.<br>3650.<br>3651.<br>3652.<br>3653.<br>3654.<br>3655.<br>3656.<br>3657.<br>3658.<br>3659.<br>3660.<br>3661.<br>3662.<br>3663.<br>3664.<br>3665.<br>3666.<br>3667.<br>3668.<br>3669.<br>3670.<br>3671.<br>3672.<br>3673.<br>3674.<br>3675.<br>3676.<br>3677.<br>3678.<br>3679.<br>3680.<br>3681.<br>3682.<br>3683.<br>3684.<br>3685.<br>3686.<br>3687.<br>3688.<br>3689.<br>3690.<br>3691.<br>3692.<br>3693.<br>3694.<br>3695.<br>3696.<br>3697.<br>3698.<br>3699.<br>3700.<br>3701.<br>3702.<br>3703.<br>3704.<br>3705.<br>3706.<br>3707.<br>3708.<br>3709.<br>3710.<br>3711.<br>3712.<br>3713.<br>3714.<br>3715.<br>3716.<br>3717.<br>3718.<br>3719.<br>3720.<br>3721.<br>3722.<br>3723.<br>3724.<br>3725.<br>3726.<br>3727.<br>3728.<br>3729.<br>3730.<br>3731.<br>3732.<br>3733.<br>3734.<br>3735.<br>3736.<br>3737.<br>3738.<br>3739.<br>3740.<br>3741.<br>3742.<br>3743.<br>3744.<br>3745.<br>3746.<br>3747.<br>3748.<br>3749.<br>3750.<br>3751.<br>3752.<br>3753.<br>3754.<br>3755.<br>3756.<br>3757.<br>3758.<br>3759.<br>3760.<br>3761.<br>3762.<br>3763.<br>3764.<br>3765.<br>3766.<br>3767.<br>3768.<br>3769.<br>3770.<br>3771.<br>3772.<br>3773.<br>3774.<br>3775.<br>3776.<br>3777.<br>3778.<br>3779.<br>3780.<br>3781.<br>3782.<br>3783.<br>3784.<br>3785.<br>3786.<br>3787.<br>3788.<br>3789.<br>3790.<br>3791.<br>3792.<br>3793.<br>3794.<br>3795.<br>3796.<br>3797.<br>3798.<br>3799.<br>3800.<br>3801.<br>3802.<br>3803.<br>3804.<br>3805.<br>3806.<br>3807.<br>3808.<br>3809.<br>3810.<br>3811.<br>3812.<br>3813.<br>3814.<br>3815.<br>3816.<br>3817.<br>3818.<br>3819.<br>3820.<br>3821.<br>3822.<br>3823.<br>3824.<br>3825.<br>3826.<br>3827.<br>3828.<br>3829.<br>3830.<br>3831.<br>3832.<br>3833.<br>3834.<br>3835.<br>3836.<br>3837.<br>3838.<br>3839.<br>3840.<br>3841.<br>3842.<br>3843.<br>3844.<br>3845.<br>3846.<br>3847.<br>3848.<br>3849.<br>3850.<br>3851.<br>3852.<br>3853.<br>3854.<br>3855.<br>3856.<br>3857.<br>3858.<br>3859.<br>3860.<br>3861.<br>3862.<br>3863.<br>3864.<br>3865.<br>3866.<br>3867.<br>3868.<br>3869.<br>3870.<br>3871.<br>3872.<br>3873.<br>3874.<br>3875.<br>3876.<br>3877.<br>3878.<br>3879.<br>3880.<br>3881.<br>3882.<br>3883.<br>3884.<br>3885.<br>3886.<br>3887.<br>3888.<br>3889.<br>3890.<br>3891.<br>3892.<br>3893.<br>3894.<br>3895.<br>3896.<br>3897.<br>3898.<br>3899.<br>3900.<br>3901.<br>3902.<br>3903.<br>3904.<br>3905.<br>3906.<br>3907.<br>3908.<br>3909.<br>3910.<br>3911.<br>3912.<br>3913.<br>3914.<br>3915.<br>3916.<br>3917.<br>3918.<br>3919.<br>3920.<br>3921.<br>3922.<br>3923.<br>3924.<br>3925.<br>3926.<br>3927.<br>3928.<br>3929.<br>3930.<br>3931.<br>3932.<br>3933.<br>3934.<br>3935.<br>3936.<br>3937.<br>3938.<br>3939.<br>3940.<br>3941.<br>3942.<br>3943.<br>3944.<br>3945.<br>3946.<br>3947.<br>3948.<br>3949.<br>3950.<br>3951.<br>3952.<br>3953.<br>3954.<br>3955.<br>3956.<br>3957.<br>3958.<br>3959.<br>3960.<br>3961.<br>3962.<br>3963.<br>3964.<br>3965.<br>3966.<br>3967.<br>3968.<br>3969.<br>3970.<br>3971.<br>3972.<br>3973.<br>3974.<br>3975.<br>3976.<br>3977.<br>3978.<br>3979.<br>3980.<br>3981.<br>3982.<br>3983.<br>3984.<br>3985.<br>3986.<br>3987.<br>3988.<br>3989.<br>3990.<br>3991.<br>3992.<br>3993.<br>3994.<br>3995.<br>3996.<br>3997.<br>3998.<br>3999.<br>4000.<br>4001.<br>4002.<br>4003.<br>4004.<br>4005.<br>4006.<br>4007.<br>4008.<br>4009.<br>4010.<br>4011.<br>4012.<br>4013.<br>4014.<br>4015.<br>4016.<br>4017.<br>4018.<br>4019.<br>4020.<br>4021.<br>4022.<br>4023.<br>4024.<br>4025.<br>4026.<br>4027.<br>4028.<br>4029.<br>4030.<br>4031.<br>4032.<br>4033.<br>4034.<br>4035.<br>4036.<br>4037.<br>4038.<br>4039.<br>4040.<br>4041.<br>4042.<br>4043.<br>4044.<br>4045.<br>4046.<br>4047.<br>4048.<br>4049.<br>4050.<br>4051.<br>4052.<br>4053.<br>4054.<br>4055.<br>4056.<br>4057.<br>4058.<br>4059.<br>4060.<br>4061.<br>4062.<br>4063.<br>4064.<br>4065.<br>4066.<br>4067.<br>4068.<br>4069.<br>4070.<br>4071.<br>4072.<br>4073.<br>4074.<br>4075.<br>4076.<br>4077.<br>4078.<br>4079.<br>4080.<br>4081.<br>4082.<br>4083.<br>4084.<br>4085.<br>4086.<br>4087.<br>4088.<br>4089.<br>4090.<br>4091.<br>4092.<br>4093.<br>4094.<br>4095.<br>4096.<br>4097.<br>4098.<br>4099.<br>4100.<br>4101.<br>4102.<br>4103.<br>4104.<br>4105.<br>4106.<br>4107.<br>4108.<br>4109.<br>4110.<br>4111.<br>4112.<br>4113.<br>4114.<br>4115.<br>4116.<br>4117.<br>4118.<br>4119.<br>4120.<br>4121.<br>4122.<br>4123.<br>4124.<br>4125.<br>4126.<br>4127.<br>4128.<br>4129.<br>4130.<br>4131.<br>4132.<br>4133.<br>4134.<br>4135.<br>4136.<br>4137.<br>4138.<br>4139.<br>4140.<br>4141.<br>4142.<br>4143.<br>4144.<br>4145.<br>4146.<br>4147.<br>4148.<br>4149.<br>4150.<br>4151.<br>4152.<br>4153.<br>4154.<br>4155.<br>4156.<br>4157.<br>4158.<br>4159.<br>4160.<br>4161.<br>4162.<br>4163.<br>4164.<br>4165.<br>4166.<br>4167.<br>4168.<br>4169.<br>4170.<br>4171.<br>4172.<br>4173.<br>4174.<br>4175.<br>4176.<br>4177.<br>4178.<br>4179.<br>4180.<br>4181.<br>4182.<br>4183.<br>4184.<br>4185.<br>4186.<br>4187.<br>4188.<br>4189.<br>4190.<br>4191.<br>4192.<br>4193.<br>4194.<br>4195.<br>4196.<br>4197.<br>4198.<br>4199.<br>4200.<br>4201.<br>4202.<br>4203.<br>4204.<br>4205.<br>4206.<br>4207.<br>4208.<br>4209.<br>4210.<br>4211.<br>4212.<br>4213.<br>4214.<br>4215.<br>4216.<br>4217.<br>4218.<br>4219.<br>4220.<br>4221.<br>4222.<br>4223.<br>4224.<br>4225.<br>4226.<br>4227.<br>4228.<br>4229.<br>4230.<br>4231.<br>4232.<br>4233.<br>4234.<br>4235.<br>4236.<br>4237.<br>4238.<br>4239.<br>4240.<br>4241.<br>4242.<br>4243.<br>4244.<br>4245.<br>4246.<br>4247.<br>4248.<br>4249.<br>4250.<br>4251.<br>4252.<br>4253.<br>4254.<br>4255.<br>4256.<br>4257.<br>4258.<br>4259.<br>4260.<br>4261.<br>4262.<br>4263.<br>4264.<br>4265.<br>4266.<br>4267.<br>4268.<br>4269.<br>4270.<br>4271.<br>4272.<br>4273.<br>4274.<br>4275.<br>4276.<br>4277.<br>4278.<br>4279.<br>4280.<br>4281.<br>4282.<br>4283.<br>4284.<br>4285.<br>4286.<br>4287.<br>4288.<br>4289.<br>4290.<br>4291.<br>4292.<br>4293.<br>4294.<br>4295.<br>4296.<br>4297.<br>4298.<br>4299.<br>4300.<br>4301.<br>4302.<br>4303.<br>4304.<br>4305.<br>4306.<br>4307.<br>4308.<br>4309.<br>4310.<br>4311.<br>4312.<br>4313.<br>4314.<br>4315.<br>4316.<br>4317.<br>4318.<br>4319.<br>4320.<br>4321.<br>4322.<br>4323.<br>4324.<br>4325.<br>4326.<br>4327.<br>4328.<br>4329.<br>4330.<br>4331.<br>4332.<br>4333.<br>4334.<br>4335.<br>4336.<br>4337.<br>4338.<br>4339.<br>4340.<br>4341.<br>4342.<br>4343.<br>4344.<br>4345.<br>4346.<br>4347.<br>4348.<br>4349.<br>4350.<br>4351.<br>4352.<br>4353.<br>4354.<br>4355.<br>4356.<br>4357.<br>4358.<br>4359.<br>4360.<br>4361.<br>4362.<br>4363.<br>4364.<br>4365.<br>4366.<br>4367.<br>4368.<br>4369.<br>4370.<br>4371.<br>4372.<br>4373.<br>4374.<br>4375.<br>4376.<br>4377.<br>4378.<br>4379.<br>4380.<br>4381.<br>4382.<br>4383.<br>4384.<br>4385.<br>4386.<br>4387.<br>4388.<br>4389.<br>4390.<br>4391.<br>4392.<br>4393.<br>4394.<br>4395.<br>4396.<br>4397.<br>4398.<br>4399.<br>4400.<br>4401.<br>4402.<br>4403.<br>4404.<br>4405.<br>4406.<br>4407.<br>4408.<br>4409.<br>4410.<br>4411.<br>4412.<br>4413.<br>4414.<br>4415.<br>4416.<br>4417.<br>4418.<br>4419.<br>4420.<br>4421.<br>4422.<br>4423.<br>4424.<br>4425.<br>4426.<br>4427.<br>4428.<br>4429.<br>4430.<br>4431.<br>4432.<br>4433.<br>4434.<br>4435.<br>4436.<br>4437.<br>4438.<br>4439.<br>4440.<br>4441.<br>4442.<br>4443.<br>4444.<br>4445.<br>4446.<br>4447.<br>4448.<br>4449.<br>4450.<br>4451.<br>4452.<br>4453.<br>4454.<br>4455.<br>4456.<br>4457.<br>4458.<br>4459.<br>4460.<br>4461.<br>4462.<br>4463.<br>4464.<br>4465.<br>4466.<br>4467.<br>4468.<br>4469.<br>4470.<br>4471.<br>4472.<br>4473.<br>4474.<br>4475.<br>4476.<br>4477.<br>4478.<br>4479.<br>4480.<br>4481.<br>4482.<br>4483.<br>4484.<br>4485.<br>4486.<br>4487.<br>4488.<br>4489.<br>4490.<br>4491.<br>4492.<br>4493.<br>4494.<br>4495.<br>4496.<br>4497.<br>4498.<br>4499.<br>4500.<br>4501.<br>4502.<br>4503.<br>4504.<br>4505.<br>4506.<br>4507.<br>4508.<br>4509.<br>4510.<br>4511.<br>4512.<br>4513.<br>4514.<br>4515.<br>4516.<br>4517.<br>4518.<br>4519.<br>4520.<br>4521.<br>4522.<br>4523.<br>4524.<br>4525.<br>4526.<br>4527.<br>4528.<br>4529.<br>4530.<br>4531.<br>4532.<br>4533.<br>4534.<br>4535.<br>4536.<br>4537.<br>4538.<br>4539.<br>4540.<br>4541.<br>4542.<br>4543.<br>4544.<br>4545.<br>4546.<br>4547.<br>4548.<br>4549.<br>4550.<br>4551.<br>4552.<br>4553.<br>4554.<br>4555.<br>4556.<br>4557.<br>4558.<br>4559.<br>4560.<br>4561.<br>4562.<br>4563.<br>4564.<br>4565.<br>4566.<br>4567.<br>4568.<br>4569.<br>4570.<br>4571.<br>4572.<br>4573.<br>4574.<br>4575.<br>4576.<br>4577.<br>4578.<br>4579.<br>4580.<br>4581.<br>4582.<br>4583.<br>4584.<br>4585.<br>4586.<br>4587.<br>4588.<br>4589.<br>4590.<br>4591.<br>4592.<br>4593.<br>4594.<br>4595.<br>4596.<br>4597.<br>4598.<br>4599.<br>4600.<br>4601.<br>4602.<br>4603.<br>4604.<br>4605.<br>4606.<br>4607.<br>4608.<br>4609.<br>4610.<br>4611.<br>4612.<br>4613.<br>4614.<br>4615.<br>4616.<br>4617.<br>4618.<br>4619.<br>4620.<br>4621.<br>4622.<br>4623.<br>4624.<br>4625.<br>4626.<br>4627.<br>4628.<br>4629.<br>4630.<br>4631.<br>4632.<br>4633.<br>4634.<br>4635.<br>4636.<br>4637.<br>4638.<br>4639.<br>4640.<br>4641.<br>4642.<br>4643.<br>4644.<br>4645.<br>4646.<br>4647.<br>4648.<br>4649.<br>4650.<br>4651.<br>4652.<br>4653.<br>4654.<br>4655.<br>4656.<br>4657.<br>4658.<br>4659.<br>4660.<br>4661.<br>4662.<br>4663.<br>4664.<br>4665.<br>4666.<br>4667.<br>4668.<br>4669.<br>4670.<br>4671.<br>4672.<br>4673.<br>4674.<br>4675.<br>4676.<br>4677.<br>4678.<br>4679.<br>4680.<br>4681.<br>4682.<br>4683.<br>4684.<br>4685.<br>4686.<br>4687.<br>4688.<br>4689.<br>4690.<br>4691.<br>4692.<br>4693.<br>4694.<br>4695.<br>4696.<br>4697.<br>4698.<br>4699.<br>4700.<br>4701.<br>4702.<br>4703.<br>4704.<br>4705.<br>4706.<br>4707.<br>4708.<br>4709.<br>4710.<br>4711.<br>4712.<br>4713.<br>4714.<br>4715.<br>4716.<br>4717.<br>4718.<br>4719.<br>4720.<br>4721.<br>4722.<br>4723.<br>4724.<br>4725.<br>4726.<br>4727.<br>4728.<br>4729.<br>4730.<br>4731.<br>4732.<br>4733.<br>4734.<br>4735.<br>4736.<br>4737.<br>4738.<br>4739.<br>4740.<br>4741.<br>4742.<br>4743.<br>4744.<br>4745.<br>4746.<br>4747.<br>4748.<br>4749.<br>4750.<br>4751.<br>4752.<br>4753.<br>4754.<br>4755.<br>4756.<br>4757.<br>4758.<br>4759.<br>4760.<br>4761.<br>4762.<br>4763.<br>4764.<br>4765.<br>4766.<br>4767.<br>4768.<br>4769.<br>4770.<br>4771.<br>4772.<br>4773.<br>4774.<br>4775.<br>4776.<br>4777.<br>4778.<br>4779.<br>4780.<br>4781.<br>4782.<br>4783.<br>4784.<br>4785.<br>4786.<br>4787.<br>4788.<br>4789.<br>4790.<br>4791.<br>4792.<br>4793.<br>4794.<br>4795.<br>4796.<br>4797.<br>4798.<br>4799.<br>4800.<br>4801.<br>4802.<br>4803.<br>4804.<br>4805.<br>4806.<br>4807.<br>4808.<br>4809.<br>4810.<br>4811.<br>4812.<br>4813.<br>4814.<br>4815.<br>4816.<br>4817.<br>4818.<br>4819.<br>4820.<br>4821.<br>4822.<br>4823.<br>4824.<br>4825.<br>4826.<br>4827.<br>4828.<br>4829.<br>4830.<br>4831.<br>4832.<br>4833.<br>4834.<br>4835.<br>4836.<br>4837.<br>4838.<br>4839.<br>4840.<br>4841.<br>4842.<br>4843.<br>4844.<br>4845.<br>4846.<br>4847.<br>4848.<br>4849.<br>4850.<br>4851.<br>4852.<br>4853.<br>4854.<br>4855.<br>4856.<br>4857.<br>4858.<br>4859.<br>4860.<br>4861.<br>4862.<br>4863.<br>4864.<br>4865.<br>4866.<br>4867.<br>4868.<br>4869.<br>4870.<br>4871.<br>4872.<br>4873.<br>4874.<br>4875.<br>4876.<br>4877.<br>4878.<br>4879.<br>4880.<br>4881.<br>4882.<br>4883.<br>4884.<br>4885.<br>4886.<br>4887.<br>4888.<br>4889.<br>4890.<br>4891.<br>4892.<br>4893.<br>4894.<br>4895.<br>4896.<br>4897.<br>4898.<br>4899.<br>4900.<br>4901.<br>4902.<br>4903.<br>4904.<br>4905.<br>4906.<br>4907.<br>4908.<br>4909.<br>4910.<br>4911.<br>4912.<br>4913.<br>4914.<br>4915.<br>4916.<br>4917.<br>4918.<br>4919.<br>4920.<br>4921.<br>4922.<br>4923.<br>4924.<br>4925.<br>4926.<br>4927.<br>4928.<br>4929.<br>4930.<br>4931.<br>4932.<br>4933.<br>4934.<br>4935.<br>4936.<br>4937.<br>4938.<br>4939.<br>4940.<br>4941.<br>4942.<br>4943.<br>4944.<br>4945.<br>4946.<br>4947.<br>4948.<br>4949.<br>4950.<br>4951.<br>4952.<br>4953.<br>4954.<br>4955.<br>4956.<br>4957.<br>4958.<br>4959.<br>4960.<br>4961.<br>4962.<br>4963.<br>4964.<br>4965.<br>4966.<br>4967.<br>4968.<br>4969.<br>4970.<br>4971.<br>4972.<br>4973.<br>4974.<br>4975.<br>4976.<br>4977.<br>4978.<br>4979.<br>4980.<br>4981.<br>4982.<br>4983.<br>4984.<br>4985.<br>4986.<br>4987.<br>4988.<br>4989.<br>4990.<br>4991.<br>4992.<br>4993.<br>4994.<br>4995.<br>4996.<br>4997.<br>4998.<br>4999.<br>5000.<br>5001.<br>5002.<br>5003.<br>5004.<br>5005.<br>5006.<br>5007.<br>5008.<br>5009.<br>5010.<br>5011.<br>5012.<br>5013.<br>5014.<br>5015.<br>5016.<br>5017.<br>5018.<br>5019.<br>5020.<br>5021.<br>5022.<br>5023.<br>5024.<br>5025.<br>5026.<br>5027.<br>5028.<br>5029.<br>5030.<br>5031.<br>5032.<br>5033.<br>5034.<br>5035.<br>5036.<br>5037.<br>5038.<br>5039.<br>5040.<br>5041.<br>5042.<br>5043.<br>5044.<br>5045.<br>5046.<br>5047.<br>5048.<br>5049.<br>5050.<br>5051.<br>5052.<br>5053.<br>5054.<br>5055.<br>5056.<br>5057.<br>5058.<br>5059.<br>5060.<br>5061.<br>5062.<br>5063.<br>5064.<br>5065.<br>5066.<br>5067.<br>5068.<br>5069.<br>5070.<br>5071.<br>5072.<br>5073.<br>5074.<br>5075.<br>5076.<br>5077.<br>5078.<br>5079.<br>5080.<br>5081.<br>5082.<br>5083.<br>5084.<br>5085.<br>5086.<br>5087.<br>5088.<br>5089.<br>5090.<br>5091.<br>5092.<br>5093.<br>5094.<br>5095.<br>5096.<br>5097.<br>5098.<br>5099.<br>5100.<br>5101.<br>5102.<br>5103.<br>5104.<br>5105.<br>5106.<br>5107.<br>5108.<br>5109.<br>5110.<br>5111.<br>5112.<br>5113.<br>5114.<br>5115.<br>5116.<br>5117.<br>5118.<br>5119.<br>5120.<br>5121.<br>5122.<br>5123.<br>5124.<br>5125.<br>5126.<br>5127.<br>5128.<br>5129.<br>5130.<br>5131.<br>5132.<br>5133.<br>5134.<br>5135.<br>5136.<br>5137.<br>5138.<br>5139.<br>5140.<br>5141.<br>5142.<br>5143.<br>5144.<br>5145.<br>5146.<br>5147.<br>5148.<br>5149.<br>5150.<br>5151.<br>5152.<br>5153.<br>5154.<br>5155.<br>5156.<br>5157.<br>5158.<br>5159.<br>5160.<br>5161.<br>5162.<br>5163.<br>5164.<br>5165.<br>5166.<br>5167.<br>5168.<br>5169.<br>5170.<br>5171.<br>5172.<br>5173.<br>5174.<br>5175.<br>5176.<br>5177.<br>5178.<br>5179.<br>5180.<br>5181.<br>5182.<br>5183.<br>5184.<br>5185.<br>5186.<br>5187.<br>5188.<br>5189.<br>5190.<br>5191.<br>5192.<br>5193.<br>5194.<br>5195.<br>5196.<br>5197.<br>5198.<br>5199.<br>5200.<br>5201.<br>5202.<br>5203.<br>5204.<br>5205.<br>5206.<br>5207.<br>5208.<br>5209.<br>5210.<br>5211.<br>5212.<br>5213.<br>5214.<br>5215.<br>5216.<br>5217.<br>5218.<br>5219.<br>5220.<br>5221.<br>5222.<br>5223.<br>5224.<br>5225.<br>5226.<br>5227.<br>5228.<br>5229.<br>5230.<br>5231.<br>5232.<br>5233.<br>5234.<br>5235.<br>5236.<br>5237.<br>5238.<br>5239.<br>5240.<br>5241.<br>5242.<br>5243.<br>5244.<br>5245.<br>5246.<br>5247.<br>5248.<br>5249.<br>5250.<br>5251.<br>5252.<br>5253.<br>5254.<br>5255.<br>5256.<br>5257.<br>5258.<br>5259.<br>5260.<br>5261.<br>5262.<br>5263.<br>5264.<br>5265.<br>5266.<br>5267.<br>5268.<br>5269.<br>5270.<br>5271.<br>5272.<br>5273.<br>5274.<br>5275.<br>5276.<br>5277.<br>5278.<br>5279.<br>5280.<br>5281.<br>5282.<br>5283.<br>5284.<br>5285.<br>5286.<br>5287.<br>5288.<br>5289.<br>5290.<br>5291.<br>5292.<br>5293.<br>5294.<br>5295.<br>5296.<br>5297.<br>5298.<br>5299.<br>5300.<br>5301.<br>5302.<br>5303.<br>5304.<br>5305.<br>5306.<br>5307.<br>5308.<br>5309.<br>5310.<br>5311.<br>5312.<br>5313.<br>5314.<br>5315.<br>5316.<br>5317.<br>5318.<br>5319.<br>5320.<br>5321.<br>5322.<br>5323.<br>5324.<br>5325.<br>5326.<br>5327.<br>5328.<br>5329.<br>5330.<br>5331.<br>5332.<br>5333.<br>5334.<br>5335.<br>5336.<br>5337.<br>5338.<br>5339.<br>5340.<br>5341.<br>5342.<br>5343.<br>5344.<br>5345.<br>5346.<br>5347.<br>5348.<br>5349.<br>5350.<br>5351.<br>5352.<br>5353.<br>5354.<br>5355.<br>5356.<br>5357.<br>5358.<br>5359.<br>5360.<br>5361.<br>5362.<br>5363.<br>5364.<br>5365.<br>5366.<br>5367.<br>5368.<br>5369.<br>5370.<br>5371.<br>5372.<br>5373.<br>5374.<br>5375.<br>5376.<br>5377.<br>5378.<br>5379.<br>5380.<br>5381.<br>5382.<br>5383.<br>5384.<br>5385.<br>5386.<br>5387.<br>5388.<br>5389.<br>5390.<br>5391.<br>5392.<br>5393.<br>5394.<br>5395.<br>5396.<br>5397.<br>5398.<br>5399.<br>5400.<br>5401.<br>5402.<br>5403.<br>5404.<br>5405.<br>5406.<br>5407.<br>5408.<br>5409.<br>5410.<br>5411.<br>5412.<br>5413.<br>5414.<br>5415.<br>5416.<br>5417.<br>5418.<br>5419.<br>5420.<br>5421.<br>5422.<br>5423.<br>5424.<br>5425.<br>5426.<br>5427.<br>5428.<br>5429.<br>5430.<br>5431.<br>5432.<br>5433.<br>5434.<br>5435.<br>5436.<br>5437.<br>5438.<br>5439.<br>5440.<br>5441.<br>5442.<br>5443.<br>5444.<br>5445.<br>5446.<br>5447.<br>5448.<br>5449.<br>5450.<br>5451.<br>5452.<br>5453.<br>5454.<br>5455.<br>5456.<br>5457.<br>5458.<br>5459.<br>5460.<br>5461.<br>5462.<br>5463.<br>5464.<br>5465.<br>5466.<br>5467.<br>5468.<br>5469.<br>5470.<br>5471.<br>5472.<br>5473.<br>5474.<br>5475.<br>5476.<br>5477.<br>5478.<br>5479.<br>5480.<br>5481.<br>5482.<br>5483.<br>5484.<br>5485.<br>5486.<br>5487.<br>5488.<br>5489.<br>5490.<br>5491.<br>5492.<br>5493.<br>5494.<br>5495.<br>5496.<br>5497.<br>5498.<br>5499.<br>5500.<br>5501.<br>5502.<br>5503.<br>5504.<br>5505.<br>5506.<br>5507.<br>5508.<br>5509.<br>5510.<br>5511.<br>5512.<br>5513.<br>5514.<br>5515.<br>5516.<br>5517.<br>5518.<br>5519.<br>5520.<br>5521.<br>5522.<br>5523.<br>5524.<br>5525.<br>5526.<br>5527.<br>5528.<br>5529.<br>5530.<br>5531.<br>5532.<br>5533.<br>5534.<br>5535.<br>5536.<br>5537.<br>5538.<br>5539.<br>5540.<br>5541.<br>5542.<br>5543.<br>5544.<br>5545.<br>5546.<br>5547.<br>5548.<br>5549.<br>5550.<br>5551.<br>5552.<br>5553.<br>5554.<br>5555.<br>5556.<br>5557.<br>5558.<br>5559.<br>5560.<br>5561.<br>5562.<br>5563.<br>5564.<br>5565.<br>5566.<br>5567.<br>5568.<br>5569.<br>5570.<br>5571.<br>5572.<br>5573.<br>5574.<br>5575.<br>5576.<br>5577.<br>5578.<br>5579.<br>5580.<br>5581.<br>5582.<br>5583.<br>5584.<br>5585.<br>5586.<br>5587.<br>5588.<br>5589.<br>5590.<br>5591.<br>5592.<br>5593.<br>5594.<br>5595.<br>5596.<br>5597.<br>5598.<br>5599.<br>5600.<br>5601.<br>5602.<br>5603.<br>5604.<br>5605.<br>5606.<br>5607.<br>5608.<br>5609.<br>5610.<br>5611.<br>5612.<br>5613.<br>5614.<br>5615.<br>5616.<br>5617.<br>5618.<br>5619.<br>5620.<br>5621.<br>5622.<br>5623.<br>5624.<br>5625.<br>5626.<br>5627.<br>5628.<br>5629.<br>5630.<br>5631.<br>5632.<br>5633.<br>5634.<br>5635.<br>5636.<br>5637.<br>5638.<br>5639.<br>5640.<br>5641.<br>5642.<br>5643.<br>5644.<br>5645.<br>5646.<br>5647.<br>5648.<br>5649.<br>5650.<br>5651.<br>5652.<br>5653.<br>5654.<br>5655.<br>5656.<br>5657.<br>5658.<br>5659.<br>5660.<br>5661.<br>5662.<br>5663.<br>5664.<br>5665.<br>5666.<br>5667.<br>5668.<br>5669.<br>5670.<br>5671.<br>5672.<br>5673.<br>5674.<br>5675.<br>5676.<br>5677.<br>5678.<br>5679.<br>5680.<br>5681.<br>5682.<br>5683.<br>5684.<br>5685.<br>5686.<br>5687.<br>5688.<br>5689.<br>5690.<br>5691.<br>5692.<br>5693.<br>5694.<br>5695.<br>5696.<br>5697.<br>5698.<br>5699.<br>5700.<br>5701.<br>5702.<br>5703.<br>5704.<br>5705.<br>5706.<br>5707.<br>5708.<br>5709.<br>5710.<br>5711.<br>5712.<br>5713.<br>5714.<br>5715.<br>5716.<br>5717.<br>5718.<br>5719.<br>5720.<br>5721.<br>5722.<br>5723.<br>5724.<br>5725.<br>5726.<br>5727.<br>5728.<br>5729.<br>5730.<br>5731.<br>5732.<br>5733.<br>5734.<br>5735.<br>5736.<br>5737.<br>5738.<br>5739.<br>5740.<br>5741.<br>5742.<br>5743.<br>5744.<br>5745.<br>5746.<br>5747.<br>5748.<br>5749.<br>5750.<br>5751.<br>5752.<br>5753.<br>5754.<br>5755.<br>5756.<br>5757.<br>5758.<br>5759.<br>5760.<br>5761.<br>5762.<br>5763.<br>5764.<br>5765.<br>5766.<br>5767.<br>5768.<br>5769.<br>5770.<br>5771.<br>5772.<br>5773.<br>5774.<br>5775.<br>5776.<br>5777.<br>5778.<br>5779.<br>5780.<br>5781.<br>5782.<br>5783.<br>5784.<br>5785.<br>5786.<br>5787.<br>5788.<br>5789.<br>5790.<br>5791.<br>5792.<br>5793.<br>5794.<br>5795.<br>5796.<br>5797.<br>5798.<br>5799.<br>5800.<br>5801.<br>5802.<br>5803.<br>5804.<br>5805.<br>5806.<br>5807.<br>5808.<br>5809.<br>5810.<br>5811.<br>5812.<br>5813.<br>5814.<br>5815.<br>5816.<br>5817.<br>5818.<br>5819.<br>5820.<br>5821.<br>5822.<br>5823.<br>5824.<br>5825.<br>5826.<br>5827.<br>5828.<br>5829.<br>5830.<br>5831.<br>5832.<br>5833.<br>5834.<br>5835.<br>5836.<br>5837.<br>5838.<br>5839.<br>5840.<br>5841.<br>5842.<br>5843.<br>5844.<br>5845.<br>5846.<br>5847.<br>5848.<br>5849.<br>5850.<br>5851.<br>5852.<br>5853.<br>5854.<br>5855.<br>5856.<br>5857.<br>5858.<br>5859.<br>5860.<br>5861.<br>5862.<br>5863.<br>5864.<br>5865.<br>5866.<br>5867.<br>5868.<br>5869.<br>5870.<br>5871.<br>5872.<br>5873.<br>5874.<br>5875.<br>5876.<br>5877.<br>5878.<br>5879.<br>5880.<br>5881.<br>5882.<br>5883.<br>5884.<br>5885.<br>5886.<br>5887.<br>5888.<br>5889.<br>5890.<br>5891.<br>5892.<br>5893.<br>5894.<br>5895.<br>5896.<br>5897.<br>5898.<br>5899.<br>5900.<br>5901.<br>5902.<br>5903.<br>5904.<br>5905.<br>5906.<br>5907.<br>5908.<br>5909.<br>5910.<br>5911.<br>5912.<br>5913.<br>5914.<br>5915.<br>5916.<br>5917.<br>5918.<br>5919.<br>5920.<br>5921.<br>5922.<br>5923.<br>5924.<br>5925.<br>5926.<br>5927.<br>5928.<br>5929.<br>5930.<br>5931.<br>5932.<br>5933.<br>5934.<br>5935.<br>5936.<br>5937.<br>5938.<br>5939.<br>5940.<br>5941.<br>5942.<br>5943.<br>5944.<br>5945.<br>5946.<br>5947.<br>5948.<br>5949.<br>5950.<br>5951.<br>5952.<br>5953.<br>5954.<br>5955.<br>5956.<br>5957.<br>5958.<br>5959.<br>5960.<br>5961.<br>5962.<br>5963.<br>5964.<br>5965.<br>5966.<br>5967.<br>5968.<br>5969.<br>5970.<br>5971.<br>5972.<br>5973.<br>5974.<br>5975.<br>5976.<br>5977.<br>5978.<br>5979.<br>5980.<br>5981.<br>5982.<br>5983.<br>5984.<br>5985.<br>5986.<br>5987.<br>5988.<br>5989.<br>5990.<br>5991.<br>5992.<br>5993.<br>5994.<br>5995.<br>5996.<br>5997.<br>5998.<br>5999.<br>6000.<br>6001.<br>6002.<br>6003.<br>6004.<br>6005.<br>6006.<br>6007.<br>6008.<br>6009.<br>6010.<br>6011.<br>6012.<br>6013.<br>6014.<br>6015.<br>6016.<br>6017.<br>6018.<br>6019.<br>6020.<br>6021.<br>6022.<br>6023.<br>6024.<br>6025.<br>6026.<br>6027.<br>6028.<br>6029.<br>6030.<br>6031.<br>6032.<br>6033.<br>6034.<br>6035.<br>6036.<br>6037.<br>6038.<br>6039.<br>6040.<br>6041.<br>6042.<br>6043.<br>6044.<br>6045.<br>6046.<br>6047.<br>6048.<br>6049.<br>6050.<br>6051.<br>6052.<br>6053.<br>6054.<br>6055.<br>6056.<br>6057.<br>6058.<br>6059.<br>6060.<br>6061.<br>6062.<br>6063.<br>6064.<br>6065.<br>6066.<br>6067.<br>6068.<br>6069.<br>6070.<br>6071.<br>6072.<br>6073.<br>6074.<br>6075.<br>6076.<br>6077.<br>6078.<br>6079.<br>6080.<br>6081.<br>6082.<br>6083.<br>6084.<br>6085.<br>6086.<br>6087.<br>6088.<br>6089.<br>6090.<br>6091.<br>6092.<br>6093.<br>6094.<br>6095.<br>6096.<br>6097.<br>6098.<br>6099.<br>6100.<br>6101.<br>6102.<br>6103.<br>6104.<br>6105.<br>6106.<br>6107.<br>6108.<br>6109.<br>6110.<br>6111.<br>6112.<br>6113.<br>6114.<br>6115.<br>6116.<br>6117.<br>6118.<br>6119.<br>6120.<br>6121.<br>6122.<br>6123.<br>6124.<br>6125.<br>6126.<br>6127.<br>6128.<br>6129.<br>6130.<br>6131.<br>6132.<br>6133.<br>6134.<br>6135.<br>6136.<br>6137.<br>6138.<br>6139.<br>6140.<br>6141.<br>6142.<br>6143.<br>6144.<br>6145.<br>6146.<br>6147.<br>6148.<br>6149.<br>6150.<br>6151.<br>6152.<br>6153.<br>6154.<br>6155.<br>6156.<br>6157.<br>6158.<br>6159.<br>6160.<br>6161.<br>6162.<br>6163.<br>6164.<br>6165.<br>6166.<br>6167.<br>6168.<br>6169.<br>6170.<br>6171.<br>6172.<br>6173.<br>6174.<br>6175.<br>6176.<br>6177.<br>6178.<br>6179.<br>6180.<br>6181.<br>6182.<br>6183.<br>6184.<br>6185.<br>6186.<br>6187.<br>6188.<br>6189.<br>6190.<br>6191.<br>6192.<br>6193.<br>6194.<br>6195.<br>6196.<br>6197.<br>6198.<br>6199.<br>6200.<br>6201.<br>6202.<br>6203.<br>6204.<br>6205.<br>6206.<br>6207.<br>6208.<br>6209.<br>6210.<br>6211.<br>6212.<br>6213.<br>6214.<br>6215.<br>6216.<br>6217.<br>6218.<br>6219.<br>6220.<br>6221.<br>6222.<br>6223.<br>6224.<br>6225.<br>6226.<br>6227.<br>6228.<br>6229.<br>6230.<br>6231.<br>6232.<br>6233.<br>6234.<br>6235.<br>6236.<br>6237.<br>6238.<br>6239.<br>6240.<br>6241.<br>6242.<br>6243.<br>6244.<br>6245.<br>6246.<br>6247.<br>6248.<br>6249.<br>6250.<br>6251.<br>6252.<br>6253.<br>6254.<br>6255.<br>6256.<br>6257.<br>6258.<br>6259.<br>6260.<br>6261.<br>6262.<br>6263.<br>6264.<br>6265.<br>6266.<br>6267.<br>6268.<br>6269.<br>6270.<br>6271.<br>6272.<br>6273.<br>6274.<br>6275.<br>6276.<br>6277.<br>6278.<br>6279.<br>6280.<br>6281.<br>6282.<br>6283.<br>6284.<br>6285.<br>6286.<br>6287.<br>6288.<br>6289.<br>6290.<br>6291.<br>6292.<br>6293.<br>6294.<br>6295.<br>6296.<br>6297.<br>6298.<br>6299.<br>6300.<br>6301.<br>6302.<br>6303.<br>6304.<br>6305.<br>6306.<br>6307.<br>6308.<br>6309.<br>6310.<br>6311.<br>6312.<br>6313.<br>6314.<br>6315.<br>6316.<br>6317.<br>6318.<br>6319.<br>6320.<br>6321.<br>6322.<br>6323.<br>6324.<br>6325.<br>6326.<br>6327.<br>6328.<br>6329.<br>6330.<br>6331.<br>6332.<br>6333.<br>6334.<br>6335.<br>6336.<br>6337.<br>6338.<br>6339.<br>6340.<br>6341.<br>6342.<br>6343.<br>6344.<br>6345.<br>6346.<br>6347.<br>6348.<br>6349.<br>6350.<br>6351.<br>6352.<br>6353.<br>6354.<br>6355.<br>6356.<br>6357.<br>6358.<br>6359.<br>6360.<br>6361.<br>6362.<br>6363.<br>6364.<br>6365.<br>6366.<br>6367.<br>6368.<br>6369.<br>6370.<br>6371.<br>6372.<br>6373.<br>6374.<br>6375.<br>6376.<br>6377.<br>6378.<br>6379.<br>6380.<br>6381.<br>6382.<br>6383.<br>6384.<br>6385.<br>6386.<br>6387.<br>6388.<br>6389.<br>6390.<br>6391.<br>6392.<br>6393.<br>6394.<br>6395.<br>6396.<br>6397.<br>6398.<br>6399.<br>6400.<br>6401.<br>6402.<br>6403.<br>6404.<br>6405.<br>6406.<br>6407.<br>6408.<br>6409.<br>6410.<br>6411.<br>6412.<br>6413.<br>6414.<br>6415.<br>6416.<br>6417.<br>6418.<br>6419.<br>6420.<br>6421.<br>6422.<br>6423.<br>6424.<br>6425.<br>6426.<br>6427.<br>6428.<br>6429.<br>6430.<br>6431.<br>6432.<br>6433.<br>6434.<br>6435.<br>6436.<br>6437.<br>6438.<br>6439.<br>6440.<br>6441.<br>6442.<br>6443.<br>6444.<br>6445.<br>6446.<br>6447.<br>6448.<br>6449.<br>6450.<br>6451.<br>6452.<br>6453.<br>6454.<br>6455.<br>6456.<br>6457.<br>6458.<br>6459.<br>6460.<br>6461.<br>6462.<br>6463.<br>6464.<br>6465.<br>6466.<br>6467.<br>6468.<br>6469.<br>6470.<br>6471.<br>6472.<br>6473.<br>6474.<br>6475.<br>6476.<br>6477.<br>6478.<br>6479.<br>6480.<br>6481.<br>6482.<br>6483.<br>6484.<br>6485.<br>6486.<br>6487.<br>6488.<br>6489.<br>6490.<br>6491.<br>6492.<br>6493.<br>6494.<br>6495.<br>6496.<br>6497.<br>6498.<br>6499.<br>6500.<br>6501.<br>6502.<br>6503.<br>6504.<br>6505.<br>6506.<br>6507.<br>6508.<br>6509.<br>6510.<br>6511.<br>6512.<br>6513.<br>6514.<br>6515.<br>6516.<br>6517.<br>6518.<br>6519.<br>6520.<br>6521.<br>6522.<br>6523.<br>6524.<br>6525.<br>6526.<br>6527.<br>6528.<br>6529.<br>6530.<br>6531.<br>6532.<br>6533.<br>6534.<br>6535.<br>6536.<br>6537.<br>6538.<br>6539.<br>6540.<br>6541.<br>6542.<br>6543.<br>6544.<br>6545.<br>6546.<br>6547.<br>6548.<br>6549.<br>6550.<br>6551.<br>6552.<br>6553.<br>6554.<br>6555.<br>6556.<br>6557.<br>6558.<br>6559.<br>6560.<br>6561.<br>6562.<br>6563.<br>6564.<br>6565.<br>6566.<br>6567.<br>6568.<br>6569.<br>6570.<br>6571.<br>6572.<br>6573.<br>6574.<br>6575.<br>6576.<br>6577.<br>6578.<br>6579.<br>6580.<br>6581.<br>6582.<br>6583.<br>6584.<br>6585.<br>6586.<br>6587.<br>6588.<br>6589.<br>6590.<br>6591.<br>6592.<br>6593.<br>6594.<br>6595.<br>6596.<br>6597.<br>6598.<br>6599.<br>6600.<br>6601.<br>6602.<br>6603.<br>6604.<br>6605.<br>6606.<br>6607.<br>6608.<br>6609.<br>6610.<br>6611.<br>6612.<br>6613.<br>6614.<br>6615.<br>6616.<br>6617.<br>6618.<br>6619.<br>6620.<br>6621.<br>6622.<br>6623.<br>6624.<br>6625.<br>6626.<br>6627.<br>6628.<br>6629.<br>6630.<br>6631.<br>6632.<br>6633.<br>6634.<br>6635.<br>6636.<br>6637.<br>6638.<br>6639.<br>6640.<br>6641.<br>6642.<br>6643.<br>6644.<br>6645.<br>6646.<br>6647.<br>6648.<br>6649.<br>6650.<br>6651.<br>6652.<br>6653.<br>6654.<br>6655.<br>6656.<br>6657.<br>6658.<br>6659.<br>6660.<br>6661.<br>6662.<br>6663.<br>6664.<br>6665.<br>6666.<br>6667.<br>6668.<br>6669.<br>6670.<br>6671.<br>6672.<br>6673.<br>6674.<br>6675.<br>6676.<br>6677.<br>6678.<br>6679.<br>6680.<br>6681.<br>6682.<br>6683.<br>6684.<br>6685.<br>6686.<br>6687.<br>6688.<br>6689.<br>6690.<br>6691.<br>6692.<br>6693.<br>6694.<br>6695.<br>6696.<br>6697.<br>6698.<br>6699.<br>6700.<br>6701.<br>6702.<br>6703.<br>6704.<br>6705.<br>6706.<br>6707.<br>6708.<br>6709.<br>6710.<br>6711.<br>6712.<br>6713.<br>6714.<br>6715.<br>6716.<br>6717.<br>6718.<br>6719.<br>6720.<br>6721.<br>6722.<br>6723.<br>6724.<br>6725.<br>6726.<br>6727.<br>6728.<br>6729.<br>6730.<br>6731.<br>6732.<br>6733.<br>6734.<br>6735.<br>6736.<br>6737.<br>6738.<br>6739.<br>6740.<br>6741.<br>6742.<br>6743.<br>6744.<br>6745.<br>6746.<br>6747.<br>6748.<br>6749.<br>6750.<br>6751.<br>6752.<br>6753.<br>6754.<br>6755.<br>6756.<br>6757.<br>6758.<br>6759.<br>6760.<br>6761.<br>6762.<br>6763.<br>6764.<br>6765.<br>6766.<br>6767.<br>6768.<br>6769.<br>6770.<br>6771.<br>6772.<br>6773.<br>6774.<br>6775.<br>6776.<br>6777.<br>6778.<br>6779.<br>6780.<br>6781.<br>6782.<br>6783.<br>6784.<br>6785.<br>6786.<br>6787.<br>6788.<br>6789.<br>6790.<br>6791.<br>6792.<br>6793.<br>6794.<br>6795.<br>6796.<br>6797.<br>6798.<br>6799.<br>6800.<br>6801.<br>6802.<br>6803.<br>6804.<br>6805.<br>6806.<br>6807.<br>6808.<br>6809.<br>6810.<br>6811.<br>6812.<br>6813.<br>6814.<br>6815.<br>6816.<br>6817.<br>6818.<br>6819.<br>6820.<br>6821.<br>6822.<br>6823.<br>6824.<br>6825.<br>6826.<br>6827.<br>6828.<br>6829.<br>6830.<br>6831.<br>6832.<br>6833.<br>6834.<br>6835.<br>6836.<br>6837.<br>6838.<br>6839.<br>6840.<br>6841.<br>6842.<br>6843.<br>6844.<br>6845.<br>6846.<br>6847.<br>6848.<br>6849.<br>6850.<br>6851.<br>6852.<br>6853.<br>6854.<br>6855.<br>6856.<br>6857.<br>6858.<br>6859.<br>6860.<br>6861.<br>6862.<br>6863.<br>6864.<br>6865.<br>6866.<br>6867.<br>6868.<br>6869.<br>6870.<br>6871.<br>6872.<br>6873.<br>6874.<br>6875.<br>6876.<br>6877.<br>6878.<br>6879.<br>6880.<br>6881.<br>6882.<br>6883.<br>6884.<br>6885.<br>6886.<br>6887.<br>6888.<br>6889.<br>6890.<br>6891.<br>6892.<br>6893.<br>6894.<br>6895.<br>6896.<br>6897.<br>6898.<br>6899.<br>6900.<br>6901.<br>6902.<br>6903.<br>6904.<br>6905.<br>6906.<br>6907.<br>6908.<br>6909.<br>6910.<br>6911.<br>6912.<br>6913.<br>6914.<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * Kernel-based Virtual Machine driver for Linux
 *
 * This module enables machines with Intel VT-x extensions to run virtual
 * machines without emulation or binary translation.
 *
 * MMU support
 *
 * Copyright (C) 2006 Qumranet, Inc.
 * Copyright 2010 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Yaniv Kamay  &lt;yaniv@qumranet.com&gt;
 *   Avi Kivity   &lt;avi@qumranet.com&gt;
 */

#include &quot;irq.h&quot;
#include &quot;ioapic.h&quot;
#include &quot;mmu.h&quot;
#include &quot;mmu_internal.h&quot;
#include &quot;tdp_mmu.h&quot;
#include &quot;x86.h&quot;
#include &quot;kvm_cache_regs.h&quot;
#include &quot;kvm_emulate.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;spte.h&quot;

#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/types.h&gt;
#include &lt;linux/string.h&gt;
#include &lt;linux/mm.h&gt;
#include &lt;linux/highmem.h&gt;
#include &lt;linux/moduleparam.h&gt;
#include &lt;linux/export.h&gt;
#include &lt;linux/swap.h&gt;
#include &lt;linux/hugetlb.h&gt;
#include &lt;linux/compiler.h&gt;
#include &lt;linux/srcu.h&gt;
#include &lt;linux/slab.h&gt;
#include &lt;linux/sched/signal.h&gt;
#include &lt;linux/uaccess.h&gt;
#include &lt;linux/hash.h&gt;
#include &lt;linux/kern_levels.h&gt;
#include &lt;linux/kthread.h&gt;

#include &lt;asm/page.h&gt;
#include &lt;asm/memtype.h&gt;
#include &lt;asm/cmpxchg.h&gt;
#include &lt;asm/io.h&gt;
#include &lt;asm/set_memory.h&gt;
#include &lt;asm/vmx.h&gt;
#include &lt;asm/kvm_page_track.h&gt;
#include &quot;trace.h&quot;

extern bool itlb_multihit_kvm_mitigation;

int __read_mostly nx_huge_pages = -1;
static uint __read_mostly nx_huge_pages_recovery_period_ms;
#ifdef CONFIG_PREEMPT_RT
/* Recovery can cause latency spikes, disable it for PREEMPT_RT.  */
static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
#else
static uint __read_mostly nx_huge_pages_recovery_ratio = 60;
#endif

static int set_nx_huge_pages(const char *val, const struct kernel_param *kp);
static int set_nx_huge_pages_recovery_param(const char *val, const struct kernel_param *kp);

static const struct kernel_param_ops nx_huge_pages_ops = {
	.set = set_nx_huge_pages,
	.get = param_get_bool,
};

static const struct kernel_param_ops nx_huge_pages_recovery_param_ops = {
	.set = set_nx_huge_pages_recovery_param,
	.get = param_get_uint,
};

module_param_cb(nx_huge_pages, &amp;nx_huge_pages_ops, &amp;nx_huge_pages, 0644);
__MODULE_PARM_TYPE(nx_huge_pages, &quot;bool&quot;);
module_param_cb(nx_huge_pages_recovery_ratio, &amp;nx_huge_pages_recovery_param_ops,
		&amp;nx_huge_pages_recovery_ratio, 0644);
__MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, &quot;uint&quot;);
module_param_cb(nx_huge_pages_recovery_period_ms, &amp;nx_huge_pages_recovery_param_ops,
		&amp;nx_huge_pages_recovery_period_ms, 0644);
__MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, &quot;uint&quot;);

static bool __read_mostly force_flush_and_sync_on_reuse;
module_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);

/*
 * When setting this variable to true it enables Two-Dimensional-Paging
 * where the hardware walks 2 page tables:
 * 1. the guest-virtual to guest-physical
 * 2. while doing 1. it walks guest-physical to host-physical
 * If the hardware supports that we don&#x27;t need to do shadow paging.
 */
bool tdp_enabled = false;

static int max_huge_page_level __read_mostly;
static int tdp_root_level __read_mostly;
static int max_tdp_level __read_mostly;

#ifdef MMU_DEBUG
bool dbg = 0;
module_param(dbg, bool, 0644);
#endif

#define PTE_PREFETCH_NUM		8

#include &lt;trace/events/kvm.h&gt;

/* make pte_list_desc fit well in cache lines */
#define PTE_LIST_EXT 14

/*
 * Slight optimization of cacheline layout, by putting `more&#x27; and `spte_count&#x27;
 * at the start; then accessing it will only use one single cacheline for
 * either full (entries==PTE_LIST_EXT) case or entries&lt;=6.
 */
struct pte_list_desc {
	struct pte_list_desc *more;
	/*
	 * Stores number of entries stored in the pte_list_desc.  No need to be
	 * u64 but just for easier alignment.  When PTE_LIST_EXT, means full.
	 */
	u64 spte_count;
	u64 *sptes[PTE_LIST_EXT];
};

struct kvm_shadow_walk_iterator {
	u64 addr;
	hpa_t shadow_addr;
	u64 *sptep;
	int level;
	unsigned index;
};

#define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
	for (shadow_walk_init_using_root(&amp;(_walker), (_vcpu),              \
					 (_root), (_addr));                \
	     shadow_walk_okay(&amp;(_walker));			           \
	     shadow_walk_next(&amp;(_walker)))

#define for_each_shadow_entry(_vcpu, _addr, _walker)            \
	for (shadow_walk_init(&amp;(_walker), _vcpu, _addr);	\
	     shadow_walk_okay(&amp;(_walker));			\
	     shadow_walk_next(&amp;(_walker)))

#define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
	for (shadow_walk_init(&amp;(_walker), _vcpu, _addr);		\
	     shadow_walk_okay(&amp;(_walker)) &amp;&amp;				\
		({ spte = mmu_spte_get_lockless(_walker.sptep); 1; });	\
	     __shadow_walk_next(&amp;(_walker), spte))

static struct kmem_cache *pte_list_desc_cache;
struct kmem_cache *mmu_page_header_cache;
static struct percpu_counter kvm_total_used_mmu_pages;

static void mmu_spte_set(u64 *sptep, u64 spte);

struct kvm_mmu_role_regs {
	const unsigned long cr0;
	const unsigned long cr4;
	const u64 efer;
};

#define CREATE_TRACE_POINTS
#include &quot;mmutrace.h&quot;

/*
 * Yes, lot&#x27;s of underscores.  They&#x27;re a hint that you probably shouldn&#x27;t be
 * reading from the role_regs.  Once the root_role is constructed, it becomes
 * the single source of truth for the MMU&#x27;s state.
 */
#define BUILD_MMU_ROLE_REGS_ACCESSOR(reg, name, flag)			\
static inline bool __maybe_unused					\
____is_##reg##_##name(const struct kvm_mmu_role_regs *regs)		\
{									\
	return !!(regs-&gt;reg &amp; flag);					\
}
BUILD_MMU_ROLE_REGS_ACCESSOR(cr0, pg, X86_CR0_PG);
BUILD_MMU_ROLE_REGS_ACCESSOR(cr0, wp, X86_CR0_WP);
BUILD_MMU_ROLE_REGS_ACCESSOR(cr4, pse, X86_CR4_PSE);
<blue>BUILD_MMU_ROLE_REGS_ACCESSOR(cr4, pae, X86_CR4_PAE);</blue>
<blue>BUILD_MMU_ROLE_REGS_ACCESSOR(cr4, smep, X86_CR4_SMEP);</blue>
BUILD_MMU_ROLE_REGS_ACCESSOR(cr4, smap, X86_CR4_SMAP);
BUILD_MMU_ROLE_REGS_ACCESSOR(cr4, pke, X86_CR4_PKE);
<blue>BUILD_MMU_ROLE_REGS_ACCESSOR(cr4, la57, X86_CR4_LA57);</blue>
<blue>BUILD_MMU_ROLE_REGS_ACCESSOR(efer, nx, EFER_NX);</blue>
<blue>BUILD_MMU_ROLE_REGS_ACCESSOR(efer, lma, EFER_LMA);</blue>

/*
 * The MMU itself (with a valid role) is the single source of truth for the
 * MMU.  Do not use the regs used to build the MMU/role, nor the vCPU.  The
 * regs don&#x27;t account for dependencies, e.g. clearing CR4 bits if CR0.PG=1,
 * and the vCPU may be incorrect/irrelevant.
 */
#define BUILD_MMU_ROLE_ACCESSOR(base_or_ext, reg, name)		\
static inline bool __maybe_unused is_##reg##_##name(struct kvm_mmu *mmu)	\
{								\
	return !!(mmu-&gt;cpu_role. base_or_ext . reg##_##name);	\
}
<blue>BUILD_MMU_ROLE_ACCESSOR(base, cr0, wp);</blue>
<blue>BUILD_MMU_ROLE_ACCESSOR(ext,  cr4, pse);</blue>
<blue>BUILD_MMU_ROLE_ACCESSOR(ext,  cr4, smep);</blue>
BUILD_MMU_ROLE_ACCESSOR(ext,  cr4, smap);
BUILD_MMU_ROLE_ACCESSOR(ext,  cr4, pke);
BUILD_MMU_ROLE_ACCESSOR(ext,  cr4, la57);
<blue>BUILD_MMU_ROLE_ACCESSOR(base, efer, nx);</blue>
BUILD_MMU_ROLE_ACCESSOR(ext,  efer, lma);

static inline bool is_cr0_pg(struct kvm_mmu *mmu)
{
<blue>        return mmu->cpu_role.base.level > 0;</blue>
}

static inline bool is_cr4_pae(struct kvm_mmu *mmu)
{
<blue>        return !mmu->cpu_role.base.has_4_byte_gpte;</blue>
}

static struct kvm_mmu_role_regs vcpu_to_role_regs(struct kvm_vcpu *vcpu)
{
	struct kvm_mmu_role_regs regs = {
<blue>		.cr0 = kvm_read_cr0_bits(vcpu, KVM_MMU_CR0_ROLE_BITS),</blue>
		.cr4 = kvm_read_cr4_bits(vcpu, KVM_MMU_CR4_ROLE_BITS),
		.efer = vcpu-&gt;arch.efer,
	};

	return regs;
}

static inline bool kvm_available_flush_tlb_with_range(void)
{
<yellow>	return kvm_x86_ops.tlb_remote_flush_with_range;</yellow>
}

static void kvm_flush_remote_tlbs_with_range(struct kvm *kvm,
		struct kvm_tlb_range *range)
{
	int ret = -ENOTSUPP;

<blue>	if (range && kvm_x86_ops.tlb_remote_flush_with_range)</blue>
<yellow>		ret = static_call(kvm_x86_tlb_remote_flush_with_range)(kvm, range);</yellow>

	if (ret)
<blue>		kvm_flush_remote_tlbs(kvm);</blue>
}

void kvm_flush_remote_tlbs_with_address(struct kvm *kvm,
		u64 start_gfn, u64 pages)
<blue>{</blue>
	struct kvm_tlb_range range;

	range.start_gfn = start_gfn;
	range.pages = pages;

<blue>	kvm_flush_remote_tlbs_with_range(kvm, &range);</blue>
<yellow>}</yellow>

static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
			   unsigned int access)
{
<blue>	u64 spte = make_mmio_spte(vcpu, gfn, access);</blue>

<yellow>	trace_mark_mmio_spte(sptep, gfn, spte);</yellow>
<blue>	mmu_spte_set(sptep, spte);</blue>
}

static gfn_t get_mmio_spte_gfn(u64 spte)
{
<blue>	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;</blue>

	gpa |= (spte &gt;&gt; SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)
	       &amp; shadow_nonpresent_or_rsvd_mask;

	return gpa &gt;&gt; PAGE_SHIFT;
}

static unsigned get_mmio_spte_access(u64 spte)
{
	return spte &amp; shadow_mmio_access_mask;
}

static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
{
	u64 kvm_gen, spte_gen, gen;

	gen = kvm_vcpu_memslots(vcpu)-&gt;generation;
	if (unlikely(gen &amp; KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
		return false;

	kvm_gen = gen &amp; MMIO_SPTE_GEN_MASK;
<blue>	spte_gen = get_mmio_spte_generation(spte);</blue>

<yellow>	trace_check_mmio_spte(spte, kvm_gen, spte_gen);</yellow>
	return likely(kvm_gen == spte_gen);
}

static int is_cpuid_PSE36(void)
{
	return 1;
}

#ifdef CONFIG_X86_64
static void __set_spte(u64 *sptep, u64 spte)
{
<blue>	WRITE_ONCE(*sptep, spte);</blue>
}

static void __update_clear_spte_fast(u64 *sptep, u64 spte)
{
<blue>	WRITE_ONCE(*sptep, spte);</blue>
}

static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
{
<blue>	return xchg(sptep, spte);</blue>
}

static u64 __get_spte_lockless(u64 *sptep)
{
<yellow>	return READ_ONCE(*sptep);</yellow>
}
#else
union split_spte {
	struct {
		u32 spte_low;
		u32 spte_high;
	};
	u64 spte;
};

static void count_spte_clear(u64 *sptep, u64 spte)
{
	struct kvm_mmu_page *sp =  sptep_to_sp(sptep);

	if (is_shadow_present_pte(spte))
		return;

	/* Ensure the spte is completely set before we increase the count */
	smp_wmb();
	sp-&gt;clear_spte_count++;
}

static void __set_spte(u64 *sptep, u64 spte)
{
	union split_spte *ssptep, sspte;

	ssptep = (union split_spte *)sptep;
	sspte = (union split_spte)spte;

	ssptep-&gt;spte_high = sspte.spte_high;

	/*
	 * If we map the spte from nonpresent to present, We should store
	 * the high bits firstly, then set present bit, so cpu can not
	 * fetch this spte while we are setting the spte.
	 */
	smp_wmb();

	WRITE_ONCE(ssptep-&gt;spte_low, sspte.spte_low);
}

static void __update_clear_spte_fast(u64 *sptep, u64 spte)
{
	union split_spte *ssptep, sspte;

	ssptep = (union split_spte *)sptep;
	sspte = (union split_spte)spte;

	WRITE_ONCE(ssptep-&gt;spte_low, sspte.spte_low);

	/*
	 * If we map the spte from present to nonpresent, we should clear
	 * present bit firstly to avoid vcpu fetch the old high bits.
	 */
	smp_wmb();

	ssptep-&gt;spte_high = sspte.spte_high;
	count_spte_clear(sptep, spte);
}

static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
{
	union split_spte *ssptep, sspte, orig;

	ssptep = (union split_spte *)sptep;
	sspte = (union split_spte)spte;

	/* xchg acts as a barrier before the setting of the high bits */
	orig.spte_low = xchg(&amp;ssptep-&gt;spte_low, sspte.spte_low);
	orig.spte_high = ssptep-&gt;spte_high;
	ssptep-&gt;spte_high = sspte.spte_high;
	count_spte_clear(sptep, spte);

	return orig.spte;
}

/*
 * The idea using the light way get the spte on x86_32 guest is from
 * gup_get_pte (mm/gup.c).
 *
 * An spte tlb flush may be pending, because kvm_set_pte_rmap
 * coalesces them and we are running out of the MMU lock.  Therefore
 * we need to protect against in-progress updates of the spte.
 *
 * Reading the spte while an update is in progress may get the old value
 * for the high part of the spte.  The race is fine for a present-&gt;non-present
 * change (because the high part of the spte is ignored for non-present spte),
 * but for a present-&gt;present change we must reread the spte.
 *
 * All such changes are done in two steps (present-&gt;non-present and
 * non-present-&gt;present), hence it is enough to count the number of
 * present-&gt;non-present updates: if it changed while reading the spte,
 * we might have hit the race.  This is done using clear_spte_count.
 */
static u64 __get_spte_lockless(u64 *sptep)
{
	struct kvm_mmu_page *sp =  sptep_to_sp(sptep);
	union split_spte spte, *orig = (union split_spte *)sptep;
	int count;

retry:
	count = sp-&gt;clear_spte_count;
	smp_rmb();

	spte.spte_low = orig-&gt;spte_low;
	smp_rmb();

	spte.spte_high = orig-&gt;spte_high;
	smp_rmb();

	if (unlikely(spte.spte_low != orig-&gt;spte_low ||
	      count != sp-&gt;clear_spte_count))
		goto retry;

	return spte.spte;
}
#endif

/* Rules for using mmu_spte_set:
 * Set the sptep from nonpresent to present.
 * Note: the sptep being assigned *must* be either not present
 * or in a state where the hardware will not attempt to update
 * the spte.
 */
static void mmu_spte_set(u64 *sptep, u64 new_spte)
{
<blue>	WARN_ON(is_shadow_present_pte(*sptep));</blue>
<blue>	__set_spte(sptep, new_spte);</blue>
}

/*
 * Update the SPTE (excluding the PFN), but do not track changes in its
 * accessed/dirty status.
 */
static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
{
<blue>	u64 old_spte = *sptep;</blue>

<yellow>	WARN_ON(!is_shadow_present_pte(new_spte));</yellow>
<blue>	check_spte_writable_invariants(new_spte);</blue>

<blue>	if (!is_shadow_present_pte(old_spte)) {</blue>
<blue>		mmu_spte_set(sptep, new_spte);</blue>
		return old_spte;
	}

<blue>	if (!spte_has_volatile_bits(old_spte))</blue>
<blue>		__update_clear_spte_fast(sptep, new_spte);</blue>
	else
<yellow>		old_spte = __update_clear_spte_slow(sptep, new_spte);</yellow>

<blue>	WARN_ON(spte_to_pfn(old_spte) != spte_to_pfn(new_spte));</blue>

	return old_spte;
<blue>}</blue>

/* Rules for using mmu_spte_update:
 * Update the state bits, it means the mapped pfn is not changed.
 *
 * Whenever an MMU-writable SPTE is overwritten with a read-only SPTE, remote
 * TLBs must be flushed. Otherwise rmap_write_protect will find a read-only
 * spte, even though the writable spte might be cached on a CPU&#x27;s TLB.
 *
 * Returns true if the TLB needs to be flushed
 */
static bool mmu_spte_update(u64 *sptep, u64 new_spte)
{
	bool flush = false;
<blue>	u64 old_spte = mmu_spte_update_no_track(sptep, new_spte);</blue>

	if (!is_shadow_present_pte(old_spte))
		return false;

	/*
	 * For the spte updated out of mmu-lock is safe, since
	 * we always atomically update it, see the comments in
	 * spte_has_volatile_bits().
	 */
<blue>	if (is_mmu_writable_spte(old_spte) &&</blue>
<yellow>	      !is_writable_pte(new_spte))</yellow>
		flush = true;

	/*
	 * Flush TLB when accessed/dirty states are changed in the page tables,
	 * to guarantee consistency between TLB and page tables.
	 */

<blue>	if (is_accessed_spte(old_spte) && !is_accessed_spte(new_spte)) {</blue>
		flush = true;
<yellow>		kvm_set_pfn_accessed(spte_to_pfn(old_spte));</yellow>
	}

<blue>	if (is_dirty_spte(old_spte) && !is_dirty_spte(new_spte)) {</blue>
		flush = true;
<yellow>		kvm_set_pfn_dirty(spte_to_pfn(old_spte));</yellow>
	}

	return flush;
<blue>}</blue>

/*
 * Rules for using mmu_spte_clear_track_bits:
 * It sets the sptep from present to nonpresent, and track the
 * state bits, it is used to clear the last level sptep.
 * Returns the old PTE.
 */
static u64 mmu_spte_clear_track_bits(struct kvm *kvm, u64 *sptep)
{
	kvm_pfn_t pfn;
<blue>	u64 old_spte = *sptep;</blue>
<blue>	int level = sptep_to_sp(sptep)->role.level;</blue>
	struct page *page;

	if (!is_shadow_present_pte(old_spte) ||
<blue>	    !spte_has_volatile_bits(old_spte))</blue>
<blue>		__update_clear_spte_fast(sptep, 0ull);</blue>
	else
<blue>		old_spte = __update_clear_spte_slow(sptep, 0ull);</blue>

<blue>	if (!is_shadow_present_pte(old_spte))</blue>
		return old_spte;

	kvm_update_page_stats(kvm, level, -1);

	pfn = spte_to_pfn(old_spte);

	/*
	 * KVM doesn&#x27;t hold a reference to any pages mapped into the guest, and
	 * instead uses the mmu_notifier to ensure that KVM unmaps any pages
	 * before they are reclaimed.  Sanity check that, if the pfn is backed
	 * by a refcounted page, the refcount is elevated.
	 */
	page = kvm_pfn_to_refcounted_page(pfn);
<blue>	WARN_ON(page && !page_count(page));</blue>

<blue>	if (is_accessed_spte(old_spte))</blue>
<blue>		kvm_set_pfn_accessed(pfn);</blue>

<blue>	if (is_dirty_spte(old_spte))</blue>
<blue>		kvm_set_pfn_dirty(pfn);</blue>

	return old_spte;
<blue>}</blue>

/*
 * Rules for using mmu_spte_clear_no_track:
 * Directly clear spte without caring the state bits of sptep,
 * it is used to set the upper level spte.
 */
static void mmu_spte_clear_no_track(u64 *sptep)
{
<yellow>	__update_clear_spte_fast(sptep, 0ull);</yellow>
}

static u64 mmu_spte_get_lockless(u64 *sptep)
{
<yellow>	return __get_spte_lockless(sptep);</yellow>
}

/* Returns the Accessed status of the PTE and resets it at the same time. */
static bool mmu_spte_age(u64 *sptep)
{
<yellow>	u64 spte = mmu_spte_get_lockless(sptep);</yellow>

<yellow>	if (!is_accessed_spte(spte))</yellow>
		return false;

	if (spte_ad_enabled(spte)) {
<yellow>		clear_bit((ffs(shadow_accessed_mask) - 1),</yellow>
			  (unsigned long *)sptep);
	} else {
		/*
		 * Capture the dirty status of the page, so that it doesn&#x27;t get
		 * lost when the SPTE is marked for access tracking.
		 */
<yellow>		if (is_writable_pte(spte))</yellow>
<yellow>			kvm_set_pfn_dirty(spte_to_pfn(spte));</yellow>

<yellow>		spte = mark_spte_for_access_track(spte);</yellow>
		mmu_spte_update_no_track(sptep, spte);
	}

	return true;
}

static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
{
<blue>	if (is_tdp_mmu(vcpu->arch.mmu)) {</blue>
<blue>		kvm_tdp_mmu_walk_lockless_begin();</blue>
	} else {
		/*
		 * Prevent page table teardown by making any free-er wait during
		 * kvm_flush_remote_tlbs() IPI to all active vcpus.
		 */
<blue>		local_irq_disable();</blue>

		/*
		 * Make sure a following spte read is not reordered ahead of the write
		 * to vcpu-&gt;mode.
		 */
		smp_store_mb(vcpu-&gt;mode, READING_SHADOW_PAGE_TABLES);
	}
}

static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
{
<blue>	if (is_tdp_mmu(vcpu->arch.mmu)) {</blue>
<blue>		kvm_tdp_mmu_walk_lockless_end();</blue>
	} else {
		/*
		 * Make sure the write to vcpu-&gt;mode is not reordered in front of
		 * reads to sptes.  If it does, kvm_mmu_commit_zap_page() can see us
		 * OUTSIDE_GUEST_MODE and proceed to free the shadow page table.
		 */
<blue>		smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);</blue>
		local_irq_enable();
	}
}

<blue>static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu, bool maybe_indirect)</blue>
{
	int r;

	/* 1 rmap, 1 parent PTE per level, and the prefetched rmaps. */
<blue>	r = kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,</blue>
				       1 + PT64_ROOT_MAX_LEVEL + PTE_PREFETCH_NUM);
	if (r)
		return r;
<blue>	r = kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_shadow_page_cache,</blue>
				       PT64_ROOT_MAX_LEVEL);
	if (r)
		return r;
<blue>	if (maybe_indirect) {</blue>
<blue>		r = kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_shadowed_info_cache,</blue>
					       PT64_ROOT_MAX_LEVEL);
		if (r)
			return r;
	}
<blue>	return kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_page_header_cache,</blue>
					  PT64_ROOT_MAX_LEVEL);
<blue>}</blue>

static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
{
	kvm_mmu_free_memory_cache(&amp;vcpu-&gt;arch.mmu_pte_list_desc_cache);
	kvm_mmu_free_memory_cache(&amp;vcpu-&gt;arch.mmu_shadow_page_cache);
	kvm_mmu_free_memory_cache(&amp;vcpu-&gt;arch.mmu_shadowed_info_cache);
	kvm_mmu_free_memory_cache(&amp;vcpu-&gt;arch.mmu_page_header_cache);
}

static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
{
<yellow>	kmem_cache_free(pte_list_desc_cache, pte_list_desc);</yellow>
}

static bool sp_has_gptes(struct kvm_mmu_page *sp);

<blue>static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)</blue>
{
<blue>	if (sp->role.passthrough)</blue>
<yellow>		return sp->gfn;</yellow>

<blue>	if (!sp->role.direct)</blue>
<blue>		return sp->shadowed_translation[index] >> PAGE_SHIFT;</blue>

<blue>	return sp->gfn + (index << ((sp->role.level - 1) * SPTE_LEVEL_BITS));</blue>
<blue>}</blue>

/*
 * For leaf SPTEs, fetch the *guest* access permissions being shadowed. Note
 * that the SPTE itself may have a more constrained access permissions that
 * what the guest enforces. For example, a guest may create an executable
 * huge PTE but KVM may disallow execution to mitigate iTLB multihit.
 */
static u32 kvm_mmu_page_get_access(struct kvm_mmu_page *sp, int index)
{
<yellow>	if (sp_has_gptes(sp))</yellow>
		return sp-&gt;shadowed_translation[index] &amp; ACC_ALL;

	/*
	 * For direct MMUs (e.g. TDP or non-paging guests) or passthrough SPs,
	 * KVM is not shadowing any guest page tables, so the &quot;guest access
	 * permissions&quot; are just ACC_ALL.
	 *
	 * For direct SPs in indirect MMUs (shadow paging), i.e. when KVM
	 * is shadowing a guest huge page with small pages, the guest access
	 * permissions being shadowed are the access permissions of the huge
	 * page.
	 *
	 * In both cases, sp-&gt;role.access contains the correct access bits.
	 */
<blue>	return sp->role.access;</blue>
}

<blue>static void kvm_mmu_page_set_translation(struct kvm_mmu_page *sp, int index,</blue>
					 gfn_t gfn, unsigned int access)
{
<blue>	if (sp_has_gptes(sp)) {</blue>
<blue>		sp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;</blue>
		return;
	}

<blue>	WARN_ONCE(access != kvm_mmu_page_get_access(sp, index),</blue>
	          &quot;access mismatch under %s page %llx (expected %u, got %u)\n&quot;,
	          sp-&gt;role.passthrough ? &quot;passthrough&quot; : &quot;direct&quot;,
	          sp-&gt;gfn, kvm_mmu_page_get_access(sp, index), access);

<blue>	WARN_ONCE(gfn != kvm_mmu_page_get_gfn(sp, index),</blue>
	          &quot;gfn mismatch under %s page %llx (expected %llx, got %llx)\n&quot;,
	          sp-&gt;role.passthrough ? &quot;passthrough&quot; : &quot;direct&quot;,
	          sp-&gt;gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
<blue>}</blue>

static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
				    unsigned int access)
{
<yellow>	gfn_t gfn = kvm_mmu_page_get_gfn(sp, index);</yellow>

	kvm_mmu_page_set_translation(sp, index, gfn, access);
}

/*
 * Return the pointer to the large page information for a given gfn,
 * handling slots that are not large page aligned.
 */
static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
		const struct kvm_memory_slot *slot, int level)
{
	unsigned long idx;

<blue>	idx = gfn_to_index(gfn, slot->base_gfn, level);</blue>
	return &amp;slot-&gt;arch.lpage_info[level - 2][idx];
}

static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
					    gfn_t gfn, int count)
{
	struct kvm_lpage_info *linfo;
	int i;

	for (i = PG_LEVEL_2M; i &lt;= KVM_MAX_HUGEPAGE_LEVEL; ++i) {
<blue>		linfo = lpage_info_slot(gfn, slot, i);</blue>
		linfo-&gt;disallow_lpage += count;
<yellow>		WARN_ON(linfo->disallow_lpage < 0);</yellow>
	}
<blue>}</blue>

void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
{
<blue>	update_gfn_disallow_lpage_count(slot, gfn, 1);</blue>
}

void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
{
<blue>	update_gfn_disallow_lpage_count(slot, gfn, -1);</blue>
}

static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
{
	struct kvm_memslots *slots;
	struct kvm_memory_slot *slot;
	gfn_t gfn;

<blue>	kvm->arch.indirect_shadow_pages++;</blue>
	gfn = sp-&gt;gfn;
	slots = kvm_memslots_for_spte_role(kvm, sp-&gt;role);
<blue>	slot = __gfn_to_memslot(slots, gfn);</blue>

	/* the non-leaf shadow pages are keeping readonly. */
<blue>	if (sp->role.level > PG_LEVEL_4K)</blue>
<blue>		return kvm_slot_page_track_add_page(kvm, slot, gfn,</blue>
						    KVM_PAGE_TRACK_WRITE);

<yellow>	kvm_mmu_gfn_disallow_lpage(slot, gfn);</yellow>

	if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
<yellow>		kvm_flush_remote_tlbs_with_address(kvm, gfn, 1);</yellow>
}

<yellow>void account_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)</yellow>
{
<yellow>	if (sp->lpage_disallowed)</yellow>
		return;

<yellow>	++kvm->stat.nx_lpage_splits;</yellow>
	list_add_tail(&amp;sp-&gt;lpage_disallowed_link,
		      &amp;kvm-&gt;arch.lpage_disallowed_mmu_pages);
	sp-&gt;lpage_disallowed = true;
<yellow>}</yellow>

static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
{
	struct kvm_memslots *slots;
	struct kvm_memory_slot *slot;
	gfn_t gfn;

<blue>	kvm->arch.indirect_shadow_pages--;</blue>
	gfn = sp-&gt;gfn;
	slots = kvm_memslots_for_spte_role(kvm, sp-&gt;role);
<blue>	slot = __gfn_to_memslot(slots, gfn);</blue>
<blue>	if (sp->role.level > PG_LEVEL_4K)</blue>
<blue>		return kvm_slot_page_track_remove_page(kvm, slot, gfn,</blue>
						       KVM_PAGE_TRACK_WRITE);

<yellow>	kvm_mmu_gfn_allow_lpage(slot, gfn);</yellow>
}

void unaccount_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
{
<yellow>	--kvm->stat.nx_lpage_splits;</yellow>
	sp-&gt;lpage_disallowed = false;
	list_del(&amp;sp-&gt;lpage_disallowed_link);
}

static struct kvm_memory_slot *
gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t gfn,
			    bool no_dirty_log)
{
	struct kvm_memory_slot *slot;

<yellow>	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);</yellow>
<blue>	if (!slot || slot->flags & KVM_MEMSLOT_INVALID)</blue>
		return NULL;
<blue>	if (no_dirty_log && kvm_slot_dirty_track_enabled(slot))</blue>
		return NULL;

	return slot;
}

/*
 * About rmap_head encoding:
 *
 * If the bit zero of rmap_head-&gt;val is clear, then it points to the only spte
 * in this rmap chain. Otherwise, (rmap_head-&gt;val &amp; ~1) points to a struct
 * pte_list_desc containing more mappings.
 */

/*
 * Returns the number of pointers in the rmap chain, not counting the new one.
 */
static int pte_list_add(struct kvm_mmu_memory_cache *cache, u64 *spte,
			struct kvm_rmap_head *rmap_head)
{
	struct pte_list_desc *desc;
	int count = 0;

<blue>	if (!rmap_head->val) {</blue>
		rmap_printk(&quot;%p %llx 0-&gt;1\n&quot;, spte, *spte);
<blue>		rmap_head->val = (unsigned long)spte;</blue>
<blue>	} else if (!(rmap_head->val & 1)) {</blue>
		rmap_printk(&quot;%p %llx 1-&gt;many\n&quot;, spte, *spte);
<blue>		desc = kvm_mmu_memory_cache_alloc(cache);</blue>
		desc-&gt;sptes[0] = (u64 *)rmap_head-&gt;val;
		desc-&gt;sptes[1] = spte;
		desc-&gt;spte_count = 2;
		rmap_head-&gt;val = (unsigned long)desc | 1;
		++count;
	} else {
		rmap_printk(&quot;%p %llx many-&gt;many\n&quot;, spte, *spte);
<yellow>		desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);</yellow>
<yellow>		while (desc->spte_count == PTE_LIST_EXT) {</yellow>
			count += PTE_LIST_EXT;
<yellow>			if (!desc->more) {</yellow>
<yellow>				desc->more = kvm_mmu_memory_cache_alloc(cache);</yellow>
				desc = desc-&gt;more;
				desc-&gt;spte_count = 0;
				break;
			}
			desc = desc-&gt;more;
		}
		count += desc-&gt;spte_count;
<yellow>		desc->sptes[desc->spte_count++] = spte;</yellow>
	}
	return count;
<blue>}</blue>

static void
pte_list_desc_remove_entry(struct kvm_rmap_head *rmap_head,
			   struct pte_list_desc *desc, int i,
			   struct pte_list_desc *prev_desc)
{
<blue>	int j = desc->spte_count - 1;</blue>

	desc-&gt;sptes[i] = desc-&gt;sptes[j];
	desc-&gt;sptes[j] = NULL;
	desc-&gt;spte_count--;
	if (desc-&gt;spte_count)
		return;
<yellow>	if (!prev_desc && !desc->more)</yellow>
<yellow>		rmap_head->val = 0;</yellow>
	else
		if (prev_desc)
<yellow>			prev_desc->more = desc->more;</yellow>
		else
<yellow>			rmap_head->val = (unsigned long)desc->more | 1;</yellow>
<yellow>	mmu_free_pte_list_desc(desc);</yellow>
}

static void pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
{
	struct pte_list_desc *desc;
	struct pte_list_desc *prev_desc;
	int i;

<blue>	if (!rmap_head->val) {</blue>
		pr_err(&quot;%s: %p 0-&gt;BUG\n&quot;, __func__, spte);
		BUG();
<blue>	} else if (!(rmap_head->val & 1)) {</blue>
		rmap_printk(&quot;%p 1-&gt;0\n&quot;, spte);
<blue>		if ((u64 *)rmap_head->val != spte) {</blue>
			pr_err(&quot;%s:  %p 1-&gt;BUG\n&quot;, __func__, spte);
			BUG();
		}
<blue>		rmap_head->val = 0;</blue>
	} else {
		rmap_printk(&quot;%p many-&gt;many\n&quot;, spte);
<blue>		desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);</blue>
		prev_desc = NULL;
		while (desc) {
<blue>			for (i = 0; i < desc->spte_count; ++i) {</blue>
<blue>				if (desc->sptes[i] == spte) {</blue>
<blue>					pte_list_desc_remove_entry(rmap_head,</blue>
							desc, i, prev_desc);
					return;
				}
			}
			prev_desc = desc;
<yellow>			desc = desc->more;</yellow>
		}
		pr_err(&quot;%s: %p many-&gt;many\n&quot;, __func__, spte);
		BUG();
	}
<blue>}</blue>

static void kvm_zap_one_rmap_spte(struct kvm *kvm,
				  struct kvm_rmap_head *rmap_head, u64 *sptep)
{
<yellow>	mmu_spte_clear_track_bits(kvm, sptep);</yellow>
	pte_list_remove(sptep, rmap_head);
}

/* Return true if at least one SPTE was zapped, false otherwise */
static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
				   struct kvm_rmap_head *rmap_head)
{
	struct pte_list_desc *desc, *next;
	int i;

<blue>	if (!rmap_head->val)</blue>
		return false;

<blue>	if (!(rmap_head->val & 1)) {</blue>
<blue>		mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);</blue>
		goto out;
	}

<blue>	desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);</blue>

	for (; desc; desc = next) {
<blue>		for (i = 0; i < desc->spte_count; i++)</blue>
<blue>			mmu_spte_clear_track_bits(kvm, desc->sptes[i]);</blue>
<blue>		next = desc->more;</blue>
		mmu_free_pte_list_desc(desc);
	}
out:
	/* rmap_head is meaningless now, remember to reset it */
<blue>	rmap_head->val = 0;</blue>
	return true;
<blue>}</blue>

unsigned int pte_list_count(struct kvm_rmap_head *rmap_head)
{
	struct pte_list_desc *desc;
	unsigned int count = 0;

<yellow>	if (!rmap_head->val)</yellow>
<yellow>		return 0;</yellow>
<yellow>	else if (!(rmap_head->val & 1))</yellow>
		return 1;

<yellow>	desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);</yellow>

	while (desc) {
<yellow>		count += desc->spte_count;</yellow>
		desc = desc-&gt;more;
	}

	return count;
<yellow>}</yellow>

static struct kvm_rmap_head *gfn_to_rmap(gfn_t gfn, int level,
					 const struct kvm_memory_slot *slot)
{
	unsigned long idx;

<blue>	idx = gfn_to_index(gfn, slot->base_gfn, level);</blue>
	return &amp;slot-&gt;arch.rmap[level - PG_LEVEL_4K][idx];
}

static bool rmap_can_add(struct kvm_vcpu *vcpu)
{
	struct kvm_mmu_memory_cache *mc;

<yellow>	mc = &vcpu->arch.mmu_pte_list_desc_cache;</yellow>
	return kvm_mmu_memory_cache_nr_free_objects(mc);
}

static void rmap_remove(struct kvm *kvm, u64 *spte)
{
	struct kvm_memslots *slots;
	struct kvm_memory_slot *slot;
	struct kvm_mmu_page *sp;
	gfn_t gfn;
	struct kvm_rmap_head *rmap_head;

<blue>	sp = sptep_to_sp(spte);</blue>
	gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));

	/*
	 * Unlike rmap_add, rmap_remove does not run in the context of a vCPU
	 * so we have to determine which memslots to use based on context
	 * information in sp-&gt;role.
	 */
	slots = kvm_memslots_for_spte_role(kvm, sp-&gt;role);

<blue>	slot = __gfn_to_memslot(slots, gfn);</blue>
<blue>	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);</blue>

	pte_list_remove(spte, rmap_head);
}

/*
 * Used by the following functions to iterate through the sptes linked by a
 * rmap.  All fields are private and not assumed to be used outside.
 */
struct rmap_iterator {
	/* private fields */
	struct pte_list_desc *desc;	/* holds the sptep if not NULL */
	int pos;			/* index of the sptep */
};

/*
 * Iteration must be started by this function.  This should also be used after
 * removing/dropping sptes from the rmap link because in such cases the
 * information in the iterator may not be valid.
 *
 * Returns sptep if found, NULL otherwise.
 */
static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
			   struct rmap_iterator *iter)
{
	u64 *sptep;

<blue>	if (!rmap_head->val)</blue>
		return NULL;

<yellow>	if (!(rmap_head->val & 1)) {</yellow>
<yellow>		iter->desc = NULL;</yellow>
<yellow>		sptep = (u64 *)rmap_head->val;</yellow>
<yellow>		goto out;</yellow>
	}

<yellow>	iter->desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);</yellow>
<yellow>	iter->pos = 0;</yellow>
	sptep = iter-&gt;desc-&gt;sptes[iter-&gt;pos];
out:
<yellow>	BUG_ON(!is_shadow_present_pte(*sptep));</yellow>
	return sptep;
}

/*
 * Must be used with a valid iterator: e.g. after rmap_get_first().
 *
 * Returns sptep if found, NULL otherwise.
 */
static u64 *rmap_get_next(struct rmap_iterator *iter)
{
	u64 *sptep;

<yellow>	if (iter->desc) {</yellow>
<yellow>		if (iter->pos < PTE_LIST_EXT - 1) {</yellow>
<yellow>			++iter->pos;</yellow>
			sptep = iter-&gt;desc-&gt;sptes[iter-&gt;pos];
			if (sptep)
				goto out;
		}

<yellow>		iter->desc = iter->desc->more;</yellow>

		if (iter-&gt;desc) {
			iter-&gt;pos = 0;
			/* desc-&gt;sptes[0] cannot be NULL */
<yellow>			sptep = iter->desc->sptes[iter->pos];</yellow>
			goto out;
		}
	}

<yellow>	return NULL;</yellow>
out:
<yellow>	BUG_ON(!is_shadow_present_pte(*sptep));</yellow>
	return sptep;
<yellow>}</yellow>

#define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
	     _spte_; _spte_ = rmap_get_next(_iter_))

static void drop_spte(struct kvm *kvm, u64 *sptep)
{
<blue>	u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);</blue>

	if (is_shadow_present_pte(old_spte))
<blue>		rmap_remove(kvm, sptep);</blue>
<blue>}</blue>

static void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)
{
	struct kvm_mmu_page *sp;

<yellow>	sp = sptep_to_sp(sptep);</yellow>
<yellow>	WARN_ON(sp->role.level == PG_LEVEL_4K);</yellow>

<yellow>	drop_spte(kvm, sptep);</yellow>

	if (flush)
<yellow>		kvm_flush_remote_tlbs_with_address(kvm, sp->gfn,</yellow>
<yellow>			KVM_PAGES_PER_HPAGE(sp->role.level));</yellow>
}

/*
 * Write-protect on the specified @sptep, @pt_protect indicates whether
 * spte write-protection is caused by protecting shadow page table.
 *
 * Note: write protection is difference between dirty logging and spte
 * protection:
 * - for dirty logging, the spte can be set to writable at anytime if
 *   its dirty bitmap is properly set.
 * - for spte protection, the spte can be writable only after unsync-ing
 *   shadow page.
 *
 * Return true if tlb need be flushed.
 */
static bool spte_write_protect(u64 *sptep, bool pt_protect)
{
<yellow>	u64 spte = *sptep;</yellow>

<yellow>	if (!is_writable_pte(spte) &&</yellow>
<yellow>	    !(pt_protect && is_mmu_writable_spte(spte)))</yellow>
		return false;

	rmap_printk(&quot;spte %p %llx\n&quot;, sptep, *sptep);

<yellow>	if (pt_protect)</yellow>
<yellow>		spte &= ~shadow_mmu_writable_mask;</yellow>
<yellow>	spte = spte & ~PT_WRITABLE_MASK;</yellow>

	return mmu_spte_update(sptep, spte);
}

static bool rmap_write_protect(struct kvm_rmap_head *rmap_head,
			       bool pt_protect)
{
	u64 *sptep;
	struct rmap_iterator iter;
	bool flush = false;

<blue>	for_each_rmap_spte(rmap_head, &iter, sptep)</blue>
<yellow>		flush |= spte_write_protect(sptep, pt_protect);</yellow>

	return flush;
<blue>}</blue>

static bool spte_clear_dirty(u64 *sptep)
{
	u64 spte = *sptep;

	rmap_printk(&quot;spte %p %llx\n&quot;, sptep, *sptep);

	MMU_WARN_ON(!spte_ad_enabled(spte));
<yellow>	spte &= ~shadow_dirty_mask;</yellow>
	return mmu_spte_update(sptep, spte);
}

static bool spte_wrprot_for_clear_dirty(u64 *sptep)
{
<yellow>	bool was_writable = test_and_clear_bit(PT_WRITABLE_SHIFT,</yellow>
					       (unsigned long *)sptep);
<yellow>	if (was_writable && !spte_ad_enabled(*sptep))</yellow>
<yellow>		kvm_set_pfn_dirty(spte_to_pfn(*sptep));</yellow>

	return was_writable;
}

/*
 * Gets the GFN ready for another round of dirty logging by clearing the
 *	- D bit on ad-enabled SPTEs, and
 *	- W bit on ad-disabled SPTEs.
 * Returns true iff any D or W bits were cleared.
 */
static bool __rmap_clear_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			       const struct kvm_memory_slot *slot)
{
	u64 *sptep;
	struct rmap_iterator iter;
	bool flush = false;

<yellow>	for_each_rmap_spte(rmap_head, &iter, sptep)</yellow>
<yellow>		if (spte_ad_need_write_protect(*sptep))</yellow>
<yellow>			flush |= spte_wrprot_for_clear_dirty(sptep);</yellow>
		else
<yellow>			flush |= spte_clear_dirty(sptep);</yellow>

	return flush;
<yellow>}</yellow>

/**
 * kvm_mmu_write_protect_pt_masked - write protect selected PT level pages
 * @kvm: kvm instance
 * @slot: slot to protect
 * @gfn_offset: start of the BITS_PER_LONG pages we care about
 * @mask: indicates which pages we should protect
 *
 * Used when we do not need to care about huge page mappings.
 */
static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
				     struct kvm_memory_slot *slot,
				     gfn_t gfn_offset, unsigned long mask)
{
	struct kvm_rmap_head *rmap_head;

<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
		kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
<yellow>				slot->base_gfn + gfn_offset, mask, true);</yellow>

<yellow>	if (!kvm_memslots_have_rmaps(kvm))</yellow>
		return;

<yellow>	while (mask) {</yellow>
<yellow>		rmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),</yellow>
					PG_LEVEL_4K, slot);
		rmap_write_protect(rmap_head, false);

		/* clear the first set bit */
		mask &amp;= mask - 1;
	}
}

/**
 * kvm_mmu_clear_dirty_pt_masked - clear MMU D-bit for PT level pages, or write
 * protect the page if the D-bit isn&#x27;t supported.
 * @kvm: kvm instance
 * @slot: slot to clear D-bit
 * @gfn_offset: start of the BITS_PER_LONG pages we care about
 * @mask: indicates which pages we should clear D-bit
 *
 * Used for PML to re-log the dirty GPAs after userspace querying dirty_bitmap.
 */
static void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
					 struct kvm_memory_slot *slot,
					 gfn_t gfn_offset, unsigned long mask)
{
	struct kvm_rmap_head *rmap_head;

<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
		kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
<yellow>				slot->base_gfn + gfn_offset, mask, false);</yellow>

<yellow>	if (!kvm_memslots_have_rmaps(kvm))</yellow>
		return;

<yellow>	while (mask) {</yellow>
<yellow>		rmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),</yellow>
					PG_LEVEL_4K, slot);
		__rmap_clear_dirty(kvm, rmap_head, slot);

		/* clear the first set bit */
		mask &amp;= mask - 1;
	}
}

/**
 * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected
 * PT level pages.
 *
 * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to
 * enable dirty logging for them.
 *
 * We need to care about huge page mappings: e.g. during dirty logging we may
 * have such mappings.
 */
void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
				struct kvm_memory_slot *slot,
				gfn_t gfn_offset, unsigned long mask)
{
	/*
	 * Huge pages are NOT write protected when we start dirty logging in
	 * initially-all-set mode; must write protect them here so that they
	 * are split to 4K on the first write.
	 *
	 * The gfn_offset is guaranteed to be aligned to 64, but the base_gfn
	 * of memslot has no such restriction, so the range can cross two large
	 * pages.
	 */
<yellow>	if (kvm_dirty_log_manual_protect_and_init_set(kvm)) {</yellow>
<yellow>		gfn_t start = slot->base_gfn + gfn_offset + __ffs(mask);</yellow>
		gfn_t end = slot-&gt;base_gfn + gfn_offset + __fls(mask);

<yellow>		if (READ_ONCE(eager_page_split))</yellow>
<yellow>			kvm_mmu_try_split_huge_pages(kvm, slot, start, end, PG_LEVEL_4K);</yellow>

<yellow>		kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);</yellow>

		/* Cross two large pages? */
		if (ALIGN(start &lt;&lt; PAGE_SHIFT, PMD_SIZE) !=
		    ALIGN(end &lt;&lt; PAGE_SHIFT, PMD_SIZE))
<yellow>			kvm_mmu_slot_gfn_write_protect(kvm, slot, end,</yellow>
						       PG_LEVEL_2M);
	}

	/* Now handle 4K PTEs.  */
<yellow>	if (kvm_x86_ops.cpu_dirty_log_size)</yellow>
<yellow>		kvm_mmu_clear_dirty_pt_masked(kvm, slot, gfn_offset, mask);</yellow>
	else
<yellow>		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);</yellow>
<yellow>}</yellow>

int kvm_cpu_dirty_log_size(void)
{
<yellow>	return kvm_x86_ops.cpu_dirty_log_size;</yellow>
}

bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
				    struct kvm_memory_slot *slot, u64 gfn,
				    int min_level)
{
	struct kvm_rmap_head *rmap_head;
	int i;
	bool write_protected = false;

<blue>	if (kvm_memslots_have_rmaps(kvm)) {</blue>
<blue>		for (i = min_level; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {</blue>
<blue>			rmap_head = gfn_to_rmap(gfn, i, slot);</blue>
			write_protected |= rmap_write_protect(rmap_head, true);
		}
	}

<blue>	if (is_tdp_mmu_enabled(kvm))</blue>
		write_protected |=
<blue>			kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);</blue>

	return write_protected;
<blue>}</blue>

static bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
{
	struct kvm_memory_slot *slot;

	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
	return kvm_mmu_slot_gfn_write_protect(vcpu-&gt;kvm, slot, gfn, PG_LEVEL_4K);
}

static bool __kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			   const struct kvm_memory_slot *slot)
{
<blue>	return kvm_zap_all_rmap_sptes(kvm, rmap_head);</blue>
}

static bool kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			 struct kvm_memory_slot *slot, gfn_t gfn, int level,
			 pte_t unused)
{
<blue>	return __kvm_zap_rmap(kvm, rmap_head, slot);</blue>
}

static bool kvm_set_pte_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			     struct kvm_memory_slot *slot, gfn_t gfn, int level,
			     pte_t pte)
{
	u64 *sptep;
	struct rmap_iterator iter;
	bool need_flush = false;
	u64 new_spte;
	kvm_pfn_t new_pfn;

<blue>	WARN_ON(pte_huge(pte));</blue>
<blue>	new_pfn = pte_pfn(pte);</blue>

restart:
<yellow>	for_each_rmap_spte(rmap_head, &iter, sptep) {</yellow>
		rmap_printk(&quot;spte %p %llx gfn %llx (%d)\n&quot;,
			    sptep, *sptep, gfn, level);

		need_flush = true;

<yellow>		if (pte_write(pte)) {</yellow>
<yellow>			kvm_zap_one_rmap_spte(kvm, rmap_head, sptep);</yellow>
			goto restart;
		} else {
<yellow>			new_spte = kvm_mmu_changed_pte_notifier_make_spte(</yellow>
					*sptep, new_pfn);

			mmu_spte_clear_track_bits(kvm, sptep);
<yellow>			mmu_spte_set(sptep, new_spte);</yellow>
		}
	}

<yellow>	if (need_flush && kvm_available_flush_tlb_with_range()) {</yellow>
<yellow>		kvm_flush_remote_tlbs_with_address(kvm, gfn, 1);</yellow>
<yellow>		return false;</yellow>
	}

	return need_flush;
}

struct slot_rmap_walk_iterator {
	/* input fields. */
	const struct kvm_memory_slot *slot;
	gfn_t start_gfn;
	gfn_t end_gfn;
	int start_level;
	int end_level;

	/* output fields. */
	gfn_t gfn;
	struct kvm_rmap_head *rmap;
	int level;

	/* private field. */
	struct kvm_rmap_head *end_rmap;
};

static void
rmap_walk_init_level(struct slot_rmap_walk_iterator *iterator, int level)
{
<blue>	iterator->level = level;</blue>
	iterator-&gt;gfn = iterator-&gt;start_gfn;
<blue>	iterator->rmap = gfn_to_rmap(iterator->gfn, level, iterator->slot);</blue>
<blue>	iterator->end_rmap = gfn_to_rmap(iterator->end_gfn, level, iterator->slot);</blue>
}

static void
slot_rmap_walk_init(struct slot_rmap_walk_iterator *iterator,
		    const struct kvm_memory_slot *slot, int start_level,
		    int end_level, gfn_t start_gfn, gfn_t end_gfn)
{
<yellow>	iterator->slot = slot;</yellow>
	iterator-&gt;start_level = start_level;
	iterator-&gt;end_level = end_level;
	iterator-&gt;start_gfn = start_gfn;
	iterator-&gt;end_gfn = end_gfn;

	rmap_walk_init_level(iterator, iterator-&gt;start_level);
}

static bool slot_rmap_walk_okay(struct slot_rmap_walk_iterator *iterator)
{
	return !!iterator-&gt;rmap;
}

static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
{
<blue>	while (++iterator->rmap <= iterator->end_rmap) {</blue>
<blue>		iterator->gfn += (1UL << KVM_HPAGE_GFN_SHIFT(iterator->level));</blue>

		if (iterator-&gt;rmap-&gt;val)
			return;
	}

<blue>	if (++iterator->level > iterator->end_level) {</blue>
<blue>		iterator->rmap = NULL;</blue>
		return;
	}

<blue>	rmap_walk_init_level(iterator, iterator->level);</blue>
<blue>}</blue>

#define for_each_slot_rmap_range(_slot_, _start_level_, _end_level_,	\
	   _start_gfn, _end_gfn, _iter_)				\
	for (slot_rmap_walk_init(_iter_, _slot_, _start_level_,		\
				 _end_level_, _start_gfn, _end_gfn);	\
	     slot_rmap_walk_okay(_iter_);				\
	     slot_rmap_walk_next(_iter_))

typedef bool (*rmap_handler_t)(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			       struct kvm_memory_slot *slot, gfn_t gfn,
			       int level, pte_t pte);

static __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,
						 struct kvm_gfn_range *range,
						 rmap_handler_t handler)
{
	struct slot_rmap_walk_iterator iterator;
	bool ret = false;

<blue>	for_each_slot_rmap_range(range->slot, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,</blue>
				 range-&gt;start, range-&gt;end - 1, &amp;iterator)
<blue>		ret |= handler(kvm, iterator.rmap, range->slot, iterator.gfn,</blue>
			       iterator.level, range-&gt;pte);

<blue>	return ret;</blue>
}

bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
<blue>{</blue>
	bool flush = false;

<blue>	if (kvm_memslots_have_rmaps(kvm))</blue>
<blue>		flush = kvm_handle_gfn_range(kvm, range, kvm_zap_rmap);</blue>

<blue>	if (is_tdp_mmu_enabled(kvm))</blue>
<blue>		flush = kvm_tdp_mmu_unmap_gfn_range(kvm, range, flush);</blue>

	return flush;
}

bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
<blue>{</blue>
	bool flush = false;

<blue>	if (kvm_memslots_have_rmaps(kvm))</blue>
<blue>		flush = kvm_handle_gfn_range(kvm, range, kvm_set_pte_rmap);</blue>

<blue>	if (is_tdp_mmu_enabled(kvm))</blue>
<blue>		flush |= kvm_tdp_mmu_set_spte_gfn(kvm, range);</blue>

	return flush;
}

static bool kvm_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			 struct kvm_memory_slot *slot, gfn_t gfn, int level,
			 pte_t unused)
{
	u64 *sptep;
	struct rmap_iterator iter;
	int young = 0;

<yellow>	for_each_rmap_spte(rmap_head, &iter, sptep)</yellow>
<yellow>		young |= mmu_spte_age(sptep);</yellow>

	return young;
}

static bool kvm_test_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
			      struct kvm_memory_slot *slot, gfn_t gfn,
			      int level, pte_t unused)
{
	u64 *sptep;
	struct rmap_iterator iter;

<yellow>	for_each_rmap_spte(rmap_head, &iter, sptep)</yellow>
<yellow>		if (is_accessed_spte(*sptep))</yellow>
			return true;
	return false;
}

#define RMAP_RECYCLE_THRESHOLD 1000

static void __rmap_add(struct kvm *kvm,
		       struct kvm_mmu_memory_cache *cache,
		       const struct kvm_memory_slot *slot,
		       u64 *spte, gfn_t gfn, unsigned int access)
<blue>{</blue>
	struct kvm_mmu_page *sp;
	struct kvm_rmap_head *rmap_head;
	int rmap_count;

<blue>	sp = sptep_to_sp(spte);</blue>
	kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
	kvm_update_page_stats(kvm, sp-&gt;role.level, 1);

	rmap_head = gfn_to_rmap(gfn, sp-&gt;role.level, slot);
	rmap_count = pte_list_add(cache, spte, rmap_head);

	if (rmap_count &gt; kvm-&gt;stat.max_mmu_rmap_size)
<blue>		kvm->stat.max_mmu_rmap_size = rmap_count;</blue>
<blue>	if (rmap_count > RMAP_RECYCLE_THRESHOLD) {</blue>
<yellow>		kvm_zap_all_rmap_sptes(kvm, rmap_head);</yellow>
<yellow>		kvm_flush_remote_tlbs_with_address(</yellow>
<yellow>				kvm, sp->gfn, KVM_PAGES_PER_HPAGE(sp->role.level));</yellow>
	}
}

static void rmap_add(struct kvm_vcpu *vcpu, const struct kvm_memory_slot *slot,
		     u64 *spte, gfn_t gfn, unsigned int access)
{
<blue>	struct kvm_mmu_memory_cache *cache = &vcpu->arch.mmu_pte_list_desc_cache;</blue>

	__rmap_add(vcpu-&gt;kvm, cache, slot, spte, gfn, access);
}

bool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
<yellow>{</yellow>
	bool young = false;

<yellow>	if (kvm_memslots_have_rmaps(kvm))</yellow>
<yellow>		young = kvm_handle_gfn_range(kvm, range, kvm_age_rmap);</yellow>

<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
<yellow>		young |= kvm_tdp_mmu_age_gfn_range(kvm, range);</yellow>

	return young;
}

bool kvm_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
<yellow>{</yellow>
	bool young = false;

<yellow>	if (kvm_memslots_have_rmaps(kvm))</yellow>
<yellow>		young = kvm_handle_gfn_range(kvm, range, kvm_test_age_rmap);</yellow>

<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
<yellow>		young |= kvm_tdp_mmu_test_age_gfn(kvm, range);</yellow>

	return young;
}

#ifdef MMU_DEBUG
static int is_empty_shadow_page(u64 *spt)
{
	u64 *pos;
	u64 *end;

	for (pos = spt, end = pos + PAGE_SIZE / sizeof(u64); pos != end; pos++)
		if (is_shadow_present_pte(*pos)) {
			printk(KERN_ERR &quot;%s: %p %llx\n&quot;, __func__,
			       pos, *pos);
			return 0;
		}
	return 1;
}
#endif

/*
 * This value is the sum of all of the kvm instances&#x27;s
 * kvm-&gt;arch.n_used_mmu_pages values.  We need a global,
 * aggregate version in order to make the slab shrinker
 * faster
 */
static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, long nr)
{
<blue>	kvm->arch.n_used_mmu_pages += nr;</blue>
	percpu_counter_add(&amp;kvm_total_used_mmu_pages, nr);
}

static void kvm_account_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
{
	kvm_mod_used_mmu_pages(kvm, +1);
<blue>	kvm_account_pgtable_pages((void *)sp->spt, +1);</blue>
}

static void kvm_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
{
<blue>	kvm_mod_used_mmu_pages(kvm, -1);</blue>
<blue>	kvm_account_pgtable_pages((void *)sp->spt, -1);</blue>
}

static void kvm_mmu_free_shadow_page(struct kvm_mmu_page *sp)
{
	MMU_WARN_ON(!is_empty_shadow_page(sp-&gt;spt));
<blue>	hlist_del(&sp->hash_link);</blue>
<blue>	list_del(&sp->link);</blue>
	free_page((unsigned long)sp-&gt;spt);
	if (!sp-&gt;role.direct)
<blue>		free_page((unsigned long)sp->shadowed_translation);</blue>
<blue>	kmem_cache_free(mmu_page_header_cache, sp);</blue>
}

static unsigned kvm_page_table_hashfn(gfn_t gfn)
{
<blue>	return hash_64(gfn, KVM_MMU_HASH_SHIFT);</blue>
}

static void mmu_page_add_parent_pte(struct kvm_mmu_memory_cache *cache,
				    struct kvm_mmu_page *sp, u64 *parent_pte)
{
	if (!parent_pte)
		return;

<blue>	pte_list_add(cache, parent_pte, &sp->parent_ptes);</blue>
}

static void mmu_page_remove_parent_pte(struct kvm_mmu_page *sp,
				       u64 *parent_pte)
{
<blue>	pte_list_remove(parent_pte, &sp->parent_ptes);</blue>
}

static void drop_parent_pte(struct kvm_mmu_page *sp,
			    u64 *parent_pte)
{
<blue>	mmu_page_remove_parent_pte(sp, parent_pte);</blue>
	mmu_spte_clear_no_track(parent_pte);
}

static void mark_unsync(u64 *spte);
static void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)
{
	u64 *sptep;
	struct rmap_iterator iter;

<yellow>	for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {</yellow>
<yellow>		mark_unsync(sptep);</yellow>
	}
<yellow>}</yellow>

static void mark_unsync(u64 *spte)
{
	struct kvm_mmu_page *sp;

<yellow>	sp = sptep_to_sp(spte);</yellow>
	if (__test_and_set_bit(spte_index(spte), sp-&gt;unsync_child_bitmap))
		return;
<yellow>	if (sp->unsync_children++)</yellow>
		return;
<yellow>	kvm_mmu_mark_parents_unsync(sp);</yellow>
<yellow>}</yellow>

static int nonpaging_sync_page(struct kvm_vcpu *vcpu,
			       struct kvm_mmu_page *sp)
{
	return -1;
<yellow>}</yellow>

#define KVM_PAGE_ARRAY_NR 16

struct kvm_mmu_pages {
	struct mmu_page_and_offset {
		struct kvm_mmu_page *sp;
		unsigned int idx;
	} page[KVM_PAGE_ARRAY_NR];
	unsigned int nr;
};

static int mmu_pages_add(struct kvm_mmu_pages *pvec, struct kvm_mmu_page *sp,
			 int idx)
{
	int i;

<yellow>	if (sp->unsync)</yellow>
<yellow>		for (i=0; i < pvec->nr; i++)</yellow>
<yellow>			if (pvec->page[i].sp == sp)</yellow>
				return 0;

<yellow>	pvec->page[pvec->nr].sp = sp;</yellow>
	pvec-&gt;page[pvec-&gt;nr].idx = idx;
	pvec-&gt;nr++;
	return (pvec-&gt;nr == KVM_PAGE_ARRAY_NR);
<yellow>}</yellow>

static inline void clear_unsync_child_bit(struct kvm_mmu_page *sp, int idx)
{
<yellow>	--sp->unsync_children;</yellow>
<yellow>	WARN_ON((int)sp->unsync_children < 0);</yellow>
<yellow>	__clear_bit(idx, sp->unsync_child_bitmap);</yellow>
}

static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
			   struct kvm_mmu_pages *pvec)
{
	int i, ret, nr_unsync_leaf = 0;

<yellow>	for_each_set_bit(i, sp->unsync_child_bitmap, 512) {</yellow>
		struct kvm_mmu_page *child;
<yellow>		u64 ent = sp->spt[i];</yellow>

<yellow>		if (!is_shadow_present_pte(ent) || is_large_pte(ent)) {</yellow>
<yellow>			clear_unsync_child_bit(sp, i);</yellow>
			continue;
		}

<yellow>		child = to_shadow_page(ent & SPTE_BASE_ADDR_MASK);</yellow>

		if (child-&gt;unsync_children) {
<yellow>			if (mmu_pages_add(pvec, child, i))</yellow>
				return -ENOSPC;

<yellow>			ret = __mmu_unsync_walk(child, pvec);</yellow>
			if (!ret) {
<yellow>				clear_unsync_child_bit(sp, i);</yellow>
				continue;
<yellow>			} else if (ret > 0) {</yellow>
<yellow>				nr_unsync_leaf += ret;</yellow>
			} else
				return ret;
<yellow>		} else if (child->unsync) {</yellow>
			nr_unsync_leaf++;
<yellow>			if (mmu_pages_add(pvec, child, i))</yellow>
				return -ENOSPC;
		} else
<yellow>			clear_unsync_child_bit(sp, i);</yellow>
	}

	return nr_unsync_leaf;
<yellow>}</yellow>

#define INVALID_INDEX (-1)

<yellow>static int mmu_unsync_walk(struct kvm_mmu_page *sp,</yellow>
			   struct kvm_mmu_pages *pvec)
{
	pvec-&gt;nr = 0;
<blue>	if (!sp->unsync_children)</blue>
		return 0;

<yellow>	mmu_pages_add(pvec, sp, INVALID_INDEX);</yellow>
	return __mmu_unsync_walk(sp, pvec);
}

static void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
{
<yellow>	WARN_ON(!sp->unsync);</yellow>
<yellow>	trace_kvm_mmu_sync_page(sp);</yellow>
<yellow>	sp->unsync = 0;</yellow>
	--kvm-&gt;stat.mmu_unsync;
}

static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
				     struct list_head *invalid_list);
static void kvm_mmu_commit_zap_page(struct kvm *kvm,
				    struct list_head *invalid_list);

static bool sp_has_gptes(struct kvm_mmu_page *sp)
{
<blue>	if (sp->role.direct)</blue>
		return false;

<blue>	if (sp->role.passthrough)</blue>
		return false;

	return true;
}

#define for_each_valid_sp(_kvm, _sp, _list)				\
	hlist_for_each_entry(_sp, _list, hash_link)			\
		if (is_obsolete_sp((_kvm), (_sp))) {			\
		} else

#define for_each_gfn_valid_sp_with_gptes(_kvm, _sp, _gfn)		\
	for_each_valid_sp(_kvm, _sp,					\
	  &amp;(_kvm)-&gt;arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)])	\
		if ((_sp)-&gt;gfn != (_gfn) || !sp_has_gptes(_sp)) {} else

static int kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
			 struct list_head *invalid_list)
{
<yellow>	int ret = vcpu->arch.mmu->sync_page(vcpu, sp);</yellow>

	if (ret &lt; 0)
		kvm_mmu_prepare_zap_page(vcpu-&gt;kvm, sp, invalid_list);
	return ret;
}

static bool kvm_mmu_remote_flush_or_zap(struct kvm *kvm,
					struct list_head *invalid_list,
					bool remote_flush)
{
<blue>	if (!remote_flush && list_empty(invalid_list))</blue>
		return false;

<yellow>	if (!list_empty(invalid_list))</yellow>
<yellow>		kvm_mmu_commit_zap_page(kvm, invalid_list);</yellow>
	else
<yellow>		kvm_flush_remote_tlbs(kvm);</yellow>
	return true;
}

<blue>static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)</blue>
{
<blue>	if (sp->role.invalid)</blue>
		return true;

	/* TDP MMU pages due not use the MMU generation. */
<blue>	return !sp->tdp_mmu_page &&</blue>
<blue>	       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);</blue>
}

struct mmu_page_path {
	struct kvm_mmu_page *parent[PT64_ROOT_MAX_LEVEL];
	unsigned int idx[PT64_ROOT_MAX_LEVEL];
};

#define for_each_sp(pvec, sp, parents, i)			\
		for (i = mmu_pages_first(&amp;pvec, &amp;parents);	\
			i &lt; pvec.nr &amp;&amp; ({ sp = pvec.page[i].sp; 1;});	\
			i = mmu_pages_next(&amp;pvec, &amp;parents, i))

static int mmu_pages_next(struct kvm_mmu_pages *pvec,
			  struct mmu_page_path *parents,
			  int i)
{
	int n;

<yellow>	for (n = i+1; n < pvec->nr; n++) {</yellow>
<yellow>		struct kvm_mmu_page *sp = pvec->page[n].sp;</yellow>
		unsigned idx = pvec-&gt;page[n].idx;
		int level = sp-&gt;role.level;

		parents-&gt;idx[level-1] = idx;
		if (level == PG_LEVEL_4K)
			break;

<yellow>		parents->parent[level-2] = sp;</yellow>
	}

	return n;
}

static int mmu_pages_first(struct kvm_mmu_pages *pvec,
			   struct mmu_page_path *parents)
{
	struct kvm_mmu_page *sp;
	int level;

<yellow>	if (pvec->nr == 0)</yellow>
		return 0;

<yellow>	WARN_ON(pvec->page[0].idx != INVALID_INDEX);</yellow>

<yellow>	sp = pvec->page[0].sp;</yellow>
	level = sp-&gt;role.level;
<yellow>	WARN_ON(level == PG_LEVEL_4K);</yellow>

<yellow>	parents->parent[level-2] = sp;</yellow>

	/* Also set up a sentinel.  Further entries in pvec are all
	 * children of sp, so this element is never overwritten.
	 */
	parents-&gt;parent[level-1] = NULL;
<yellow>	return mmu_pages_next(pvec, parents, 0);</yellow>
<yellow>}</yellow>

static void mmu_pages_clear_parents(struct mmu_page_path *parents)
{
	struct kvm_mmu_page *sp;
	unsigned int level = 0;

	do {
<yellow>		unsigned int idx = parents->idx[level];</yellow>
		sp = parents-&gt;parent[level];
		if (!sp)
			return;

<yellow>		WARN_ON(idx == INVALID_INDEX);</yellow>
<yellow>		clear_unsync_child_bit(sp, idx);</yellow>
		level++;
	} while (!sp-&gt;unsync_children);
}

static int mmu_sync_children(struct kvm_vcpu *vcpu,
			     struct kvm_mmu_page *parent, bool can_yield)
<yellow>{</yellow>
	int i;
	struct kvm_mmu_page *sp;
	struct mmu_page_path parents;
	struct kvm_mmu_pages pages;
<yellow>	LIST_HEAD(invalid_list);</yellow>
	bool flush = false;

<yellow>	while (mmu_unsync_walk(parent, &pages)) {</yellow>
		bool protected = false;

<yellow>		for_each_sp(pages, sp, parents, i)</yellow>
			protected |= kvm_vcpu_write_protect_gfn(vcpu, sp-&gt;gfn);

<yellow>		if (protected) {</yellow>
<yellow>			kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, true);</yellow>
			flush = false;
		}

<yellow>		for_each_sp(pages, sp, parents, i) {</yellow>
			kvm_unlink_unsync_page(vcpu-&gt;kvm, sp);
<yellow>			flush |= kvm_sync_page(vcpu, sp, &invalid_list) > 0;</yellow>
<yellow>			mmu_pages_clear_parents(&parents);</yellow>
		}
<yellow>		if (need_resched() || rwlock_needbreak(&vcpu->kvm->mmu_lock)) {</yellow>
<yellow>			kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);</yellow>
<yellow>			if (!can_yield) {</yellow>
<yellow>				kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);</yellow>
				return -EINTR;
			}

<yellow>			cond_resched_rwlock_write(&vcpu->kvm->mmu_lock);</yellow>
			flush = false;
		}
	}

<yellow>	kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);</yellow>
	return 0;
}

static void __clear_sp_write_flooding_count(struct kvm_mmu_page *sp)
{
<blue>	atomic_set(&sp->write_flooding_count,  0);</blue>
}

static void clear_sp_write_flooding_count(u64 *spte)
{
<blue>	__clear_sp_write_flooding_count(sptep_to_sp(spte));</blue>
}

/*
 * The vCPU is required when finding indirect shadow pages; the shadow
 * page may already exist and syncing it needs the vCPU pointer in
 * order to read guest page tables.  Direct shadow pages are never
 * unsync, thus @vcpu can be NULL if @role.direct is true.
 */
static struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,
						     struct kvm_vcpu *vcpu,
						     gfn_t gfn,
						     struct hlist_head *sp_list,
						     union kvm_mmu_page_role role)
{
	struct kvm_mmu_page *sp;
	int ret;
	int collisions = 0;
	LIST_HEAD(invalid_list);

<blue>	for_each_valid_sp(kvm, sp, sp_list) {</blue>
<blue>		if (sp->gfn != gfn) {</blue>
<yellow>			collisions++;</yellow>
			continue;
		}

<blue>		if (sp->role.word != role.word) {</blue>
			/*
			 * If the guest is creating an upper-level page, zap
			 * unsync pages for the same gfn.  While it&#x27;s possible
			 * the guest is using recursive page tables, in all
			 * likelihood the guest has stopped using the unsync
			 * page and is installing a completely unrelated page.
			 * Unsync pages must not be left as is, because the new
			 * upper-level page will be write-protected.
			 */
<blue>			if (role.level > PG_LEVEL_4K && sp->unsync)</blue>
<yellow>				kvm_mmu_prepare_zap_page(kvm, sp,</yellow>
							 &amp;invalid_list);
			continue;
		}

		/* unsync and write-flooding only apply to indirect SPs. */
<blue>		if (sp->role.direct)</blue>
			goto out;

<blue>		if (sp->unsync) {</blue>
<yellow>			if (KVM_BUG_ON(!vcpu, kvm))</yellow>
				break;

			/*
			 * The page is good, but is stale.  kvm_sync_page does
			 * get the latest guest state, but (unlike mmu_unsync_children)
			 * it doesn&#x27;t write-protect the page or mark it synchronized!
			 * This way the validity of the mapping is ensured, but the
			 * overhead of write protection is not incurred until the
			 * guest invalidates the TLB mapping.  This allows multiple
			 * SPs for a single gfn to be unsync.
			 *
			 * If the sync fails, the page is zapped.  If so, break
			 * in order to rebuild it.
			 */
<yellow>			ret = kvm_sync_page(vcpu, sp, &invalid_list);</yellow>
			if (ret &lt; 0)
				break;

<yellow>			WARN_ON(!list_empty(&invalid_list));</yellow>
<yellow>			if (ret > 0)</yellow>
<yellow>				kvm_flush_remote_tlbs(kvm);</yellow>
		}

<blue>		__clear_sp_write_flooding_count(sp);</blue>

		goto out;
	}

	sp = NULL;
<blue>	++kvm->stat.mmu_cache_miss;</blue>

out:
<blue>	kvm_mmu_commit_zap_page(kvm, &invalid_list);</blue>

<blue>	if (collisions > kvm->stat.max_mmu_page_hash_collisions)</blue>
<yellow>		kvm->stat.max_mmu_page_hash_collisions = collisions;</yellow>
	return sp;
}

/* Caches used when allocating a new shadow page. */
struct shadow_page_caches {
	struct kvm_mmu_memory_cache *page_header_cache;
	struct kvm_mmu_memory_cache *shadow_page_cache;
	struct kvm_mmu_memory_cache *shadowed_info_cache;
};

static struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,
						      struct shadow_page_caches *caches,
						      gfn_t gfn,
						      struct hlist_head *sp_list,
						      union kvm_mmu_page_role role)
{
	struct kvm_mmu_page *sp;

<blue>	sp = kvm_mmu_memory_cache_alloc(caches->page_header_cache);</blue>
	sp-&gt;spt = kvm_mmu_memory_cache_alloc(caches-&gt;shadow_page_cache);
	if (!role.direct)
<blue>		sp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);</blue>

<blue>	set_page_private(virt_to_page(sp->spt), (unsigned long)sp);</blue>

	/*
	 * active_mmu_pages must be a FIFO list, as kvm_zap_obsolete_pages()
	 * depends on valid pages being added to the head of the list.  See
	 * comments in kvm_zap_obsolete_pages().
	 */
	sp-&gt;mmu_valid_gen = kvm-&gt;arch.mmu_valid_gen;
	list_add(&amp;sp-&gt;link, &amp;kvm-&gt;arch.active_mmu_pages);
<blue>	kvm_account_mmu_page(kvm, sp);</blue>

	sp-&gt;gfn = gfn;
	sp-&gt;role = role;
<blue>	hlist_add_head(&sp->hash_link, sp_list);</blue>
<blue>	if (sp_has_gptes(sp))</blue>
<blue>		account_shadowed(kvm, sp);</blue>

	return sp;
}

/* Note, @vcpu may be NULL if @role.direct is true; see kvm_mmu_find_shadow_page. */
static struct kvm_mmu_page *__kvm_mmu_get_shadow_page(struct kvm *kvm,
						      struct kvm_vcpu *vcpu,
						      struct shadow_page_caches *caches,
						      gfn_t gfn,
						      union kvm_mmu_page_role role)
<blue>{</blue>
	struct hlist_head *sp_list;
	struct kvm_mmu_page *sp;
	bool created = false;

<blue>	sp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];</blue>

<blue>	sp = kvm_mmu_find_shadow_page(kvm, vcpu, gfn, sp_list, role);</blue>
<blue>	if (!sp) {</blue>
		created = true;
<blue>		sp = kvm_mmu_alloc_shadow_page(kvm, caches, gfn, sp_list, role);</blue>
	}

<blue>	trace_kvm_mmu_get_page(sp, created);</blue>
	return sp;
}

static struct kvm_mmu_page *kvm_mmu_get_shadow_page(struct kvm_vcpu *vcpu,
						    gfn_t gfn,
						    union kvm_mmu_page_role role)
{
	struct shadow_page_caches caches = {
<blue>		.page_header_cache = &vcpu->arch.mmu_page_header_cache,</blue>
		.shadow_page_cache = &amp;vcpu-&gt;arch.mmu_shadow_page_cache,
		.shadowed_info_cache = &amp;vcpu-&gt;arch.mmu_shadowed_info_cache,
	};

	return __kvm_mmu_get_shadow_page(vcpu-&gt;kvm, vcpu, &amp;caches, gfn, role);
}

static union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,
						  unsigned int access)
{
<blue>	struct kvm_mmu_page *parent_sp = sptep_to_sp(sptep);</blue>
	union kvm_mmu_page_role role;

	role = parent_sp-&gt;role;
	role.level--;
	role.access = access;
	role.direct = direct;
	role.passthrough = 0;

	/*
	 * If the guest has 4-byte PTEs then that means it&#x27;s using 32-bit,
	 * 2-level, non-PAE paging. KVM shadows such guests with PAE paging
	 * (i.e. 8-byte PTEs). The difference in PTE size means that KVM must
	 * shadow each guest page table with multiple shadow page tables, which
	 * requires extra bookkeeping in the role.
	 *
	 * Specifically, to shadow the guest&#x27;s page directory (which covers a
	 * 4GiB address space), KVM uses 4 PAE page directories, each mapping
	 * 1GiB of the address space. @role.quadrant encodes which quarter of
	 * the address space each maps.
	 *
	 * To shadow the guest&#x27;s page tables (which each map a 4MiB region), KVM
	 * uses 2 PAE page tables, each mapping a 2MiB region. For these,
	 * @role.quadrant encodes which half of the region they map.
	 *
	 * Concretely, a 4-byte PDE consumes bits 31:22, while an 8-byte PDE
	 * consumes bits 29:21.  To consume bits 31:30, KVM&#x27;s uses 4 shadow
	 * PDPTEs; those 4 PAE page directories are pre-allocated and their
	 * quadrant is assigned in mmu_alloc_root().   A 4-byte PTE consumes
	 * bits 21:12, while an 8-byte PTE consumes bits 20:12.  To consume
	 * bit 21 in the PTE (the child here), KVM propagates that bit to the
	 * quadrant, i.e. sets quadrant to &#x27;0&#x27; or &#x27;1&#x27;.  The parent 8-byte PDE
	 * covers bit 21 (see above), thus the quadrant is calculated from the
	 * _least_ significant bit of the PDE index.
	 */
	if (role.has_4_byte_gpte) {
<yellow>		WARN_ON_ONCE(role.level != PG_LEVEL_4K);</yellow>
<yellow>		role.quadrant = spte_index(sptep) & 1;</yellow>
	}

<blue>	return role;</blue>
}

static struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,
						 u64 *sptep, gfn_t gfn,
						 bool direct, unsigned int access)
<blue>{</blue>
	union kvm_mmu_page_role role;

<blue>	if (is_shadow_present_pte(*sptep) && !is_large_pte(*sptep))</blue>
		return ERR_PTR(-EEXIST);

<blue>	role = kvm_mmu_child_role(sptep, direct, access);</blue>
	return kvm_mmu_get_shadow_page(vcpu, gfn, role);
}

static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
					struct kvm_vcpu *vcpu, hpa_t root,
					u64 addr)
{
<blue>	iterator->addr = addr;</blue>
	iterator-&gt;shadow_addr = root;
	iterator-&gt;level = vcpu-&gt;arch.mmu-&gt;root_role.level;

	if (iterator-&gt;level &gt;= PT64_ROOT_4LEVEL &amp;&amp;
<blue>	    vcpu->arch.mmu->cpu_role.base.level < PT64_ROOT_4LEVEL &&</blue>
<yellow>	    !vcpu->arch.mmu->root_role.direct)</yellow>
<yellow>		iterator->level = PT32E_ROOT_LEVEL;</yellow>

<yellow>	if (iterator->level == PT32E_ROOT_LEVEL) {</yellow>
		/*
		 * prev_root is currently only used for 64-bit hosts. So only
		 * the active root_hpa is valid here.
		 */
<yellow>		BUG_ON(root != vcpu->arch.mmu->root.hpa);</yellow>

		iterator-&gt;shadow_addr
<yellow>			= vcpu->arch.mmu->pae_root[(addr >> 30) & 3];</yellow>
		iterator-&gt;shadow_addr &amp;= SPTE_BASE_ADDR_MASK;
<yellow>		--iterator->level;</yellow>
		if (!iterator-&gt;shadow_addr)
<yellow>			iterator->level = 0;</yellow>
	}
<blue>}</blue>

static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
			     struct kvm_vcpu *vcpu, u64 addr)
{
<blue>	shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root.hpa,</blue>
				    addr);
}

static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
{
<blue>	if (iterator->level < PG_LEVEL_4K)</blue>
		return false;

<blue>	iterator->index = SPTE_INDEX(iterator->addr, iterator->level);</blue>
	iterator-&gt;sptep	= ((u64 *)__va(iterator-&gt;shadow_addr)) + iterator-&gt;index;
	return true;
<blue>}</blue>

static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
			       u64 spte)
{
<blue>	if (!is_shadow_present_pte(spte) || is_last_spte(spte, iterator->level)) {</blue>
<blue>		iterator->level = 0;</blue>
		return;
	}

<blue>	iterator->shadow_addr = spte & SPTE_BASE_ADDR_MASK;</blue>
<blue>	--iterator->level;</blue>
}

static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
{
<blue>	__shadow_walk_next(iterator, *iterator->sptep);</blue>
}

static void __link_shadow_page(struct kvm *kvm,
			       struct kvm_mmu_memory_cache *cache, u64 *sptep,
			       struct kvm_mmu_page *sp, bool flush)
<blue>{</blue>
	u64 spte;

	BUILD_BUG_ON(VMX_EPT_WRITABLE_MASK != PT_WRITABLE_MASK);

	/*
	 * If an SPTE is present already, it must be a leaf and therefore
	 * a large one.  Drop it, and flush the TLB if needed, before
	 * installing sp.
	 */
<blue>	if (is_shadow_present_pte(*sptep))</blue>
<yellow>		drop_large_spte(kvm, sptep, flush);</yellow>

<blue>	spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));</blue>

<blue>	mmu_spte_set(sptep, spte);</blue>

<blue>	mmu_page_add_parent_pte(cache, sp, sptep);</blue>

<blue>	if (sp->unsync_children || sp->unsync)</blue>
<yellow>		mark_unsync(sptep);</yellow>
}

static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
			     struct kvm_mmu_page *sp)
{
<blue>	__link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);</blue>
}

static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
				   unsigned direct_access)
<blue>{</blue>
<blue>	if (is_shadow_present_pte(*sptep) && !is_large_pte(*sptep)) {</blue>
		struct kvm_mmu_page *child;

		/*
		 * For the direct sp, if the guest pte&#x27;s dirty bit
		 * changed form clean to dirty, it will corrupt the
		 * sp&#x27;s access: allow writable in the read-only sp,
		 * so we should update the spte at this point to get
		 * a new sp with the correct access.
		 */
<blue>		child = to_shadow_page(*sptep & SPTE_BASE_ADDR_MASK);</blue>
		if (child-&gt;role.access == direct_access)
			return;

<blue>		drop_parent_pte(child, sptep);</blue>
<blue>		kvm_flush_remote_tlbs_with_address(vcpu->kvm, child->gfn, 1);</blue>
	}
}

/* Returns the number of zapped non-leaf child shadow pages. */
static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
			    u64 *spte, struct list_head *invalid_list)
<blue>{</blue>
	u64 pte;
	struct kvm_mmu_page *child;

<blue>	pte = *spte;</blue>
	if (is_shadow_present_pte(pte)) {
<blue>		if (is_last_spte(pte, sp->role.level)) {</blue>
<blue>			drop_spte(kvm, spte);</blue>
		} else {
<blue>			child = to_shadow_page(pte & SPTE_BASE_ADDR_MASK);</blue>
			drop_parent_pte(child, spte);

			/*
			 * Recursively zap nested TDP SPs, parentless SPs are
			 * unlikely to be used again in the near future.  This
			 * avoids retaining a large number of stale nested SPs.
			 */
<blue>			if (tdp_enabled && invalid_list &&</blue>
<blue>			    child->role.guest_mode && !child->parent_ptes.val)</blue>
<blue>				return kvm_mmu_prepare_zap_page(kvm, child,</blue>
								invalid_list);
		}
<blue>	} else if (is_mmio_spte(pte)) {</blue>
<yellow>		mmu_spte_clear_no_track(spte);</yellow>
	}
	return 0;
}

static int kvm_mmu_page_unlink_children(struct kvm *kvm,
					struct kvm_mmu_page *sp,
					struct list_head *invalid_list)
{
	int zapped = 0;
	unsigned i;

	for (i = 0; i &lt; SPTE_ENT_PER_PAGE; ++i)
<blue>		zapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);</blue>

	return zapped;
}

static void kvm_mmu_unlink_parents(struct kvm_mmu_page *sp)
{
	u64 *sptep;
	struct rmap_iterator iter;

<yellow>	while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))</yellow>
<yellow>		drop_parent_pte(sp, sptep);</yellow>
}

static int mmu_zap_unsync_children(struct kvm *kvm,
				   struct kvm_mmu_page *parent,
				   struct list_head *invalid_list)
<blue>{</blue>
	int i, zapped = 0;
	struct mmu_page_path parents;
	struct kvm_mmu_pages pages;

<blue>	if (parent->role.level == PG_LEVEL_4K)</blue>
		return 0;

<blue>	while (mmu_unsync_walk(parent, &pages)) {</blue>
		struct kvm_mmu_page *sp;

<yellow>		for_each_sp(pages, sp, parents, i) {</yellow>
			kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
<yellow>			mmu_pages_clear_parents(&parents);</yellow>
			zapped++;
		}
	}

	return zapped;
}

static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
				       struct kvm_mmu_page *sp,
				       struct list_head *invalid_list,
				       int *nr_zapped)
{
	bool list_unstable, zapped_root = false;

	lockdep_assert_held_write(&amp;kvm-&gt;mmu_lock);
<blue>	trace_kvm_mmu_prepare_zap_page(sp);</blue>
<blue>	++kvm->stat.mmu_shadow_zapped;</blue>
	*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);
<blue>	*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);</blue>
<yellow>	kvm_mmu_unlink_parents(sp);</yellow>

	/* Zapping children means active_mmu_pages has become unstable. */
<blue>	list_unstable = *nr_zapped;</blue>

<blue>	if (!sp->role.invalid && sp_has_gptes(sp))</blue>
<blue>		unaccount_shadowed(kvm, sp);</blue>

<blue>	if (sp->unsync)</blue>
<yellow>		kvm_unlink_unsync_page(kvm, sp);</yellow>
<blue>	if (!sp->root_count) {</blue>
		/* Count self */
<blue>		(*nr_zapped)++;</blue>

		/*
		 * Already invalid pages (previously active roots) are not on
		 * the active page list.  See list_del() in the &quot;else&quot; case of
		 * !sp-&gt;root_count.
		 */
		if (sp-&gt;role.invalid)
<blue>			list_add(&sp->link, invalid_list);</blue>
		else
<blue>			list_move(&sp->link, invalid_list);</blue>
<blue>		kvm_unaccount_mmu_page(kvm, sp);</blue>
	} else {
		/*
		 * Remove the active root from the active page list, the root
		 * will be explicitly freed when the root_count hits zero.
		 */
<blue>		list_del(&sp->link);</blue>

		/*
		 * Obsolete pages cannot be used on any vCPUs, see the comment
		 * in kvm_mmu_zap_all_fast().  Note, is_obsolete_sp() also
		 * treats invalid shadow pages as being obsolete.
		 */
<blue>		zapped_root = !is_obsolete_sp(kvm, sp);</blue>
	}

<blue>	if (sp->lpage_disallowed)</blue>
<yellow>		unaccount_huge_nx_page(kvm, sp);</yellow>

<blue>	sp->role.invalid = 1;</blue>

	/*
	 * Make the request to free obsolete roots after marking the root
	 * invalid, otherwise other vCPUs may not see it as invalid.
	 */
	if (zapped_root)
<blue>		kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);</blue>
	return list_unstable;
<blue>}</blue>

static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
				     struct list_head *invalid_list)
{
	int nr_zapped;

<blue>	__kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);</blue>
	return nr_zapped;
}

static void kvm_mmu_commit_zap_page(struct kvm *kvm,
				    struct list_head *invalid_list)
{
	struct kvm_mmu_page *sp, *nsp;

<blue>	if (list_empty(invalid_list))</blue>
		return;

	/*
	 * We need to make sure everyone sees our modifications to
	 * the page tables and see changes to vcpu-&gt;mode here. The barrier
	 * in the kvm_flush_remote_tlbs() achieves this. This pairs
	 * with vcpu_enter_guest and walk_shadow_page_lockless_begin/end.
	 *
	 * In addition, kvm_flush_remote_tlbs waits for all vcpus to exit
	 * guest mode and/or lockless shadow page table walks.
	 */
<blue>	kvm_flush_remote_tlbs(kvm);</blue>

	list_for_each_entry_safe(sp, nsp, invalid_list, link) {
<blue>		WARN_ON(!sp->role.invalid || sp->root_count);</blue>
<blue>		kvm_mmu_free_shadow_page(sp);</blue>
	}
<blue>}</blue>

static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
						  unsigned long nr_to_zap)
<yellow>{</yellow>
	unsigned long total_zapped = 0;
	struct kvm_mmu_page *sp, *tmp;
<yellow>	LIST_HEAD(invalid_list);</yellow>
	bool unstable;
	int nr_zapped;

	if (list_empty(&amp;kvm-&gt;arch.active_mmu_pages))
		return 0;

restart:
<yellow>	list_for_each_entry_safe_reverse(sp, tmp, &kvm->arch.active_mmu_pages, link) {</yellow>
		/*
		 * Don&#x27;t zap active root pages, the page itself can&#x27;t be freed
		 * and zapping it will just force vCPUs to realloc and reload.
		 */
<yellow>		if (sp->root_count)</yellow>
			continue;

<yellow>		unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,</yellow>
						      &amp;nr_zapped);
		total_zapped += nr_zapped;
		if (total_zapped &gt;= nr_to_zap)
			break;

<yellow>		if (unstable)</yellow>
			goto restart;
	}

<yellow>	kvm_mmu_commit_zap_page(kvm, &invalid_list);</yellow>

<yellow>	kvm->stat.mmu_recycled += total_zapped;</yellow>
	return total_zapped;
}

static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
{
	if (kvm-&gt;arch.n_max_mmu_pages &gt; kvm-&gt;arch.n_used_mmu_pages)
<blue>		return kvm->arch.n_max_mmu_pages -</blue>
			kvm-&gt;arch.n_used_mmu_pages;

	return 0;
}

static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
{
<blue>	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);</blue>

	if (likely(avail &gt;= KVM_MIN_FREE_MMU_PAGES))
		return 0;

<yellow>	kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);</yellow>

	/*
	 * Note, this check is intentionally soft, it only guarantees that one
	 * page is available, while the caller may end up allocating as many as
	 * four pages, e.g. for PAE roots or for 5-level paging.  Temporarily
	 * exceeding the (arbitrary by default) limit will not harm the host,
	 * being too aggressive may unnecessarily kill the guest, and getting an
	 * exact count is far more trouble than it&#x27;s worth, especially in the
	 * page fault paths.
	 */
	if (!kvm_mmu_available_pages(vcpu-&gt;kvm))
		return -ENOSPC;
	return 0;
}

/*
 * Changing the number of mmu pages allocated to the vm
 * Note: if goal_nr_mmu_pages is too small, you will get dead lock
 */
void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
{
<blue>	write_lock(&kvm->mmu_lock);</blue>

	if (kvm-&gt;arch.n_used_mmu_pages &gt; goal_nr_mmu_pages) {
<yellow>		kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -</yellow>
						  goal_nr_mmu_pages);

		goal_nr_mmu_pages = kvm-&gt;arch.n_used_mmu_pages;
	}

<blue>	kvm->arch.n_max_mmu_pages = goal_nr_mmu_pages;</blue>

	write_unlock(&amp;kvm-&gt;mmu_lock);
}

int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)
{
	struct kvm_mmu_page *sp;
<blue>	LIST_HEAD(invalid_list);</blue>
	int r;

	pgprintk(&quot;%s: looking for gfn %llx\n&quot;, __func__, gfn);
	r = 0;
	write_lock(&amp;kvm-&gt;mmu_lock);
<blue>	for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {</blue>
		pgprintk(&quot;%s: gfn %llx role %x\n&quot;, __func__, gfn,
			 sp-&gt;role.word);
		r = 1;
<blue>		kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);</blue>
	}
<blue>	kvm_mmu_commit_zap_page(kvm, &invalid_list);</blue>
<blue>	write_unlock(&kvm->mmu_lock);</blue>

	return r;
}

static int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva)
{
	gpa_t gpa;
	int r;

<yellow>	if (vcpu->arch.mmu->root_role.direct)</yellow>
		return 0;

<yellow>	gpa = kvm_mmu_gva_to_gpa_read(vcpu, gva, NULL);</yellow>

	r = kvm_mmu_unprotect_page(vcpu-&gt;kvm, gpa &gt;&gt; PAGE_SHIFT);

	return r;
}

static void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
{
<yellow>	trace_kvm_mmu_unsync_page(sp);</yellow>
<yellow>	++kvm->stat.mmu_unsync;</yellow>
	sp-&gt;unsync = 1;

<yellow>	kvm_mmu_mark_parents_unsync(sp);</yellow>
}

/*
 * Attempt to unsync any shadow pages that can be reached by the specified gfn,
 * KVM is creating a writable mapping for said gfn.  Returns 0 if all pages
 * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must
 * be write-protected.
 */
int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
			    gfn_t gfn, bool can_unsync, bool prefetch)
<blue>{</blue>
	struct kvm_mmu_page *sp;
	bool locked = false;

	/*
	 * Force write-protection if the page is being tracked.  Note, the page
	 * track machinery is used to write-protect upper-level shadow pages,
	 * i.e. this guards the role.level == 4K assertion below!
	 */
<blue>	if (kvm_slot_page_track_is_active(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE))</blue>
		return -EPERM;

	/*
	 * The page is not write-tracked, mark existing shadow pages unsync
	 * unless KVM is synchronizing an unsync SP (can_unsync = false).  In
	 * that case, KVM must complete emulation of the guest TLB flush before
	 * allowing shadow pages to become unsync (writable by the guest).
	 */
<blue>	for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {</blue>
<yellow>		if (!can_unsync)</yellow>
			return -EPERM;

<yellow>		if (sp->unsync)</yellow>
			continue;

<yellow>		if (prefetch)</yellow>
			return -EEXIST;

		/*
		 * TDP MMU page faults require an additional spinlock as they
		 * run with mmu_lock held for read, not write, and the unsync
		 * logic is not thread safe.  Take the spinklock regardless of
		 * the MMU type to avoid extra conditionals/parameters, there&#x27;s
		 * no meaningful penalty if mmu_lock is held for write.
		 */
<yellow>		if (!locked) {</yellow>
			locked = true;
<yellow>			spin_lock(&kvm->arch.mmu_unsync_pages_lock);</yellow>

			/*
			 * Recheck after taking the spinlock, a different vCPU
			 * may have since marked the page unsync.  A false
			 * positive on the unprotected check above is not
			 * possible as clearing sp-&gt;unsync _must_ hold mmu_lock
			 * for write, i.e. unsync cannot transition from 0-&gt;1
			 * while this CPU holds mmu_lock for read (or write).
			 */
<yellow>			if (READ_ONCE(sp->unsync))</yellow>
				continue;
		}

<yellow>		WARN_ON(sp->role.level != PG_LEVEL_4K);</yellow>
<yellow>		kvm_unsync_page(kvm, sp);</yellow>
	}
<blue>	if (locked)</blue>
<yellow>		spin_unlock(&kvm->arch.mmu_unsync_pages_lock);</yellow>

	/*
	 * We need to ensure that the marking of unsync pages is visible
	 * before the SPTE is updated to allow writes because
	 * kvm_mmu_sync_roots() checks the unsync flags without holding
	 * the MMU lock and so can race with this. If the SPTE was updated
	 * before the page had been marked as unsync-ed, something like the
	 * following could happen:
	 *
	 * CPU 1                    CPU 2
	 * ---------------------------------------------------------------------
	 * 1.2 Host updates SPTE
	 *     to be writable
	 *                      2.1 Guest writes a GPTE for GVA X.
	 *                          (GPTE being in the guest page table shadowed
	 *                           by the SP from CPU 1.)
	 *                          This reads SPTE during the page table walk.
	 *                          Since SPTE.W is read as 1, there is no
	 *                          fault.
	 *
	 *                      2.2 Guest issues TLB flush.
	 *                          That causes a VM Exit.
	 *
	 *                      2.3 Walking of unsync pages sees sp-&gt;unsync is
	 *                          false and skips the page.
	 *
	 *                      2.4 Guest accesses GVA X.
	 *                          Since the mapping in the SP was not updated,
	 *                          so the old mapping for GVA X incorrectly
	 *                          gets used.
	 * 1.1 Host marks SP
	 *     as unsync
	 *     (sp-&gt;unsync = true)
	 *
	 * The write barrier below ensures that 1.1 happens before 1.2 and thus
	 * the situation in 2.4 does not arise.  It pairs with the read barrier
	 * in is_unsync_root(), placed between 2.1&#x27;s load of SPTE.W and 2.3.
	 */
	smp_wmb();

<blue>	return 0;</blue>
}

static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
			u64 *sptep, unsigned int pte_access, gfn_t gfn,
			kvm_pfn_t pfn, struct kvm_page_fault *fault)
<blue>{</blue>
<blue>	struct kvm_mmu_page *sp = sptep_to_sp(sptep);</blue>
	int level = sp-&gt;role.level;
	int was_rmapped = 0;
	int ret = RET_PF_FIXED;
	bool flush = false;
	bool wrprot;
	u64 spte;

	/* Prefetching always gets a writable pfn.  */
<blue>	bool host_writable = !fault || fault->map_writable;</blue>
<blue>	bool prefetch = !fault || fault->prefetch;</blue>
<blue>	bool write_fault = fault && fault->write;</blue>

	pgprintk(&quot;%s: spte %llx write_fault %d gfn %llx\n&quot;, __func__,
		 *sptep, write_fault, gfn);

<blue>	if (unlikely(is_noslot_pfn(pfn))) {</blue>
<blue>		vcpu->stat.pf_mmio_spte_created++;</blue>
		mark_mmio_spte(vcpu, sptep, gfn, pte_access);
		return RET_PF_EMULATE;
	}

<blue>	if (is_shadow_present_pte(*sptep)) {</blue>
		/*
		 * If we overwrite a PTE page pointer with a 2MB PMD, unlink
		 * the parent of the now unreachable PTE.
		 */
<blue>		if (level > PG_LEVEL_4K && !is_large_pte(*sptep)) {</blue>
			struct kvm_mmu_page *child;
			u64 pte = *sptep;

<blue>			child = to_shadow_page(pte & SPTE_BASE_ADDR_MASK);</blue>
			drop_parent_pte(child, sptep);
			flush = true;
<blue>		} else if (pfn != spte_to_pfn(*sptep)) {</blue>
			pgprintk(&quot;hfn old %llx new %llx\n&quot;,
				 spte_to_pfn(*sptep), pfn);
<yellow>			drop_spte(vcpu->kvm, sptep);</yellow>
			flush = true;
		} else
			was_rmapped = 1;
	}

<blue>	wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,</blue>
			   true, host_writable, &amp;spte);

	if (*sptep == spte) {
		ret = RET_PF_SPURIOUS;
	} else {
<blue>		flush |= mmu_spte_update(sptep, spte);</blue>
<yellow>		trace_kvm_mmu_set_spte(level, gfn, sptep);</yellow>
	}

<blue>	if (wrprot) {</blue>
<yellow>		if (write_fault)</yellow>
			ret = RET_PF_EMULATE;
	}

<blue>	if (flush)</blue>
<yellow>		kvm_flush_remote_tlbs_with_address(vcpu->kvm, gfn,</yellow>
<yellow>				KVM_PAGES_PER_HPAGE(level));</yellow>

	pgprintk(&quot;%s: setting spte %llx\n&quot;, __func__, *sptep);

<blue>	if (!was_rmapped) {</blue>
<blue>		WARN_ON_ONCE(ret == RET_PF_SPURIOUS);</blue>
<blue>		rmap_add(vcpu, slot, sptep, gfn, pte_access);</blue>
	} else {
		/* Already rmapped but the pte_access bits may have changed. */
<blue>		kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);</blue>
	}

	return ret;
}

static int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
				    struct kvm_mmu_page *sp,
				    u64 *start, u64 *end)
<blue>{</blue>
	struct page *pages[PTE_PREFETCH_NUM];
	struct kvm_memory_slot *slot;
<blue>	unsigned int access = sp->role.access;</blue>
	int i, ret;
	gfn_t gfn;

	gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
<blue>	slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK);</blue>
	if (!slot)
		return -1;

<blue>	ret = gfn_to_page_many_atomic(slot, gfn, pages, end - start);</blue>
	if (ret &lt;= 0)
		return -1;

<blue>	for (i = 0; i < ret; i++, gfn++, start++) {</blue>
		mmu_set_spte(vcpu, slot, start, access, gfn,
<blue>			     page_to_pfn(pages[i]), NULL);</blue>
<blue>		put_page(pages[i]);</blue>
	}

	return 0;
}

static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
				  struct kvm_mmu_page *sp, u64 *sptep)
{
	u64 *spte, *start = NULL;
	int i;

<blue>	WARN_ON(!sp->role.direct);</blue>

<blue>	i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);</blue>
	spte = sp-&gt;spt + i;

<blue>	for (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {</blue>
<blue>		if (is_shadow_present_pte(*spte) || spte == sptep) {</blue>
<blue>			if (!start)</blue>
				continue;
<blue>			if (direct_pte_prefetch_many(vcpu, sp, start, spte) < 0)</blue>
				return;
			start = NULL;
<blue>		} else if (!start)</blue>
			start = spte;
	}
<blue>	if (start)</blue>
<blue>		direct_pte_prefetch_many(vcpu, sp, start, spte);</blue>
<blue>}</blue>

static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
{
	struct kvm_mmu_page *sp;

<yellow>	sp = sptep_to_sp(sptep);</yellow>

	/*
	 * Without accessed bits, there&#x27;s no way to distinguish between
	 * actually accessed translations and prefetched, so disable pte
	 * prefetch if accessed bits aren&#x27;t available.
	 */
	if (sp_ad_disabled(sp))
		return;

<yellow>	if (sp->role.level > PG_LEVEL_4K)</yellow>
		return;

	/*
	 * If addresses are being invalidated, skip prefetching to avoid
	 * accidentally prefetching those addresses.
	 */
<yellow>	if (unlikely(vcpu->kvm->mmu_invalidate_in_progress))</yellow>
		return;

<yellow>	__direct_pte_prefetch(vcpu, sp, sptep);</yellow>
}

/*
 * Lookup the mapping level for @gfn in the current mm.
 *
 * WARNING!  Use of host_pfn_mapping_level() requires the caller and the end
 * consumer to be tied into KVM&#x27;s handlers for MMU notifier events!
 *
 * There are several ways to safely use this helper:
 *
 * - Check mmu_invalidate_retry_hva() after grabbing the mapping level, before
 *   consuming it.  In this case, mmu_lock doesn&#x27;t need to be held during the
 *   lookup, but it does need to be held while checking the MMU notifier.
 *
 * - Hold mmu_lock AND ensure there is no in-progress MMU notifier invalidation
 *   event for the hva.  This can be done by explicit checking the MMU notifier
 *   or by ensuring that KVM already has a valid mapping that covers the hva.
 *
 * - Do not use the result to install new mappings, e.g. use the host mapping
 *   level only to decide whether or not to zap an entry.  In this case, it&#x27;s
 *   not required to hold mmu_lock (though it&#x27;s highly likely the caller will
 *   want to hold mmu_lock anyways, e.g. to modify SPTEs).
 *
 * Note!  The lookup can still race with modifications to host page tables, but
 * the above &quot;rules&quot; ensure KVM will not _consume_ the result of the walk if a
 * race with the primary MMU occurs.
 */
static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,
				  const struct kvm_memory_slot *slot)
{
	int level = PG_LEVEL_4K;
	unsigned long hva;
	unsigned long flags;
	pgd_t pgd;
	p4d_t p4d;
	pud_t pud;
	pmd_t pmd;

	/*
	 * Note, using the already-retrieved memslot and __gfn_to_hva_memslot()
	 * is not solely for performance, it&#x27;s also necessary to avoid the
	 * &quot;writable&quot; check in __gfn_to_hva_many(), which will always fail on
	 * read-only memslots due to gfn_to_hva() assuming writes.  Earlier
	 * page fault steps have already verified the guest isn&#x27;t writing a
	 * read-only memslot.
	 */
<blue>	hva = __gfn_to_hva_memslot(slot, gfn);</blue>

	/*
	 * Disable IRQs to prevent concurrent tear down of host page tables,
	 * e.g. if the primary MMU promotes a P*D to a huge page and then frees
	 * the original page table.
	 */
	local_irq_save(flags);

	/*
	 * Read each entry once.  As above, a non-leaf entry can be promoted to
	 * a huge page _during_ this walk.  Re-reading the entry could send the
	 * walk into the weeks, e.g. p*d_large() returns false (sees the old
	 * value) and then p*d_offset() walks into the target huge page instead
	 * of the old page table (sees the new value).
	 */
<blue>	pgd = READ_ONCE(*pgd_offset(kvm->mm, hva));</blue>
<yellow>	if (pgd_none(pgd))</yellow>
		goto out;

<blue>	p4d = READ_ONCE(*p4d_offset(&pgd, hva));</blue>
<blue>	if (p4d_none(p4d) || !p4d_present(p4d))</blue>
		goto out;

<blue>	pud = READ_ONCE(*pud_offset(&p4d, hva));</blue>
<blue>	if (pud_none(pud) || !pud_present(pud))</blue>
		goto out;

<blue>	if (pud_large(pud)) {</blue>
		level = PG_LEVEL_1G;
		goto out;
	}

<blue>	pmd = READ_ONCE(*pmd_offset(&pud, hva));</blue>
<blue>	if (pmd_none(pmd) || !pmd_present(pmd))</blue>
		goto out;

<blue>	if (pmd_large(pmd))</blue>
		level = PG_LEVEL_2M;

out:
<blue>	local_irq_restore(flags);</blue>
	return level;
}

int kvm_mmu_max_mapping_level(struct kvm *kvm,
			      const struct kvm_memory_slot *slot, gfn_t gfn,
			      int max_level)
<blue>{</blue>
	struct kvm_lpage_info *linfo;
	int host_level;

<blue>	max_level = min(max_level, max_huge_page_level);</blue>
<blue>	for ( ; max_level > PG_LEVEL_4K; max_level--) {</blue>
<blue>		linfo = lpage_info_slot(gfn, slot, max_level);</blue>
		if (!linfo-&gt;disallow_lpage)
			break;
	}

<blue>	if (max_level == PG_LEVEL_4K)</blue>
		return PG_LEVEL_4K;

<blue>	host_level = host_pfn_mapping_level(kvm, gfn, slot);</blue>
<blue>	return min(host_level, max_level);</blue>
}

void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
{
<blue>	struct kvm_memory_slot *slot = fault->slot;</blue>
	kvm_pfn_t mask;

<blue>	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;</blue>

	if (unlikely(fault-&gt;max_level == PG_LEVEL_4K))
		return;

<blue>	if (is_error_noslot_pfn(fault->pfn))</blue>
		return;

<blue>	if (kvm_slot_dirty_track_enabled(slot))</blue>
		return;

	/*
	 * Enforce the iTLB multihit workaround after capturing the requested
	 * level, which will be used to do precise, accurate accounting.
	 */
<blue>	fault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot,</blue>
						     fault-&gt;gfn, fault-&gt;max_level);
<blue>	if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)</blue>
		return;

	/*
	 * mmu_invalidate_retry() was successful and mmu_lock is held, so
	 * the pmd can&#x27;t be split from under us.
	 */
<blue>	fault->goal_level = fault->req_level;</blue>
<blue>	mask = KVM_PAGES_PER_HPAGE(fault->goal_level) - 1;</blue>
	VM_BUG_ON((fault-&gt;gfn &amp; mask) != (fault-&gt;pfn &amp; mask));
	fault-&gt;pfn &amp;= ~mask;
<blue>}</blue>

<yellow>void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)</yellow>
{
<yellow>	if (cur_level > PG_LEVEL_4K &&</yellow>
<yellow>	    cur_level == fault->goal_level &&</yellow>
<yellow>	    is_shadow_present_pte(spte) &&</yellow>
<yellow>	    !is_large_pte(spte)) {</yellow>
		/*
		 * A small SPTE exists for this pfn, but FNAME(fetch)
		 * and __direct_map would like to create a large PTE
		 * instead: just force them to go down another level,
		 * patching back for them into pfn the next 9 bits of
		 * the address.
		 */
<yellow>		u64 page_mask = KVM_PAGES_PER_HPAGE(cur_level) -</yellow>
<yellow>				KVM_PAGES_PER_HPAGE(cur_level - 1);</yellow>
		fault-&gt;pfn |= fault-&gt;gfn &amp; page_mask;
		fault-&gt;goal_level--;
	}
<yellow>}</yellow>

static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
{
	struct kvm_shadow_walk_iterator it;
	struct kvm_mmu_page *sp;
	int ret;
<yellow>	gfn_t base_gfn = fault->gfn;</yellow>

	kvm_mmu_hugepage_adjust(vcpu, fault);

<yellow>	trace_kvm_mmu_spte_requested(fault);</yellow>
<yellow>	for_each_shadow_entry(vcpu, fault->addr, it) {</yellow>
		/*
		 * We cannot overwrite existing page tables with an NX
		 * large page, as the leaf could be executable.
		 */
<yellow>		if (fault->nx_huge_page_workaround_enabled)</yellow>
<yellow>			disallowed_hugepage_adjust(fault, *it.sptep, it.level);</yellow>

<yellow>		base_gfn = fault->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);</yellow>
		if (it.level == fault-&gt;goal_level)
			break;

<yellow>		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);</yellow>
		if (sp == ERR_PTR(-EEXIST))
			continue;

<yellow>		link_shadow_page(vcpu, it.sptep, sp);</yellow>
<yellow>		if (fault->is_tdp && fault->huge_page_disallowed &&</yellow>
<yellow>		    fault->req_level >= it.level)</yellow>
<yellow>			account_huge_nx_page(vcpu->kvm, sp);</yellow>
	}

<yellow>	if (WARN_ON_ONCE(it.level != fault->goal_level))</yellow>
		return -EFAULT;

<yellow>	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,</yellow>
			   base_gfn, fault-&gt;pfn, fault);
	if (ret == RET_PF_SPURIOUS)
		return ret;

<yellow>	direct_pte_prefetch(vcpu, it.sptep);</yellow>
	return ret;
}

static void kvm_send_hwpoison_signal(unsigned long address, struct task_struct *tsk)
{
	send_sig_mceerr(BUS_MCEERR_AR, (void __user *)address, PAGE_SHIFT, tsk);
}

static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)
{
	/*
	 * Do not cache the mmio info caused by writing the readonly gfn
	 * into the spte otherwise read access on readonly gfn also can
	 * caused mmio page fault and treat it as mmio access.
	 */
	if (pfn == KVM_PFN_ERR_RO_FAULT)
		return RET_PF_EMULATE;

<yellow>	if (pfn == KVM_PFN_ERR_HWPOISON) {</yellow>
<yellow>		kvm_send_hwpoison_signal(kvm_vcpu_gfn_to_hva(vcpu, gfn), current);</yellow>
		return RET_PF_RETRY;
	}

	return -EFAULT;
}

<blue>static int handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,</blue>
			       unsigned int access)
{
	/* The pfn is invalid, report the error! */
<blue>	if (unlikely(is_error_pfn(fault->pfn)))</blue>
<blue>		return kvm_handle_bad_page(vcpu, fault->gfn, fault->pfn);</blue>

<blue>	if (unlikely(!fault->slot)) {</blue>
<blue>		gva_t gva = fault->is_tdp ? 0 : fault->addr;</blue>

<blue>		vcpu_cache_mmio_info(vcpu, gva, fault->gfn,</blue>
				     access &amp; shadow_mmio_access_mask);
		/*
		 * If MMIO caching is disabled, emulate immediately without
		 * touching the shadow page tables as attempting to install an
		 * MMIO SPTE will just be an expensive nop.  Do not cache MMIO
		 * whose gfn is greater than host.MAXPHYADDR, any guest that
		 * generates such gfns is running nested and is being tricked
		 * by L0 userspace (you can observe gfn &gt; L1.MAXPHYADDR if
		 * and only if L1&#x27;s MAXPHYADDR is inaccurate with respect to
		 * the hardware&#x27;s).
		 */
<blue>		if (unlikely(!enable_mmio_caching) ||</blue>
<blue>		    unlikely(fault->gfn > kvm_mmu_max_gfn()))</blue>
			return RET_PF_EMULATE;
	}

	return RET_PF_CONTINUE;
<blue>}</blue>

static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
{
	/*
	 * Page faults with reserved bits set, i.e. faults on MMIO SPTEs, only
	 * reach the common page fault handler if the SPTE has an invalid MMIO
	 * generation number.  Refreshing the MMIO generation needs to go down
	 * the slow path.  Note, EPT Misconfigs do NOT set the PRESENT flag!
	 */
<blue>	if (fault->rsvd)</blue>
		return false;

	/*
	 * #PF can be fast if:
	 *
	 * 1. The shadow page table entry is not present and A/D bits are
	 *    disabled _by KVM_, which could mean that the fault is potentially
	 *    caused by access tracking (if enabled).  If A/D bits are enabled
	 *    by KVM, but disabled by L1 for L2, KVM is forced to disable A/D
	 *    bits for L2 and employ access tracking, but the fast page fault
	 *    mechanism only supports direct MMUs.
	 * 2. The shadow page table entry is present, the access is a write,
	 *    and no reserved bits are set (MMIO SPTEs cannot be &quot;fixed&quot;), i.e.
	 *    the fault was caused by a write-protection violation.  If the
	 *    SPTE is MMU-writable (determined later), the fault can be fixed
	 *    by setting the Writable bit, which can be done out of mmu_lock.
	 */
<blue>	if (!fault->present)</blue>
<blue>		return !kvm_ad_enabled();</blue>

	/*
	 * Note, instruction fetches and writes are mutually exclusive, ignore
	 * the &quot;exec&quot; flag.
	 */
<blue>	return fault->write;</blue>
}

/*
 * Returns true if the SPTE was fixed successfully. Otherwise,
 * someone else modified the SPTE from its original value.
 */
static bool
fast_pf_fix_direct_spte(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
			u64 *sptep, u64 old_spte, u64 new_spte)
{
	/*
	 * Theoretically we could also set dirty bit (and flush TLB) here in
	 * order to eliminate unnecessary PML logging. See comments in
	 * set_spte. But fast_page_fault is very unlikely to happen with PML
	 * enabled, so we do not do this. This might result in the same GPA
	 * to be logged in PML buffer again when the write really happens, and
	 * eventually to be called by mark_page_dirty twice. But it&#x27;s also no
	 * harm. This also avoids the TLB flush needed after setting dirty bit
	 * so non-PML cases won&#x27;t be impacted.
	 *
	 * Compare with set_spte where instead shadow_dirty_mask is set.
	 */
	if (!try_cmpxchg64(sptep, &amp;old_spte, new_spte))
		return false;

<yellow>	if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))</yellow>
<yellow>		mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);</yellow>

	return true;
}

<yellow>static bool is_access_allowed(struct kvm_page_fault *fault, u64 spte)</yellow>
{
<blue>	if (fault->exec)</blue>
<yellow>		return is_executable_pte(spte);</yellow>

<blue>	if (fault->write)</blue>
<blue>		return is_writable_pte(spte);</blue>

	/* Fault was on Read access */
<yellow>	return spte & PT_PRESENT_MASK;</yellow>
<blue>}</blue>

/*
 * Returns the last level spte pointer of the shadow page walk for the given
 * gpa, and sets *spte to the spte value. This spte may be non-preset. If no
 * walk could be performed, returns NULL and *spte does not contain valid data.
 *
 * Contract:
 *  - Must be called between walk_shadow_page_lockless_{begin,end}.
 *  - The returned sptep must not be used after walk_shadow_page_lockless_end.
 */
static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
{
	struct kvm_shadow_walk_iterator iterator;
	u64 old_spte;
	u64 *sptep = NULL;

<yellow>	for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {</yellow>
		sptep = iterator.sptep;
		*spte = old_spte;
	}

	return sptep;
}

/*
 * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
 */
static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
{
	struct kvm_mmu_page *sp;
<yellow>	int ret = RET_PF_INVALID;</yellow>
	u64 spte = 0ull;
	u64 *sptep = NULL;
	uint retry_count = 0;

<blue>	if (!page_fault_can_be_fast(fault))</blue>
		return ret;

<blue>	walk_shadow_page_lockless_begin(vcpu);</blue>

	do {
		u64 new_spte;

<blue>		if (is_tdp_mmu(vcpu->arch.mmu))</blue>
<blue>			sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);</blue>
		else
<yellow>			sptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);</yellow>

<blue>		if (!is_shadow_present_pte(spte))</blue>
			break;

<blue>		sp = sptep_to_sp(sptep);</blue>
<blue>		if (!is_last_spte(spte, sp->role.level))</blue>
			break;

		/*
		 * Check whether the memory access that caused the fault would
		 * still cause it if it were to be performed right now. If not,
		 * then this is a spurious fault caused by TLB lazily flushed,
		 * or some other CPU has already fixed the PTE after the
		 * current CPU took the fault.
		 *
		 * Need not check the access of upper level table entries since
		 * they are always ACC_ALL.
		 */
<blue>		if (is_access_allowed(fault, spte)) {</blue>
<yellow>			ret = RET_PF_SPURIOUS;</yellow>
			break;
		}

		new_spte = spte;

		/*
		 * KVM only supports fixing page faults outside of MMU lock for
		 * direct MMUs, nested MMUs are always indirect, and KVM always
		 * uses A/D bits for non-nested MMUs.  Thus, if A/D bits are
		 * enabled, the SPTE can&#x27;t be an access-tracked SPTE.
		 */
<blue>		if (unlikely(!kvm_ad_enabled()) && is_access_track_spte(spte))</blue>
<yellow>			new_spte = restore_acc_track_spte(new_spte);</yellow>

		/*
		 * To keep things simple, only SPTEs that are MMU-writable can
		 * be made fully writable outside of mmu_lock, e.g. only SPTEs
		 * that were write-protected for dirty-logging or access
		 * tracking are handled here.  Don&#x27;t bother checking if the
		 * SPTE is writable to prioritize running with A/D bits enabled.
		 * The is_access_allowed() check above handles the common case
		 * of the fault being spurious, and the SPTE is known to be
		 * shadow-present, i.e. except for access tracking restoration
		 * making the new SPTE writable, the check is wasteful.
		 */
<blue>		if (fault->write && is_mmu_writable_spte(spte)) {</blue>
			new_spte |= PT_WRITABLE_MASK;

			/*
			 * Do not fix write-permission on the large spte when
			 * dirty logging is enabled. Since we only dirty the
			 * first page into the dirty-bitmap in
			 * fast_pf_fix_direct_spte(), other pages are missed
			 * if its slot has dirty logging enabled.
			 *
			 * Instead, we let the slow page fault path create a
			 * normal spte to fix the access.
			 */
<yellow>			if (sp->role.level > PG_LEVEL_4K &&</yellow>
<yellow>			    kvm_slot_dirty_track_enabled(fault->slot))</yellow>
				break;
		}

		/* Verify that the fault can be handled in the fast path */
<blue>		if (new_spte == spte ||</blue>
<yellow>		    !is_access_allowed(fault, new_spte))</yellow>
			break;

		/*
		 * Currently, fast page fault only works for direct mapping
		 * since the gfn is not stable for indirect shadow page. See
		 * Documentation/virt/kvm/locking.rst to get more detail.
		 */
<yellow>		if (fast_pf_fix_direct_spte(vcpu, fault, sptep, spte, new_spte)) {</yellow>
<yellow>			ret = RET_PF_FIXED;</yellow>
			break;
		}

<yellow>		if (++retry_count > 4) {</yellow>
<yellow>			printk_once(KERN_WARNING</yellow>
				&quot;kvm: Fast #PF retrying more than 4 times.\n&quot;);
			break;
		}

	} while (true);

<blue>	trace_fast_page_fault(vcpu, fault, sptep, spte, ret);</blue>
<blue>	walk_shadow_page_lockless_end(vcpu);</blue>

<blue>	if (ret != RET_PF_INVALID)</blue>
<yellow>		vcpu->stat.pf_fast++;</yellow>

	return ret;
}

static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
			       struct list_head *invalid_list)
<blue>{</blue>
	struct kvm_mmu_page *sp;

<blue>	if (!VALID_PAGE(*root_hpa))</blue>
		return;

<blue>	sp = to_shadow_page(*root_hpa & SPTE_BASE_ADDR_MASK);</blue>
<yellow>	if (WARN_ON(!sp))</yellow>
		return;

<blue>	if (is_tdp_mmu_page(sp))</blue>
<blue>		kvm_tdp_mmu_put_root(kvm, sp, false);</blue>
<blue>	else if (!--sp->root_count && sp->role.invalid)</blue>
<blue>		kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);</blue>

<blue>	*root_hpa = INVALID_PAGE;</blue>
}

/* roots_to_free must be some combination of the KVM_MMU_ROOT_* flags */
void kvm_mmu_free_roots(struct kvm *kvm, struct kvm_mmu *mmu,
			ulong roots_to_free)
<blue>{</blue>
	int i;
	LIST_HEAD(invalid_list);
	bool free_active_root;

	BUILD_BUG_ON(KVM_MMU_NUM_PREV_ROOTS &gt;= BITS_PER_LONG);

	/* Before acquiring the MMU lock, see if we need to do any real work. */
	free_active_root = (roots_to_free &amp; KVM_MMU_ROOT_CURRENT)
<blue>		&& VALID_PAGE(mmu->root.hpa);</blue>

	if (!free_active_root) {
<blue>		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)</blue>
<blue>			if ((roots_to_free & KVM_MMU_ROOT_PREVIOUS(i)) &&</blue>
<blue>			    VALID_PAGE(mmu->prev_roots[i].hpa))</blue>
				break;

		if (i == KVM_MMU_NUM_PREV_ROOTS)
			return;
	}

<blue>	write_lock(&kvm->mmu_lock);</blue>

<blue>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)</blue>
<blue>		if (roots_to_free & KVM_MMU_ROOT_PREVIOUS(i))</blue>
<blue>			mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa,</blue>
					   &amp;invalid_list);

<blue>	if (free_active_root) {</blue>
<blue>		if (to_shadow_page(mmu->root.hpa)) {</blue>
<blue>			mmu_free_root_page(kvm, &mmu->root.hpa, &invalid_list);</blue>
<yellow>		} else if (mmu->pae_root) {</yellow>
<yellow>			for (i = 0; i < 4; ++i) {</yellow>
<yellow>				if (!IS_VALID_PAE_ROOT(mmu->pae_root[i]))</yellow>
					continue;

<yellow>				mmu_free_root_page(kvm, &mmu->pae_root[i],</yellow>
						   &amp;invalid_list);
				mmu-&gt;pae_root[i] = INVALID_PAE_ROOT;
			}
		}
<blue>		mmu->root.hpa = INVALID_PAGE;</blue>
		mmu-&gt;root.pgd = 0;
	}

<blue>	kvm_mmu_commit_zap_page(kvm, &invalid_list);</blue>
<blue>	write_unlock(&kvm->mmu_lock);</blue>
}
EXPORT_SYMBOL_GPL(kvm_mmu_free_roots);

void kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)
{
	unsigned long roots_to_free = 0;
	hpa_t root_hpa;
	int i;

	/*
	 * This should not be called while L2 is active, L2 can&#x27;t invalidate
	 * _only_ its own roots, e.g. INVVPID unconditionally exits.
	 */
<yellow>	WARN_ON_ONCE(mmu->root_role.guest_mode);</yellow>

<yellow>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {</yellow>
<yellow>		root_hpa = mmu->prev_roots[i].hpa;</yellow>
		if (!VALID_PAGE(root_hpa))
			continue;

<yellow>		if (!to_shadow_page(root_hpa) ||</yellow>
			to_shadow_page(root_hpa)-&gt;role.guest_mode)
<yellow>			roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);</yellow>
	}

<yellow>	kvm_mmu_free_roots(kvm, mmu, roots_to_free);</yellow>
}
EXPORT_SYMBOL_GPL(kvm_mmu_free_guest_mode_roots);


static int mmu_check_root(struct kvm_vcpu *vcpu, gfn_t root_gfn)
{
	int ret = 0;

	if (!kvm_vcpu_is_visible_gfn(vcpu, root_gfn)) {
<yellow>		kvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);</yellow>
		ret = 1;
	}

	return ret;
}

static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
			    u8 level)
{
<blue>	union kvm_mmu_page_role role = vcpu->arch.mmu->root_role;</blue>
	struct kvm_mmu_page *sp;

	role.level = level;
	role.quadrant = quadrant;

<yellow>	WARN_ON_ONCE(quadrant && !role.has_4_byte_gpte);</yellow>
<blue>	WARN_ON_ONCE(role.direct && role.has_4_byte_gpte);</blue>

<blue>	sp = kvm_mmu_get_shadow_page(vcpu, gfn, role);</blue>
	++sp-&gt;root_count;

<blue>	return __pa(sp->spt);</blue>
}

static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
{
	struct kvm_mmu *mmu = vcpu-&gt;arch.mmu;
<blue>	u8 shadow_root_level = mmu->root_role.level;</blue>
	hpa_t root;
	unsigned i;
	int r;

	write_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);
<blue>	r = make_mmu_pages_available(vcpu);</blue>
	if (r &lt; 0)
		goto out_unlock;

<blue>	if (is_tdp_mmu_enabled(vcpu->kvm)) {</blue>
<blue>		root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);</blue>
<blue>		mmu->root.hpa = root;</blue>
<yellow>	} else if (shadow_root_level >= PT64_ROOT_4LEVEL) {</yellow>
<yellow>		root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);</yellow>
		mmu-&gt;root.hpa = root;
<yellow>	} else if (shadow_root_level == PT32E_ROOT_LEVEL) {</yellow>
<yellow>		if (WARN_ON_ONCE(!mmu->pae_root)) {</yellow>
			r = -EIO;
			goto out_unlock;
		}

		for (i = 0; i &lt; 4; ++i) {
<yellow>			WARN_ON_ONCE(IS_VALID_PAE_ROOT(mmu->pae_root[i]));</yellow>

<yellow>			root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0,</yellow>
					      PT32_ROOT_LEVEL);
			mmu-&gt;pae_root[i] = root | PT_PRESENT_MASK |
					   shadow_me_value;
		}
<yellow>		mmu->root.hpa = __pa(mmu->pae_root);</yellow>
	} else {
<yellow>		WARN_ONCE(1, "Bad TDP root level = %d\n", shadow_root_level);</yellow>
		r = -EIO;
		goto out_unlock;
	}

	/* root.pgd is ignored for direct MMUs. */
	mmu-&gt;root.pgd = 0;
out_unlock:
<yellow>	write_unlock(&vcpu->kvm->mmu_lock);</yellow>
	return r;
}

static int mmu_first_shadow_root_alloc(struct kvm *kvm)
{
	struct kvm_memslots *slots;
	struct kvm_memory_slot *slot;
	int r = 0, i, bkt;

	/*
	 * Check if this is the first shadow root being allocated before
	 * taking the lock.
	 */
<blue>	if (kvm_shadow_root_allocated(kvm))</blue>
		return 0;

<blue>	mutex_lock(&kvm->slots_arch_lock);</blue>

	/* Recheck, under the lock, whether this is the first shadow root. */
<blue>	if (kvm_shadow_root_allocated(kvm))</blue>
		goto out_unlock;

	/*
	 * Check if anything actually needs to be allocated, e.g. all metadata
	 * will be allocated upfront if TDP is disabled.
	 */
<blue>	if (kvm_memslots_have_rmaps(kvm) &&</blue>
<yellow>	    kvm_page_track_write_tracking_enabled(kvm))</yellow>
		goto out_success;

<blue>	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {</blue>
<blue>		slots = __kvm_memslots(kvm, i);</blue>
<blue>		kvm_for_each_memslot(slot, bkt, slots) {</blue>
			/*
			 * Both of these functions are no-ops if the target is
			 * already allocated, so unconditionally calling both
			 * is safe.  Intentionally do NOT free allocations on
			 * failure to avoid having to track which allocations
			 * were made now versus when the memslot was created.
			 * The metadata is guaranteed to be freed when the slot
			 * is freed, and will be kept/used if userspace retries
			 * KVM_RUN instead of killing the VM.
			 */
<blue>			r = memslot_rmap_alloc(slot, slot->npages);</blue>
			if (r)
				goto out_unlock;
<blue>			r = kvm_page_track_write_tracking_alloc(slot);</blue>
			if (r)
				goto out_unlock;
		}
	}

	/*
	 * Ensure that shadow_root_allocated becomes true strictly after
	 * all the related pointers are set.
	 */
out_success:
<blue>	smp_store_release(&kvm->arch.shadow_root_allocated, true);</blue>

out_unlock:
<yellow>	mutex_unlock(&kvm->slots_arch_lock);</yellow>
	return r;
}

static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
{
	struct kvm_mmu *mmu = vcpu-&gt;arch.mmu;
	u64 pdptrs[4], pm_mask;
	gfn_t root_gfn, root_pgd;
	int quadrant, i, r;
	hpa_t root;

<blue>	root_pgd = mmu->get_guest_pgd(vcpu);</blue>
	root_gfn = root_pgd &gt;&gt; PAGE_SHIFT;

	if (mmu_check_root(vcpu, root_gfn))
		return 1;

	/*
	 * On SVM, reading PDPTRs might access guest memory, which might fault
	 * and thus might sleep.  Grab the PDPTRs before acquiring mmu_lock.
	 */
<blue>	if (mmu->cpu_role.base.level == PT32E_ROOT_LEVEL) {</blue>
<yellow>		for (i = 0; i < 4; ++i) {</yellow>
<yellow>			pdptrs[i] = mmu->get_pdptr(vcpu, i);</yellow>
			if (!(pdptrs[i] &amp; PT_PRESENT_MASK))
				continue;

<yellow>			if (mmu_check_root(vcpu, pdptrs[i] >> PAGE_SHIFT))</yellow>
				return 1;
		}
	}

<blue>	r = mmu_first_shadow_root_alloc(vcpu->kvm);</blue>
	if (r)
		return r;

<blue>	write_lock(&vcpu->kvm->mmu_lock);</blue>
<blue>	r = make_mmu_pages_available(vcpu);</blue>
	if (r &lt; 0)
		goto out_unlock;

	/*
	 * Do we shadow a long mode page table? If so we need to
	 * write-protect the guests page table root.
	 */
<blue>	if (mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL) {</blue>
		root = mmu_alloc_root(vcpu, root_gfn, 0,
<blue>				      mmu->root_role.level);</blue>
<blue>		mmu->root.hpa = root;</blue>
		goto set_root_pgd;
	}

<yellow>	if (WARN_ON_ONCE(!mmu->pae_root)) {</yellow>
		r = -EIO;
		goto out_unlock;
	}

	/*
	 * We shadow a 32 bit page table. This may be a legacy 2-level
	 * or a PAE 3-level page table. In either case we need to be aware that
	 * the shadow page table may be a PAE or a long mode page table.
	 */
<yellow>	pm_mask = PT_PRESENT_MASK | shadow_me_value;</yellow>
	if (mmu-&gt;root_role.level &gt;= PT64_ROOT_4LEVEL) {
		pm_mask |= PT_ACCESSED_MASK | PT_WRITABLE_MASK | PT_USER_MASK;

<yellow>		if (WARN_ON_ONCE(!mmu->pml4_root)) {</yellow>
			r = -EIO;
			goto out_unlock;
		}
<yellow>		mmu->pml4_root[0] = __pa(mmu->pae_root) | pm_mask;</yellow>

		if (mmu-&gt;root_role.level == PT64_ROOT_5LEVEL) {
<yellow>			if (WARN_ON_ONCE(!mmu->pml5_root)) {</yellow>
				r = -EIO;
				goto out_unlock;
			}
<yellow>			mmu->pml5_root[0] = __pa(mmu->pml4_root) | pm_mask;</yellow>
		}
	}

<yellow>	for (i = 0; i < 4; ++i) {</yellow>
<yellow>		WARN_ON_ONCE(IS_VALID_PAE_ROOT(mmu->pae_root[i]));</yellow>

<yellow>		if (mmu->cpu_role.base.level == PT32E_ROOT_LEVEL) {</yellow>
<yellow>			if (!(pdptrs[i] & PT_PRESENT_MASK)) {</yellow>
<yellow>				mmu->pae_root[i] = INVALID_PAE_ROOT;</yellow>
				continue;
			}
<yellow>			root_gfn = pdptrs[i] >> PAGE_SHIFT;</yellow>
		}

		/*
		 * If shadowing 32-bit non-PAE page tables, each PAE page
		 * directory maps one quarter of the guest&#x27;s non-PAE page
		 * directory. Othwerise each PAE page direct shadows one guest
		 * PAE page directory so that quadrant should be 0.
		 */
<yellow>		quadrant = (mmu->cpu_role.base.level == PT32_ROOT_LEVEL) ? i : 0;</yellow>

<yellow>		root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);</yellow>
		mmu-&gt;pae_root[i] = root | pm_mask;
	}

<yellow>	if (mmu->root_role.level == PT64_ROOT_5LEVEL)</yellow>
<yellow>		mmu->root.hpa = __pa(mmu->pml5_root);</yellow>
<yellow>	else if (mmu->root_role.level == PT64_ROOT_4LEVEL)</yellow>
<yellow>		mmu->root.hpa = __pa(mmu->pml4_root);</yellow>
	else
<yellow>		mmu->root.hpa = __pa(mmu->pae_root);</yellow>

set_root_pgd:
	mmu-&gt;root.pgd = root_pgd;
out_unlock:
<yellow>	write_unlock(&vcpu->kvm->mmu_lock);</yellow>

	return r;
}

static int mmu_alloc_special_roots(struct kvm_vcpu *vcpu)
{
	struct kvm_mmu *mmu = vcpu-&gt;arch.mmu;
	bool need_pml5 = mmu-&gt;root_role.level &gt; PT64_ROOT_4LEVEL;
	u64 *pml5_root = NULL;
	u64 *pml4_root = NULL;
	u64 *pae_root;

	/*
	 * When shadowing 32-bit or PAE NPT with 64-bit NPT, the PML4 and PDP
	 * tables are allocated and initialized at root creation as there is no
	 * equivalent level in the guest&#x27;s NPT to shadow.  Allocate the tables
	 * on demand, as running a 32-bit L1 VMM on 64-bit KVM is very rare.
	 */
	if (mmu-&gt;root_role.direct ||
<blue>	    mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL ||</blue>
	    mmu-&gt;root_role.level &lt; PT64_ROOT_4LEVEL)
		return 0;

	/*
	 * NPT, the only paging mode that uses this horror, uses a fixed number
	 * of levels for the shadow page tables, e.g. all MMUs are 4-level or
	 * all MMus are 5-level.  Thus, this can safely require that pml5_root
	 * is allocated if the other roots are valid and pml5 is needed, as any
	 * prior MMU would also have required pml5.
	 */
<yellow>	if (mmu->pae_root && mmu->pml4_root && (!need_pml5 || mmu->pml5_root))</yellow>
		return 0;

	/*
	 * The special roots should always be allocated in concert.  Yell and
	 * bail if KVM ends up in a state where only one of the roots is valid.
	 */
<yellow>	if (WARN_ON_ONCE(!tdp_enabled || mmu->pae_root || mmu->pml4_root ||</yellow>
			 (need_pml5 &amp;&amp; mmu-&gt;pml5_root)))
		return -EIO;

	/*
	 * Unlike 32-bit NPT, the PDP table doesn&#x27;t need to be in low mem, and
	 * doesn&#x27;t need to be decrypted.
	 */
<yellow>	pae_root = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);</yellow>
	if (!pae_root)
		return -ENOMEM;

#ifdef CONFIG_X86_64
<yellow>	pml4_root = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);</yellow>
	if (!pml4_root)
		goto err_pml4;

<yellow>	if (need_pml5) {</yellow>
<yellow>		pml5_root = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);</yellow>
		if (!pml5_root)
			goto err_pml5;
	}
#endif

<yellow>	mmu->pae_root = pae_root;</yellow>
	mmu-&gt;pml4_root = pml4_root;
	mmu-&gt;pml5_root = pml5_root;

	return 0;

#ifdef CONFIG_X86_64
err_pml5:
<yellow>	free_page((unsigned long)pml4_root);</yellow>
err_pml4:
<yellow>	free_page((unsigned long)pae_root);</yellow>
	return -ENOMEM;
#endif
}

static bool is_unsync_root(hpa_t root)
{
	struct kvm_mmu_page *sp;

	if (!VALID_PAGE(root))
		return false;

	/*
	 * The read barrier orders the CPU&#x27;s read of SPTE.W during the page table
	 * walk before the reads of sp-&gt;unsync/sp-&gt;unsync_children here.
	 *
	 * Even if another CPU was marking the SP as unsync-ed simultaneously,
	 * any guest page table changes are not guaranteed to be visible anyway
	 * until this VCPU issues a TLB flush strictly after those changes are
	 * made.  We only need to ensure that the other CPU sets these flags
	 * before any actual changes to the page tables are made.  The comments
	 * in mmu_try_to_unsync_pages() describe what could go wrong if this
	 * requirement isn&#x27;t satisfied.
	 */
	smp_rmb();
<blue>	sp = to_shadow_page(root);</blue>

	/*
	 * PAE roots (somewhat arbitrarily) aren&#x27;t backed by shadow pages, the
	 * PDPTEs for a given PAE root need to be synchronized individually.
	 */
<yellow>	if (WARN_ON_ONCE(!sp))</yellow>
		return false;

<blue>	if (sp->unsync || sp->unsync_children)</blue>
		return true;

	return false;
}

void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
{
	int i;
	struct kvm_mmu_page *sp;

<blue>	if (vcpu->arch.mmu->root_role.direct)</blue>
		return;

<blue>	if (!VALID_PAGE(vcpu->arch.mmu->root.hpa))</blue>
		return;

<blue>	vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);</blue>

	if (vcpu-&gt;arch.mmu-&gt;cpu_role.base.level &gt;= PT64_ROOT_4LEVEL) {
<blue>		hpa_t root = vcpu->arch.mmu->root.hpa;</blue>
		sp = to_shadow_page(root);

<blue>		if (!is_unsync_root(root))</blue>
			return;

<yellow>		write_lock(&vcpu->kvm->mmu_lock);</yellow>
		mmu_sync_children(vcpu, sp, true);
		write_unlock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);
		return;
	}

<yellow>	write_lock(&vcpu->kvm->mmu_lock);</yellow>

<yellow>	for (i = 0; i < 4; ++i) {</yellow>
<yellow>		hpa_t root = vcpu->arch.mmu->pae_root[i];</yellow>

		if (IS_VALID_PAE_ROOT(root)) {
<yellow>			root &= SPTE_BASE_ADDR_MASK;</yellow>
			sp = to_shadow_page(root);
			mmu_sync_children(vcpu, sp, true);
		}
	}

<yellow>	write_unlock(&vcpu->kvm->mmu_lock);</yellow>
<blue>}</blue>

void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu)
{
	unsigned long roots_to_free = 0;
	int i;

<yellow>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)</yellow>
<yellow>		if (is_unsync_root(vcpu->arch.mmu->prev_roots[i].hpa))</yellow>
<yellow>			roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);</yellow>

	/* sync prev_roots by simply freeing them */
<yellow>	kvm_mmu_free_roots(vcpu->kvm, vcpu->arch.mmu, roots_to_free);</yellow>
}

static gpa_t nonpaging_gva_to_gpa(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
				  gpa_t vaddr, u64 access,
				  struct x86_exception *exception)
{
<yellow>	if (exception)</yellow>
<yellow>		exception->error_code = 0;</yellow>
<yellow>	return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);</yellow>
<yellow>}</yellow>

static bool mmio_info_in_cache(struct kvm_vcpu *vcpu, u64 addr, bool direct)
{
	/*
	 * A nested guest cannot use the MMIO cache if it is using nested
	 * page tables, because cr2 is a nGPA while the cache stores GPAs.
	 */
<blue>	if (mmu_is_nested(vcpu))</blue>
		return false;

	if (direct)
<blue>		return vcpu_match_mmio_gpa(vcpu, addr);</blue>

<yellow>	return vcpu_match_mmio_gva(vcpu, addr);</yellow>
<blue>}</blue>

/*
 * Return the level of the lowest level SPTE added to sptes.
 * That SPTE may be non-present.
 *
 * Must be called between walk_shadow_page_lockless_{begin,end}.
 */
static int get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes, int *root_level)
{
	struct kvm_shadow_walk_iterator iterator;
	int leaf = -1;
	u64 spte;

<blue>	for (shadow_walk_init(&iterator, vcpu, addr),</blue>
	     *root_level = iterator.level;
<blue>	     shadow_walk_okay(&iterator);</blue>
<blue>	     __shadow_walk_next(&iterator, spte)) {</blue>
		leaf = iterator.level;
<blue>		spte = mmu_spte_get_lockless(iterator.sptep);</blue>

		sptes[leaf] = spte;
	}

	return leaf;
}

/* return true if reserved bit(s) are detected on a valid, non-MMIO SPTE. */
static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
{
	u64 sptes[PT64_ROOT_MAX_LEVEL + 1];
	struct rsvd_bits_validate *rsvd_check;
	int root, leaf, level;
	bool reserved = false;

<blue>	walk_shadow_page_lockless_begin(vcpu);</blue>

<blue>	if (is_tdp_mmu(vcpu->arch.mmu))</blue>
<blue>		leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);</blue>
	else
<blue>		leaf = get_walk(vcpu, addr, sptes, &root);</blue>

<blue>	walk_shadow_page_lockless_end(vcpu);</blue>

<blue>	if (unlikely(leaf < 0)) {</blue>
		*sptep = 0ull;
		return reserved;
	}

<blue>	*sptep = sptes[leaf];</blue>

	/*
	 * Skip reserved bits checks on the terminal leaf if it&#x27;s not a valid
	 * SPTE.  Note, this also (intentionally) skips MMIO SPTEs, which, by
	 * design, always have reserved bits set.  The purpose of the checks is
	 * to detect reserved bits on non-MMIO SPTEs. i.e. buggy SPTEs.
	 */
	if (!is_shadow_present_pte(sptes[leaf]))
<blue>		leaf++;</blue>

<blue>	rsvd_check = &vcpu->arch.mmu->shadow_zero_check;</blue>

<blue>	for (level = root; level >= leaf; level--)</blue>
<blue>		reserved |= is_rsvd_spte(rsvd_check, sptes[level], level);</blue>

<blue>	if (reserved) {</blue>
		pr_err(&quot;%s: reserved bits set on MMU-present spte, addr 0x%llx, hierarchy:\n&quot;,
		       __func__, addr);
		for (level = root; level &gt;= leaf; level--)
			pr_err(&quot;------ spte = 0x%llx level = %d, rsvd bits = 0x%llx&quot;,
			       sptes[level], level,
			       get_rsvd_bits(rsvd_check, sptes[level], level));
	}

	return reserved;
}

static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
{
	u64 spte;
	bool reserved;

<blue>	if (mmio_info_in_cache(vcpu, addr, direct))</blue>
		return RET_PF_EMULATE;

<blue>	reserved = get_mmio_spte(vcpu, addr, &spte);</blue>
	if (WARN_ON(reserved))
		return -EINVAL;

<blue>	if (is_mmio_spte(spte)) {</blue>
<blue>		gfn_t gfn = get_mmio_spte_gfn(spte);</blue>
		unsigned int access = get_mmio_spte_access(spte);

<blue>		if (!check_mmio_spte(vcpu, spte))</blue>
			return RET_PF_INVALID;

<blue>		if (direct)</blue>
			addr = 0;

<yellow>		trace_handle_mmio_page_fault(addr, gfn, access);</yellow>
<blue>		vcpu_cache_mmio_info(vcpu, addr, gfn, access);</blue>
		return RET_PF_EMULATE;
	}

	/*
	 * If the page table is zapped by other cpus, let CPU fault again on
	 * the address.
	 */
	return RET_PF_RETRY;
}

static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
					 struct kvm_page_fault *fault)
{
<blue>	if (unlikely(fault->rsvd))</blue>
		return false;

<blue>	if (!fault->present || !fault->write)</blue>
		return false;

	/*
	 * guest is writing the page which is write tracked which can
	 * not be fixed by page fault handler.
	 */
<blue>	if (kvm_slot_page_track_is_active(vcpu->kvm, fault->slot, fault->gfn, KVM_PAGE_TRACK_WRITE))</blue>
		return true;

	return false;
<blue>}</blue>

static void shadow_page_table_clear_flood(struct kvm_vcpu *vcpu, gva_t addr)
<yellow>{</yellow>
	struct kvm_shadow_walk_iterator iterator;
	u64 spte;

<yellow>	walk_shadow_page_lockless_begin(vcpu);</yellow>
<yellow>	for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)</yellow>
<yellow>		clear_sp_write_flooding_count(iterator.sptep);</yellow>
<yellow>	walk_shadow_page_lockless_end(vcpu);</yellow>
}

static u32 alloc_apf_token(struct kvm_vcpu *vcpu)
{
	/* make sure the token value is not 0 */
	u32 id = vcpu-&gt;arch.apf.id;

	if (id &lt;&lt; 12 == 0)
<yellow>		vcpu->arch.apf.id = 1;</yellow>

<yellow>	return (vcpu->arch.apf.id++ << 12) | vcpu->vcpu_id;</yellow>
}

static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
				    gfn_t gfn)
{
	struct kvm_arch_async_pf arch;

<yellow>	arch.token = alloc_apf_token(vcpu);</yellow>
	arch.gfn = gfn;
	arch.direct_map = vcpu-&gt;arch.mmu-&gt;root_role.direct;
	arch.cr3 = vcpu-&gt;arch.mmu-&gt;get_guest_pgd(vcpu);

	return kvm_setup_async_pf(vcpu, cr2_or_gpa,
				  kvm_vcpu_gfn_to_hva(vcpu, gfn), &amp;arch);
}

void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
<yellow>{</yellow>
	int r;

<yellow>	if ((vcpu->arch.mmu->root_role.direct != work->arch.direct_map) ||</yellow>
<yellow>	      work->wakeup_all)</yellow>
		return;

<yellow>	r = kvm_mmu_reload(vcpu);</yellow>
	if (unlikely(r))
		return;

<yellow>	if (!vcpu->arch.mmu->root_role.direct &&</yellow>
<yellow>	      work->arch.cr3 != vcpu->arch.mmu->get_guest_pgd(vcpu))</yellow>
		return;

<yellow>	kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);</yellow>
}

static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
<blue>{</blue>
<blue>	struct kvm_memory_slot *slot = fault->slot;</blue>
	bool async;

	/*
	 * Retry the page fault if the gfn hit a memslot that is being deleted
	 * or moved.  This ensures any existing SPTEs for the old memslot will
	 * be zapped before KVM inserts a new MMIO SPTE for the gfn.
	 */
<blue>	if (slot && (slot->flags & KVM_MEMSLOT_INVALID))</blue>
		return RET_PF_RETRY;

<blue>	if (!kvm_is_visible_memslot(slot)) {</blue>
		/* Don&#x27;t expose private memslots to L2. */
<blue>		if (is_guest_mode(vcpu)) {</blue>
			fault-&gt;slot = NULL;
<blue>			fault->pfn = KVM_PFN_NOSLOT;</blue>
			fault-&gt;map_writable = false;
			return RET_PF_CONTINUE;
		}
		/*
		 * If the APIC access page exists but is disabled, go directly
		 * to emulation without caching the MMIO access or creating a
		 * MMIO SPTE.  That way the cache doesn&#x27;t need to be purged
		 * when the AVIC is re-enabled.
		 */
<blue>		if (slot && slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT &&</blue>
<blue>		    !kvm_apicv_activated(vcpu->kvm))</blue>
			return RET_PF_EMULATE;
	}

	async = false;
<blue>	fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, &async,</blue>
<blue>					  fault->write, &fault->map_writable,</blue>
					  &amp;fault-&gt;hva);
<blue>	if (!async)</blue>
		return RET_PF_CONTINUE; /* *pfn has correct page already */

<yellow>	if (!fault->prefetch && kvm_can_do_async_pf(vcpu)) {</yellow>
<yellow>		trace_kvm_try_async_get_page(fault->addr, fault->gfn);</yellow>
<yellow>		if (kvm_find_async_pf_gfn(vcpu, fault->gfn)) {</yellow>
<yellow>			trace_kvm_async_pf_repeated_fault(fault->addr, fault->gfn);</yellow>
<yellow>			kvm_make_request(KVM_REQ_APF_HALT, vcpu);</yellow>
			return RET_PF_RETRY;
<yellow>		} else if (kvm_arch_setup_async_pf(vcpu, fault->addr, fault->gfn)) {</yellow>
			return RET_PF_RETRY;
		}
	}

<yellow>	fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, NULL,</yellow>
<yellow>					  fault->write, &fault->map_writable,</yellow>
					  &amp;fault-&gt;hva);
	return RET_PF_CONTINUE;
}

/*
 * Returns true if the page fault is stale and needs to be retried, i.e. if the
 * root was invalidated by a memslot update or a relevant mmu_notifier fired.
 */
static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
				struct kvm_page_fault *fault, int mmu_seq)
{
<blue>	struct kvm_mmu_page *sp = to_shadow_page(vcpu->arch.mmu->root.hpa);</blue>

	/* Special roots, e.g. pae_root, are not backed by shadow pages. */
<blue>	if (sp && is_obsolete_sp(vcpu->kvm, sp))</blue>
		return true;

	/*
	 * Roots without an associated shadow page are considered invalid if
	 * there is a pending request to free obsolete roots.  The request is
	 * only a hint that the current root _may_ be obsolete and needs to be
	 * reloaded, e.g. if the guest frees a PGD that KVM is tracking as a
	 * previous root, then __kvm_mmu_prepare_zap_page() signals all vCPUs
	 * to reload even if no vCPU is actively using the root.
	 */
<yellow>	if (!sp && kvm_test_request(KVM_REQ_MMU_FREE_OBSOLETE_ROOTS, vcpu))</yellow>
		return true;

<blue>	return fault->slot &&</blue>
<blue>	       mmu_invalidate_retry_hva(vcpu->kvm, mmu_seq, fault->hva);</blue>
<blue>}</blue>

static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
<blue>{</blue>
<blue>	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);</blue>

	unsigned long mmu_seq;
	int r;

<blue>	fault->gfn = fault->addr >> PAGE_SHIFT;</blue>
	fault-&gt;slot = kvm_vcpu_gfn_to_memslot(vcpu, fault-&gt;gfn);

	if (page_fault_handle_page_track(vcpu, fault))
		return RET_PF_EMULATE;

<blue>	r = fast_page_fault(vcpu, fault);</blue>
	if (r != RET_PF_INVALID)
		return r;

<blue>	r = mmu_topup_memory_caches(vcpu, false);</blue>
	if (r)
		return r;

<blue>	mmu_seq = vcpu->kvm->mmu_invalidate_seq;</blue>
	smp_rmb();

	r = kvm_faultin_pfn(vcpu, fault);
	if (r != RET_PF_CONTINUE)
		return r;

<blue>	r = handle_abnormal_pfn(vcpu, fault, ACC_ALL);</blue>
	if (r != RET_PF_CONTINUE)
		return r;

	r = RET_PF_RETRY;

	if (is_tdp_mmu_fault)
<blue>		read_lock(&vcpu->kvm->mmu_lock);</blue>
	else
<yellow>		write_lock(&vcpu->kvm->mmu_lock);</yellow>

	if (is_page_fault_stale(vcpu, fault, mmu_seq))
		goto out_unlock;

	if (is_tdp_mmu_fault) {
<blue>		r = kvm_tdp_mmu_map(vcpu, fault);</blue>
	} else {
<yellow>		r = make_mmu_pages_available(vcpu);</yellow>
		if (r)
			goto out_unlock;
<yellow>		r = __direct_map(vcpu, fault);</yellow>
	}

out_unlock:
	if (is_tdp_mmu_fault)
<blue>		read_unlock(&vcpu->kvm->mmu_lock);</blue>
	else
<yellow>		write_unlock(&vcpu->kvm->mmu_lock);</yellow>
<blue>	kvm_release_pfn_clean(fault->pfn);</blue>
	return r;
}

static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
				struct kvm_page_fault *fault)
{
	pgprintk(&quot;%s: gva %lx error %x\n&quot;, __func__, fault-&gt;addr, fault-&gt;error_code);

	/* This path builds a PAE pagetable, we can map 2mb pages at maximum. */
<yellow>	fault->max_level = PG_LEVEL_2M;</yellow>
	return direct_page_fault(vcpu, fault);
}

int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
				u64 fault_address, char *insn, int insn_len)
{
	int r = 1;
<yellow>	u32 flags = vcpu->arch.apf.host_apf_flags;</yellow>

#ifndef CONFIG_X86_64
	/* A 64-bit CR2 should be impossible on 32-bit KVM. */
	if (WARN_ON_ONCE(fault_address &gt;&gt; 32))
		return -EFAULT;
#endif

	vcpu-&gt;arch.l1tf_flush_l1d = true;
	if (!flags) {
<yellow>		trace_kvm_page_fault(vcpu, fault_address, error_code);</yellow>

<yellow>		if (kvm_event_needs_reinjection(vcpu))</yellow>
<yellow>			kvm_mmu_unprotect_page_virt(vcpu, fault_address);</yellow>
<yellow>		r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,</yellow>
				insn_len);
<yellow>	} else if (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {</yellow>
<yellow>		vcpu->arch.apf.host_apf_flags = 0;</yellow>
		local_irq_disable();
		kvm_async_pf_task_wait_schedule(fault_address);
		local_irq_enable();
	} else {
<yellow>		WARN_ONCE(1, "Unexpected host async PF flags: %x\n", flags);</yellow>
	}

	return r;
<yellow>}</yellow>
EXPORT_SYMBOL_GPL(kvm_handle_page_fault);

int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
{
	/*
	 * If the guest&#x27;s MTRRs may be used to compute the &quot;real&quot; memtype,
	 * restrict the mapping level to ensure KVM uses a consistent memtype
	 * across the entire mapping.  If the host MTRRs are ignored by TDP
	 * (shadow_memtype_mask is non-zero), and the VM has non-coherent DMA
	 * (DMA doesn&#x27;t snoop CPU caches), KVM&#x27;s ABI is to honor the memtype
	 * from the guest&#x27;s MTRRs so that guest accesses to memory that is
	 * DMA&#x27;d aren&#x27;t cached against the guest&#x27;s wishes.
	 *
	 * Note, KVM may still ultimately ignore guest MTRRs for certain PFNs,
	 * e.g. KVM will force UC memtype for host MMIO.
	 */
<blue>	if (shadow_memtype_mask && kvm_arch_has_noncoherent_dma(vcpu->kvm)) {</blue>
<yellow>		for ( ; fault->max_level > PG_LEVEL_4K; --fault->max_level) {</yellow>
<yellow>			int page_num = KVM_PAGES_PER_HPAGE(fault->max_level);</yellow>
			gfn_t base = (fault-&gt;addr &gt;&gt; PAGE_SHIFT) &amp; ~(page_num - 1);

			if (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))
				break;
		}
	}

<blue>	return direct_page_fault(vcpu, fault);</blue>
}

static void nonpaging_init_context(struct kvm_mmu *context)
{
	context-&gt;page_fault = nonpaging_page_fault;
	context-&gt;gva_to_gpa = nonpaging_gva_to_gpa;
	context-&gt;sync_page = nonpaging_sync_page;
	context-&gt;invlpg = NULL;
}

static inline bool is_root_usable(struct kvm_mmu_root_info *root, gpa_t pgd,
				  union kvm_mmu_page_role role)
{
<blue>	return (role.direct || pgd == root->pgd) &&</blue>
<blue>	       VALID_PAGE(root->hpa) &&</blue>
<blue>	       role.word == to_shadow_page(root->hpa)->role.word;</blue>
}

/*
 * Find out if a previously cached root matching the new pgd/role is available,
 * and insert the current root as the MRU in the cache.
 * If a matching root is found, it is assigned to kvm_mmu-&gt;root and
 * true is returned.
 * If no match is found, kvm_mmu-&gt;root is left invalid, the LRU root is
 * evicted to make room for the current root, and false is returned.
 */
static bool cached_root_find_and_keep_current(struct kvm *kvm, struct kvm_mmu *mmu,
					      gpa_t new_pgd,
					      union kvm_mmu_page_role new_role)
{
	uint i;

<blue>	if (is_root_usable(&mmu->root, new_pgd, new_role))</blue>
		return true;

<blue>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {</blue>
		/*
		 * The swaps end up rotating the cache like this:
		 *   C   0 1 2 3   (on entry to the function)
		 *   0   C 1 2 3
		 *   1   C 0 2 3
		 *   2   C 0 1 3
		 *   3   C 0 1 2   (on exit from the loop)
		 */
<blue>		swap(mmu->root, mmu->prev_roots[i]);</blue>
<blue>		if (is_root_usable(&mmu->root, new_pgd, new_role))</blue>
			return true;
	}

<blue>	kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);</blue>
	return false;
}

/*
 * Find out if a previously cached root matching the new pgd/role is available.
 * On entry, mmu-&gt;root is invalid.
 * If a matching root is found, it is assigned to kvm_mmu-&gt;root, the LRU entry
 * of the cache becomes invalid, and true is returned.
 * If no match is found, kvm_mmu-&gt;root is left invalid and false is returned.
 */
static bool cached_root_find_without_current(struct kvm *kvm, struct kvm_mmu *mmu,
					     gpa_t new_pgd,
					     union kvm_mmu_page_role new_role)
{
	uint i;

<blue>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)</blue>
<blue>		if (is_root_usable(&mmu->prev_roots[i], new_pgd, new_role))</blue>
			goto hit;

	return false;

hit:
<blue>	swap(mmu->root, mmu->prev_roots[i]);</blue>
	/* Bubble up the remaining roots.  */
	for (; i &lt; KVM_MMU_NUM_PREV_ROOTS - 1; i++)
<blue>		mmu->prev_roots[i] = mmu->prev_roots[i + 1];</blue>
<blue>	mmu->prev_roots[i].hpa = INVALID_PAGE;</blue>
	return true;
}

static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
			    gpa_t new_pgd, union kvm_mmu_page_role new_role)
{
	/*
	 * For now, limit the caching to 64-bit hosts+VMs in order to avoid
	 * having to deal with PDPTEs. We may add support for 32-bit hosts/VMs
	 * later if necessary.
	 */
<blue>	if (VALID_PAGE(mmu->root.hpa) && !to_shadow_page(mmu->root.hpa))</blue>
<yellow>		kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);</yellow>

	if (VALID_PAGE(mmu-&gt;root.hpa))
<blue>		return cached_root_find_and_keep_current(kvm, mmu, new_pgd, new_role);</blue>
	else
<blue>		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);</blue>
}

void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
{
<blue>	struct kvm_mmu *mmu = vcpu->arch.mmu;</blue>
	union kvm_mmu_page_role new_role = mmu-&gt;root_role;

<blue>	if (!fast_pgd_switch(vcpu->kvm, mmu, new_pgd, new_role)) {</blue>
		/* kvm_mmu_ensure_valid_pgd will set up a new root.  */
		return;
	}

	/*
	 * It&#x27;s possible that the cached previous root page is obsolete because
	 * of a change in the MMU generation number. However, changing the
	 * generation number is accompanied by KVM_REQ_MMU_FREE_OBSOLETE_ROOTS,
	 * which will free the root set here and allocate a new one.
	 */
<blue>	kvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);</blue>

<blue>	if (force_flush_and_sync_on_reuse) {</blue>
<yellow>		kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);</yellow>
		kvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);
	}

	/*
	 * The last MMIO access&#x27;s GVA and GPA are cached in the VCPU. When
	 * switching to a new CR3, that GVA-&gt;GPA mapping may no longer be
	 * valid. So clear any cached MMIO info even when we don&#x27;t need to sync
	 * the shadow page tables.
	 */
<blue>	vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);</blue>

	/*
	 * If this is a direct root page, it doesn&#x27;t have a write flooding
	 * count. Otherwise, clear the write flooding count.
	 */
	if (!new_role.direct)
		__clear_sp_write_flooding_count(
<blue>				to_shadow_page(vcpu->arch.mmu->root.hpa));</blue>
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_mmu_new_pgd);

static unsigned long get_cr3(struct kvm_vcpu *vcpu)
{
<blue>	return kvm_read_cr3(vcpu);</blue>
}

<yellow>static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,</yellow>
			   unsigned int access)
{
<yellow>	if (unlikely(is_mmio_spte(*sptep))) {</yellow>
<yellow>		if (gfn != get_mmio_spte_gfn(*sptep)) {</yellow>
<yellow>			mmu_spte_clear_no_track(sptep);</yellow>
			return true;
		}

<yellow>		mark_mmio_spte(vcpu, sptep, gfn, access);</yellow>
		return true;
	}

	return false;
<yellow>}</yellow>

#define PTTYPE_EPT 18 /* arbitrary */
#define PTTYPE PTTYPE_EPT
#include &quot;paging_tmpl.h&quot;
#undef PTTYPE

#define PTTYPE 64
#include &quot;paging_tmpl.h&quot;
#undef PTTYPE

#define PTTYPE 32
#include &quot;paging_tmpl.h&quot;
#undef PTTYPE

static void
__reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,
			u64 pa_bits_rsvd, int level, bool nx, bool gbpages,
			bool pse, bool amd)
{
	u64 gbpages_bit_rsvd = 0;
	u64 nonleaf_bit8_rsvd = 0;
	u64 high_bits_rsvd;

<blue>	rsvd_check->bad_mt_xwr = 0;</blue>

	if (!gbpages)
		gbpages_bit_rsvd = rsvd_bits(7, 7);

	if (level == PT32E_ROOT_LEVEL)
		high_bits_rsvd = pa_bits_rsvd &amp; rsvd_bits(0, 62);
	else
		high_bits_rsvd = pa_bits_rsvd &amp; rsvd_bits(0, 51);

	/* Note, NX doesn&#x27;t exist in PDPTEs, this is handled below. */
<blue>	if (!nx)</blue>
<blue>		high_bits_rsvd |= rsvd_bits(63, 63);</blue>

	/*
	 * Non-leaf PML4Es and PDPEs reserve bit 8 (which would be the G bit for
	 * leaf entries) on AMD CPUs only.
	 */
<blue>	if (amd)</blue>
		nonleaf_bit8_rsvd = rsvd_bits(8, 8);

	switch (level) {
	case PT32_ROOT_LEVEL:
		/* no rsvd bits for 2 level 4K page table entries */
<yellow>		rsvd_check->rsvd_bits_mask[0][1] = 0;</yellow>
		rsvd_check-&gt;rsvd_bits_mask[0][0] = 0;
		rsvd_check-&gt;rsvd_bits_mask[1][0] =
			rsvd_check-&gt;rsvd_bits_mask[0][0];

		if (!pse) {
<yellow>			rsvd_check->rsvd_bits_mask[1][1] = 0;</yellow>
			break;
		}

		if (is_cpuid_PSE36())
			/* 36bits PSE 4MB page */
<yellow>			rsvd_check->rsvd_bits_mask[1][1] = rsvd_bits(17, 21);</yellow>
		else
			/* 32 bits PSE 4MB page */
			rsvd_check-&gt;rsvd_bits_mask[1][1] = rsvd_bits(13, 21);
		break;
	case PT32E_ROOT_LEVEL:
<blue>		rsvd_check->rsvd_bits_mask[0][2] = rsvd_bits(63, 63) |</blue>
						   high_bits_rsvd |
						   rsvd_bits(5, 8) |
						   rsvd_bits(1, 2);	/* PDPTE */
		rsvd_check-&gt;rsvd_bits_mask[0][1] = high_bits_rsvd;	/* PDE */
		rsvd_check-&gt;rsvd_bits_mask[0][0] = high_bits_rsvd;	/* PTE */
		rsvd_check-&gt;rsvd_bits_mask[1][1] = high_bits_rsvd |
						   rsvd_bits(13, 20);	/* large page */
		rsvd_check-&gt;rsvd_bits_mask[1][0] =
			rsvd_check-&gt;rsvd_bits_mask[0][0];
		break;
	case PT64_ROOT_5LEVEL:
<blue>		rsvd_check->rsvd_bits_mask[0][4] = high_bits_rsvd |</blue>
						   nonleaf_bit8_rsvd |
						   rsvd_bits(7, 7);
		rsvd_check-&gt;rsvd_bits_mask[1][4] =
			rsvd_check-&gt;rsvd_bits_mask[0][4];
		fallthrough;
	case PT64_ROOT_4LEVEL:
<blue>		rsvd_check->rsvd_bits_mask[0][3] = high_bits_rsvd |</blue>
						   nonleaf_bit8_rsvd |
						   rsvd_bits(7, 7);
		rsvd_check-&gt;rsvd_bits_mask[0][2] = high_bits_rsvd |
						   gbpages_bit_rsvd;
		rsvd_check-&gt;rsvd_bits_mask[0][1] = high_bits_rsvd;
		rsvd_check-&gt;rsvd_bits_mask[0][0] = high_bits_rsvd;
		rsvd_check-&gt;rsvd_bits_mask[1][3] =
			rsvd_check-&gt;rsvd_bits_mask[0][3];
		rsvd_check-&gt;rsvd_bits_mask[1][2] = high_bits_rsvd |
						   gbpages_bit_rsvd |
						   rsvd_bits(13, 29);
		rsvd_check-&gt;rsvd_bits_mask[1][1] = high_bits_rsvd |
						   rsvd_bits(13, 20); /* large page */
		rsvd_check-&gt;rsvd_bits_mask[1][0] =
			rsvd_check-&gt;rsvd_bits_mask[0][0];
		break;
	}
<blue>}</blue>

<yellow>static bool guest_can_use_gbpages(struct kvm_vcpu *vcpu)</yellow>
{
	/*
	 * If TDP is enabled, let the guest use GBPAGES if they&#x27;re supported in
	 * hardware.  The hardware page walker doesn&#x27;t let KVM disable GBPAGES,
	 * i.e. won&#x27;t treat them as reserved, and KVM doesn&#x27;t redo the GVA-&gt;GPA
	 * walk for performance and complexity reasons.  Not to mention KVM
	 * _can&#x27;t_ solve the problem because GVA-&gt;GPA walks aren&#x27;t visible to
	 * KVM once a TDP translation is installed.  Mimic hardware behavior so
	 * that KVM&#x27;s is at least consistent, i.e. doesn&#x27;t randomly inject #PF.
	 */
<blue>	return tdp_enabled ? boot_cpu_has(X86_FEATURE_GBPAGES) :</blue>
<yellow>			     guest_cpuid_has(vcpu, X86_FEATURE_GBPAGES);</yellow>
<blue>}</blue>

static void reset_guest_rsvds_bits_mask(struct kvm_vcpu *vcpu,
					struct kvm_mmu *context)
{
	__reset_rsvds_bits_mask(&amp;context-&gt;guest_rsvd_check,
				vcpu-&gt;arch.reserved_gpa_bits,
				context-&gt;cpu_role.base.level, is_efer_nx(context),
				guest_can_use_gbpages(vcpu),
<blue>				is_cr4_pse(context),</blue>
<blue>				guest_cpuid_is_amd_or_hygon(vcpu));</blue>
}

static void
__reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,
			    u64 pa_bits_rsvd, bool execonly, int huge_page_level)
{
	u64 high_bits_rsvd = pa_bits_rsvd &amp; rsvd_bits(0, 51);
	u64 large_1g_rsvd = 0, large_2m_rsvd = 0;
	u64 bad_mt_xwr;

<blue>	if (huge_page_level < PG_LEVEL_1G)</blue>
		large_1g_rsvd = rsvd_bits(7, 7);
<yellow>	if (huge_page_level < PG_LEVEL_2M)</yellow>
		large_2m_rsvd = rsvd_bits(7, 7);

<blue>	rsvd_check->rsvd_bits_mask[0][4] = high_bits_rsvd | rsvd_bits(3, 7);</blue>
	rsvd_check-&gt;rsvd_bits_mask[0][3] = high_bits_rsvd | rsvd_bits(3, 7);
	rsvd_check-&gt;rsvd_bits_mask[0][2] = high_bits_rsvd | rsvd_bits(3, 6) | large_1g_rsvd;
	rsvd_check-&gt;rsvd_bits_mask[0][1] = high_bits_rsvd | rsvd_bits(3, 6) | large_2m_rsvd;
	rsvd_check-&gt;rsvd_bits_mask[0][0] = high_bits_rsvd;

	/* large page */
	rsvd_check-&gt;rsvd_bits_mask[1][4] = rsvd_check-&gt;rsvd_bits_mask[0][4];
	rsvd_check-&gt;rsvd_bits_mask[1][3] = rsvd_check-&gt;rsvd_bits_mask[0][3];
	rsvd_check-&gt;rsvd_bits_mask[1][2] = high_bits_rsvd | rsvd_bits(12, 29) | large_1g_rsvd;
	rsvd_check-&gt;rsvd_bits_mask[1][1] = high_bits_rsvd | rsvd_bits(12, 20) | large_2m_rsvd;
	rsvd_check-&gt;rsvd_bits_mask[1][0] = rsvd_check-&gt;rsvd_bits_mask[0][0];

	bad_mt_xwr = 0xFFull &lt;&lt; (2 * 8);	/* bits 3..5 must not be 2 */
	bad_mt_xwr |= 0xFFull &lt;&lt; (3 * 8);	/* bits 3..5 must not be 3 */
	bad_mt_xwr |= 0xFFull &lt;&lt; (7 * 8);	/* bits 3..5 must not be 7 */
	bad_mt_xwr |= REPEAT_BYTE(1ull &lt;&lt; 2);	/* bits 0..2 must not be 010 */
	bad_mt_xwr |= REPEAT_BYTE(1ull &lt;&lt; 6);	/* bits 0..2 must not be 110 */
	if (!execonly) {
		/* bits 0..2 must not be 100 unless VMX capabilities allow it */
		bad_mt_xwr |= REPEAT_BYTE(1ull &lt;&lt; 4);
	}
<blue>	rsvd_check->bad_mt_xwr = bad_mt_xwr;</blue>
}

static void reset_rsvds_bits_mask_ept(struct kvm_vcpu *vcpu,
		struct kvm_mmu *context, bool execonly, int huge_page_level)
{
	__reset_rsvds_bits_mask_ept(&amp;context-&gt;guest_rsvd_check,
				    vcpu-&gt;arch.reserved_gpa_bits, execonly,
				    huge_page_level);
}

static inline u64 reserved_hpa_bits(void)
{
<blue>	return rsvd_bits(shadow_phys_bits, 63);</blue>
}

/*
 * the page table on host is the shadow page table for the page
 * table in guest or amd nested guest, its mmu features completely
 * follow the features in guest.
 */
static void reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu,
					struct kvm_mmu *context)
{
	/* @amd adds a check on bit of SPTEs, which KVM shouldn&#x27;t use anyways. */
	bool is_amd = true;
	/* KVM doesn&#x27;t use 2-level page tables for the shadow MMU. */
	bool is_pse = false;
	struct rsvd_bits_validate *shadow_zero_check;
	int i;

<yellow>	WARN_ON_ONCE(context->root_role.level < PT32E_ROOT_LEVEL);</yellow>

<yellow>	shadow_zero_check = &context->shadow_zero_check;</yellow>
<yellow>	__reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),</yellow>
				context-&gt;root_role.level,
				context-&gt;root_role.efer_nx,
				guest_can_use_gbpages(vcpu), is_pse, is_amd);

	if (!shadow_me_mask)
		return;

<yellow>	for (i = context->root_role.level; --i >= 0;) {</yellow>
		/*
		 * So far shadow_me_value is a constant during KVM&#x27;s life
		 * time.  Bits in shadow_me_value are allowed to be set.
		 * Bits in shadow_me_mask but not in shadow_me_value are
		 * not allowed to be set.
		 */
<yellow>		shadow_zero_check->rsvd_bits_mask[0][i] |= shadow_me_mask;</yellow>
		shadow_zero_check-&gt;rsvd_bits_mask[1][i] |= shadow_me_mask;
<yellow>		shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_value;</yellow>
		shadow_zero_check-&gt;rsvd_bits_mask[1][i] &amp;= ~shadow_me_value;
	}

}

static inline bool boot_cpu_is_amd(void)
{
<yellow>	WARN_ON_ONCE(!tdp_enabled);</yellow>
	return shadow_x_mask == 0;
}

/*
 * the direct page table on host, use as much mmu features as
 * possible, however, kvm currently does not do execution-protection.
 */
static void
reset_tdp_shadow_zero_bits_mask(struct kvm_mmu *context)
{
	struct rsvd_bits_validate *shadow_zero_check;
	int i;

	shadow_zero_check = &amp;context-&gt;shadow_zero_check;

<blue>	if (boot_cpu_is_amd())</blue>
<yellow>		__reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),</yellow>
					context-&gt;root_role.level, true,
<yellow>					boot_cpu_has(X86_FEATURE_GBPAGES),</yellow>
					false, true);
	else
<blue>		__reset_rsvds_bits_mask_ept(shadow_zero_check,</blue>
					    reserved_hpa_bits(), false,
					    max_huge_page_level);

<blue>	if (!shadow_me_mask)</blue>
		return;

<yellow>	for (i = context->root_role.level; --i >= 0;) {</yellow>
<yellow>		shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_mask;</yellow>
		shadow_zero_check-&gt;rsvd_bits_mask[1][i] &amp;= ~shadow_me_mask;
	}
}

/*
 * as the comments in reset_shadow_zero_bits_mask() except it
 * is the shadow page table for intel nested guest.
 */
static void
reset_ept_shadow_zero_bits_mask(struct kvm_mmu *context, bool execonly)
{
<blue>	__reset_rsvds_bits_mask_ept(&context->shadow_zero_check,</blue>
				    reserved_hpa_bits(), execonly,
				    max_huge_page_level);
}

#define BYTE_MASK(access) \
	((1 &amp; (access) ? 2 : 0) | \
	 (2 &amp; (access) ? 4 : 0) | \
	 (3 &amp; (access) ? 8 : 0) | \
	 (4 &amp; (access) ? 16 : 0) | \
	 (5 &amp; (access) ? 32 : 0) | \
	 (6 &amp; (access) ? 64 : 0) | \
	 (7 &amp; (access) ? 128 : 0))


static void update_permission_bitmask(struct kvm_mmu *mmu, bool ept)
{
	unsigned byte;

	const u8 x = BYTE_MASK(ACC_EXEC_MASK);
	const u8 w = BYTE_MASK(ACC_WRITE_MASK);
	const u8 u = BYTE_MASK(ACC_USER_MASK);

<blue>	bool cr4_smep = is_cr4_smep(mmu);</blue>
	bool cr4_smap = is_cr4_smap(mmu);
	bool cr0_wp = is_cr0_wp(mmu);
	bool efer_nx = is_efer_nx(mmu);

	for (byte = 0; byte &lt; ARRAY_SIZE(mmu-&gt;permissions); ++byte) {
		unsigned pfec = byte &lt;&lt; 1;

		/*
		 * Each &quot;*f&quot; variable has a 1 bit for each UWX value
		 * that causes a fault with the given PFEC.
		 */

		/* Faults from writes to non-writable pages */
<blue>		u8 wf = (pfec & PFERR_WRITE_MASK) ? (u8)~w : 0;</blue>
		/* Faults from user mode accesses to supervisor pages */
<blue>		u8 uf = (pfec & PFERR_USER_MASK) ? (u8)~u : 0;</blue>
		/* Faults from fetches of non-executable pages*/
<blue>		u8 ff = (pfec & PFERR_FETCH_MASK) ? (u8)~x : 0;</blue>
		/* Faults from kernel mode fetches of user pages */
		u8 smepf = 0;
		/* Faults from kernel mode accesses of user pages */
<blue>		u8 smapf = 0;</blue>

<blue>		if (!ept) {</blue>
			/* Faults from kernel mode accesses to user pages */
<blue>			u8 kf = (pfec & PFERR_USER_MASK) ? 0 : u;</blue>

			/* Not really needed: !nx will cause pte.nx to fault */
<blue>			if (!efer_nx)</blue>
				ff = 0;

			/* Allow supervisor writes if !cr0.wp */
<blue>			if (!cr0_wp)</blue>
<blue>				wf = (pfec & PFERR_USER_MASK) ? wf : 0;</blue>

			/* Disallow supervisor fetches of user code if cr4.smep */
<blue>			if (cr4_smep)</blue>
<blue>				smepf = (pfec & PFERR_FETCH_MASK) ? kf : 0;</blue>

			/*
			 * SMAP:kernel-mode data accesses from user-mode
			 * mappings should fault. A fault is considered
			 * as a SMAP violation if all of the following
			 * conditions are true:
			 *   - X86_CR4_SMAP is set in CR4
			 *   - A user page is accessed
			 *   - The access is not a fetch
			 *   - The access is supervisor mode
			 *   - If implicit supervisor access or X86_EFLAGS_AC is clear
			 *
			 * Here, we cover the first four conditions.
			 * The fifth is computed dynamically in permission_fault();
			 * PFERR_RSVD_MASK bit will be set in PFEC if the access is
			 * *not* subject to SMAP restrictions.
			 */
<blue>			if (cr4_smap)</blue>
<blue>				smapf = (pfec & (PFERR_RSVD_MASK|PFERR_FETCH_MASK)) ? 0 : kf;</blue>
		}

<blue>		mmu->permissions[byte] = ff | uf | wf | smepf | smapf;</blue>
	}
<blue>}</blue>

/*
* PKU is an additional mechanism by which the paging controls access to
* user-mode addresses based on the value in the PKRU register.  Protection
* key violations are reported through a bit in the page fault error code.
* Unlike other bits of the error code, the PK bit is not known at the
* call site of e.g. gva_to_gpa; it must be computed directly in
* permission_fault based on two bits of PKRU, on some machine state (CR4,
* CR0, EFER, CPL), and on other bits of the error code and the page tables.
*
* In particular the following conditions come from the error code, the
* page tables and the machine state:
* - PK is always zero unless CR4.PKE=1 and EFER.LMA=1
* - PK is always zero if RSVD=1 (reserved bit set) or F=1 (instruction fetch)
* - PK is always zero if U=0 in the page tables
* - PKRU.WD is ignored if CR0.WP=0 and the access is a supervisor access.
*
* The PKRU bitmask caches the result of these four conditions.  The error
* code (minus the P bit) and the page table&#x27;s U bit form an index into the
* PKRU bitmask.  Two bits of the PKRU bitmask are then extracted and ANDed
* with the two bits of the PKRU register corresponding to the protection key.
* For the first three conditions above the bits will be 00, thus masking
* away both AD and WD.  For all reads or if the last condition holds, WD
* only will be masked away.
*/
static void update_pkru_bitmask(struct kvm_mmu *mmu)
{
	unsigned bit;
	bool wp;

	mmu-&gt;pkru_mask = 0;

	if (!is_cr4_pke(mmu))
		return;

<blue>	wp = is_cr0_wp(mmu);</blue>

<blue>	for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {</blue>
		unsigned pfec, pkey_bits;
		bool check_pkey, check_write, ff, uf, wf, pte_user;

<blue>		pfec = bit << 1;</blue>
		ff = pfec &amp; PFERR_FETCH_MASK;
<blue>		uf = pfec & PFERR_USER_MASK;</blue>
<blue>		wf = pfec & PFERR_WRITE_MASK;</blue>

		/* PFEC.RSVD is replaced by ACC_USER_MASK. */
<blue>		pte_user = pfec & PFERR_RSVD_MASK;</blue>

		/*
		 * Only need to check the access which is not an
		 * instruction fetch and is to a user page.
		 */
		check_pkey = (!ff &amp;&amp; pte_user);
		/*
		 * write access is controlled by PKRU if it is a
		 * user access or CR0.WP = 1.
		 */
		check_write = check_pkey &amp;&amp; wf &amp;&amp; (uf || wp);

		/* PKRU.AD stops both read and write access. */
		pkey_bits = !!check_pkey;
		/* PKRU.WD stops write access. */
		pkey_bits |= (!!check_write) &lt;&lt; 1;

<blue>		mmu->pkru_mask |= (pkey_bits & 3) << pfec;</blue>
	}
}

<blue>static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,</blue>
					struct kvm_mmu *mmu)
{
<blue>	if (!is_cr0_pg(mmu))</blue>
		return;

<blue>	reset_guest_rsvds_bits_mask(vcpu, mmu);</blue>
	update_permission_bitmask(mmu, false);
<blue>	update_pkru_bitmask(mmu);</blue>
<blue>}</blue>

static void paging64_init_context(struct kvm_mmu *context)
{
	context-&gt;page_fault = paging64_page_fault;
	context-&gt;gva_to_gpa = paging64_gva_to_gpa;
	context-&gt;sync_page = paging64_sync_page;
	context-&gt;invlpg = paging64_invlpg;
}

static void paging32_init_context(struct kvm_mmu *context)
{
	context-&gt;page_fault = paging32_page_fault;
	context-&gt;gva_to_gpa = paging32_gva_to_gpa;
	context-&gt;sync_page = paging32_sync_page;
	context-&gt;invlpg = paging32_invlpg;
}

static union kvm_cpu_role
kvm_calc_cpu_role(struct kvm_vcpu *vcpu, const struct kvm_mmu_role_regs *regs)
{
	union kvm_cpu_role role = {0};

	role.base.access = ACC_ALL;
<blue>	role.base.smm = is_smm(vcpu);</blue>
	role.base.guest_mode = is_guest_mode(vcpu);
	role.ext.valid = 1;

	if (!____is_cr0_pg(regs)) {
<blue>		role.base.direct = 1;</blue>
		return role;
	}

<blue>	role.base.efer_nx = ____is_efer_nx(regs);</blue>
	role.base.cr0_wp = ____is_cr0_wp(regs);
<blue>	role.base.smep_andnot_wp = ____is_cr4_smep(regs) && !____is_cr0_wp(regs);</blue>
<blue>	role.base.smap_andnot_wp = ____is_cr4_smap(regs) && !____is_cr0_wp(regs);</blue>
<blue>	role.base.has_4_byte_gpte = !____is_cr4_pae(regs);</blue>

	if (____is_efer_lma(regs))
<blue>		role.base.level = ____is_cr4_la57(regs) ? PT64_ROOT_5LEVEL</blue>
							: PT64_ROOT_4LEVEL;
<blue>	else if (____is_cr4_pae(regs))</blue>
		role.base.level = PT32E_ROOT_LEVEL;
	else
		role.base.level = PT32_ROOT_LEVEL;

<blue>	role.ext.cr4_smep = ____is_cr4_smep(regs);</blue>
	role.ext.cr4_smap = ____is_cr4_smap(regs);
	role.ext.cr4_pse = ____is_cr4_pse(regs);

	/* PKEY and LA57 are active iff long mode is active. */
	role.ext.cr4_pke = ____is_efer_lma(regs) &amp;&amp; ____is_cr4_pke(regs);
	role.ext.cr4_la57 = ____is_efer_lma(regs) &amp;&amp; ____is_cr4_la57(regs);
<blue>	role.ext.efer_lma = ____is_efer_lma(regs);</blue>
	return role;
<blue>}</blue>

<blue>static inline int kvm_mmu_get_tdp_level(struct kvm_vcpu *vcpu)</blue>
{
	/* tdp_root_level is architecture forced level, use it if nonzero */
<blue>	if (tdp_root_level)</blue>
		return tdp_root_level;

	/* Use 5-level TDP if and only if it&#x27;s useful/necessary. */
<blue>	if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)</blue>
		return 4;

	return max_tdp_level;
}

static union kvm_mmu_page_role
kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,
				union kvm_cpu_role cpu_role)
{
	union kvm_mmu_page_role role = {0};

	role.access = ACC_ALL;
	role.cr0_wp = true;
	role.efer_nx = true;
<blue>	role.smm = cpu_role.base.smm;</blue>
	role.guest_mode = cpu_role.base.guest_mode;
	role.ad_disabled = !kvm_ad_enabled();
<blue>	role.level = kvm_mmu_get_tdp_level(vcpu);</blue>
	role.direct = true;
	role.has_4_byte_gpte = false;

	return role;
}

static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
			     union kvm_cpu_role cpu_role)
{
	struct kvm_mmu *context = &amp;vcpu-&gt;arch.root_mmu;
<blue>	union kvm_mmu_page_role root_role = kvm_calc_tdp_mmu_root_page_role(vcpu, cpu_role);</blue>

	if (cpu_role.as_u64 == context-&gt;cpu_role.as_u64 &amp;&amp;
<blue>	    root_role.word == context->root_role.word)</blue>
		return;

	context-&gt;cpu_role.as_u64 = cpu_role.as_u64;
<blue>	context->root_role.word = root_role.word;</blue>
	context-&gt;page_fault = kvm_tdp_page_fault;
	context-&gt;sync_page = nonpaging_sync_page;
	context-&gt;invlpg = NULL;
	context-&gt;get_guest_pgd = get_cr3;
	context-&gt;get_pdptr = kvm_pdptr_read;
	context-&gt;inject_page_fault = kvm_inject_page_fault;

<blue>	if (!is_cr0_pg(context))</blue>
		context-&gt;gva_to_gpa = nonpaging_gva_to_gpa;
<blue>	else if (is_cr4_pae(context))</blue>
		context-&gt;gva_to_gpa = paging64_gva_to_gpa;
	else
		context-&gt;gva_to_gpa = paging32_gva_to_gpa;

	reset_guest_paging_metadata(vcpu, context);
<blue>	reset_tdp_shadow_zero_bits_mask(context);</blue>
}

static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
				    union kvm_cpu_role cpu_role,
				    union kvm_mmu_page_role root_role)
<yellow>{</yellow>
<yellow>	if (cpu_role.as_u64 == context->cpu_role.as_u64 &&</yellow>
<yellow>	    root_role.word == context->root_role.word)</yellow>
		return;

	context-&gt;cpu_role.as_u64 = cpu_role.as_u64;
<yellow>	context->root_role.word = root_role.word;</yellow>

<yellow>	if (!is_cr0_pg(context))</yellow>
		nonpaging_init_context(context);
<yellow>	else if (is_cr4_pae(context))</yellow>
		paging64_init_context(context);
	else
		paging32_init_context(context);

	reset_guest_paging_metadata(vcpu, context);
<yellow>	reset_shadow_zero_bits_mask(vcpu, context);</yellow>
}

static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
				union kvm_cpu_role cpu_role)
{
	struct kvm_mmu *context = &amp;vcpu-&gt;arch.root_mmu;
	union kvm_mmu_page_role root_role;

	root_role = cpu_role.base;

	/* KVM uses PAE paging whenever the guest isn&#x27;t using 64-bit paging. */
	root_role.level = max_t(u32, root_role.level, PT32E_ROOT_LEVEL);

	/*
	 * KVM forces EFER.NX=1 when TDP is disabled, reflect it in the MMU role.
	 * KVM uses NX when TDP is disabled to handle a variety of scenarios,
	 * notably for huge SPTEs if iTLB multi-hit mitigation is enabled and
	 * to generate correct permissions for CR0.WP=0/CR4.SMEP=1/EFER.NX=0.
	 * The iTLB multi-hit workaround can be toggled at any time, so assume
	 * NX can be used by any non-nested shadow MMU to avoid having to reset
	 * MMU contexts.
	 */
	root_role.efer_nx = true;

	shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
}

void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,
			     unsigned long cr4, u64 efer, gpa_t nested_cr3)
{
	struct kvm_mmu *context = &amp;vcpu-&gt;arch.guest_mmu;
<yellow>	struct kvm_mmu_role_regs regs = {</yellow>
		.cr0 = cr0,
		.cr4 = cr4 &amp; ~X86_CR4_PKE,
		.efer = efer,
	};
	union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &amp;regs);
	union kvm_mmu_page_role root_role;

	/* NPT requires CR0.PG=1. */
<yellow>	WARN_ON_ONCE(cpu_role.base.direct);</yellow>

	root_role = cpu_role.base;
<yellow>	root_role.level = kvm_mmu_get_tdp_level(vcpu);</yellow>
<yellow>	if (root_role.level == PT64_ROOT_5LEVEL &&</yellow>
<yellow>	    cpu_role.base.level == PT64_ROOT_4LEVEL)</yellow>
		root_role.passthrough = 1;

<yellow>	shadow_mmu_init_context(vcpu, context, cpu_role, root_role);</yellow>
	kvm_mmu_new_pgd(vcpu, nested_cr3);
}
EXPORT_SYMBOL_GPL(kvm_init_shadow_npt_mmu);

static union kvm_cpu_role
kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
				   bool execonly, u8 level)
{
	union kvm_cpu_role role = {0};

	/*
	 * KVM does not support SMM transfer monitors, and consequently does not
	 * support the &quot;entry to SMM&quot; control either.  role.base.smm is always 0.
	 */
<yellow>	WARN_ON_ONCE(is_smm(vcpu));</yellow>
	role.base.level = level;
	role.base.has_4_byte_gpte = false;
	role.base.direct = false;
<blue>	role.base.ad_disabled = !accessed_dirty;</blue>
	role.base.guest_mode = true;
	role.base.access = ACC_ALL;

	role.ext.word = 0;
	role.ext.execonly = execonly;
	role.ext.valid = 1;

	return role;
}

void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
			     int huge_page_level, bool accessed_dirty,
			     gpa_t new_eptp)
{
	struct kvm_mmu *context = &amp;vcpu-&gt;arch.guest_mmu;
<blue>	u8 level = vmx_eptp_page_walk_level(new_eptp);</blue>
	union kvm_cpu_role new_mode =
<blue>		kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty,</blue>
						   execonly, level);

	if (new_mode.as_u64 != context-&gt;cpu_role.as_u64) {
		/* EPT, and thus nested EPT, does not consume CR0, CR4, nor EFER. */
		context-&gt;cpu_role.as_u64 = new_mode.as_u64;
<blue>		context->root_role.word = new_mode.base.word;</blue>

		context-&gt;page_fault = ept_page_fault;
		context-&gt;gva_to_gpa = ept_gva_to_gpa;
		context-&gt;sync_page = ept_sync_page;
		context-&gt;invlpg = ept_invlpg;

		update_permission_bitmask(context, true);
		context-&gt;pkru_mask = 0;
		reset_rsvds_bits_mask_ept(vcpu, context, execonly, huge_page_level);
<blue>		reset_ept_shadow_zero_bits_mask(context, execonly);</blue>
	}

<blue>	kvm_mmu_new_pgd(vcpu, new_eptp);</blue>
}
EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);

static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
			     union kvm_cpu_role cpu_role)
{
	struct kvm_mmu *context = &amp;vcpu-&gt;arch.root_mmu;

	kvm_init_shadow_mmu(vcpu, cpu_role);

	context-&gt;get_guest_pgd     = get_cr3;
	context-&gt;get_pdptr         = kvm_pdptr_read;
	context-&gt;inject_page_fault = kvm_inject_page_fault;
}

static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
				union kvm_cpu_role new_mode)
{
	struct kvm_mmu *g_context = &amp;vcpu-&gt;arch.nested_mmu;

<blue>	if (new_mode.as_u64 == g_context->cpu_role.as_u64)</blue>
		return;

	g_context-&gt;cpu_role.as_u64   = new_mode.as_u64;
<blue>	g_context->get_guest_pgd     = get_cr3;</blue>
	g_context-&gt;get_pdptr         = kvm_pdptr_read;
	g_context-&gt;inject_page_fault = kvm_inject_page_fault;

	/*
	 * L2 page tables are never shadowed, so there is no need to sync
	 * SPTEs.
	 */
	g_context-&gt;invlpg            = NULL;

	/*
	 * Note that arch.mmu-&gt;gva_to_gpa translates l2_gpa to l1_gpa using
	 * L1&#x27;s nested page tables (e.g. EPT12). The nested translation
	 * of l2_gva to l1_gpa is done by arch.nested_mmu.gva_to_gpa using
	 * L2&#x27;s page tables as the first level of translation and L1&#x27;s
	 * nested page tables as the second level of translation. Basically
	 * the gva_to_gpa functions between mmu and nested_mmu are swapped.
	 */
<blue>	if (!is_paging(vcpu))</blue>
		g_context-&gt;gva_to_gpa = nonpaging_gva_to_gpa;
<blue>	else if (is_long_mode(vcpu))</blue>
		g_context-&gt;gva_to_gpa = paging64_gva_to_gpa;
<blue>	else if (is_pae(vcpu))</blue>
		g_context-&gt;gva_to_gpa = paging64_gva_to_gpa;
	else
		g_context-&gt;gva_to_gpa = paging32_gva_to_gpa;

	reset_guest_paging_metadata(vcpu, g_context);
}

void kvm_init_mmu(struct kvm_vcpu *vcpu)
<blue>{</blue>
<blue>	struct kvm_mmu_role_regs regs = vcpu_to_role_regs(vcpu);</blue>
	union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &amp;regs);

	if (mmu_is_nested(vcpu))
<blue>		init_kvm_nested_mmu(vcpu, cpu_role);</blue>
<blue>	else if (tdp_enabled)</blue>
<blue>		init_kvm_tdp_mmu(vcpu, cpu_role);</blue>
	else
<yellow>		init_kvm_softmmu(vcpu, cpu_role);</yellow>
}
EXPORT_SYMBOL_GPL(kvm_init_mmu);

void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)
{
	/*
	 * Invalidate all MMU roles to force them to reinitialize as CPUID
	 * information is factored into reserved bit calculations.
	 *
	 * Correctly handling multiple vCPU models with respect to paging and
	 * physical address properties) in a single VM would require tracking
	 * all relevant CPUID information in kvm_mmu_page_role. That is very
	 * undesirable as it would increase the memory requirements for
	 * gfn_track (see struct kvm_mmu_page_role comments).  For now that
	 * problem is swept under the rug; KVM&#x27;s CPUID API is horrific and
	 * it&#x27;s all but impossible to solve it without introducing a new API.
	 */
<yellow>	vcpu->arch.root_mmu.root_role.word = 0;</yellow>
	vcpu-&gt;arch.guest_mmu.root_role.word = 0;
	vcpu-&gt;arch.nested_mmu.root_role.word = 0;
	vcpu-&gt;arch.root_mmu.cpu_role.ext.valid = 0;
	vcpu-&gt;arch.guest_mmu.cpu_role.ext.valid = 0;
	vcpu-&gt;arch.nested_mmu.cpu_role.ext.valid = 0;
<yellow>	kvm_mmu_reset_context(vcpu);</yellow>

	/*
	 * Changing guest CPUID after KVM_RUN is forbidden, see the comment in
	 * kvm_arch_vcpu_ioctl().
	 */
<yellow>	KVM_BUG_ON(vcpu->arch.last_vmentry_cpu != -1, vcpu->kvm);</yellow>
<yellow>}</yellow>

void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
{
<blue>	kvm_mmu_unload(vcpu);</blue>
	kvm_init_mmu(vcpu);
}
EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);

<blue>int kvm_mmu_load(struct kvm_vcpu *vcpu)</blue>
<blue>{</blue>
	int r;

<blue>	r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);</blue>
	if (r)
		goto out;
<blue>	r = mmu_alloc_special_roots(vcpu);</blue>
	if (r)
		goto out;
<blue>	if (vcpu->arch.mmu->root_role.direct)</blue>
<blue>		r = mmu_alloc_direct_roots(vcpu);</blue>
	else
<blue>		r = mmu_alloc_shadow_roots(vcpu);</blue>
	if (r)
		goto out;

<blue>	kvm_mmu_sync_roots(vcpu);</blue>

<blue>	kvm_mmu_load_pgd(vcpu);</blue>

	/*
	 * Flush any TLB entries for the new root, the provenance of the root
	 * is unknown.  Even if KVM ensures there are no stale TLB entries
	 * for a freed root, in theory another hypervisor could have left
	 * stale entries.  Flushing on alloc also allows KVM to skip the TLB
	 * flush when freeing a root (see kvm_tdp_mmu_put_root()).
	 */
<blue>	static_call(kvm_x86_flush_tlb_current)(vcpu);</blue>
out:
	return r;
}

void kvm_mmu_unload(struct kvm_vcpu *vcpu)
{
<blue>	struct kvm *kvm = vcpu->kvm;</blue>

	kvm_mmu_free_roots(kvm, &amp;vcpu-&gt;arch.root_mmu, KVM_MMU_ROOTS_ALL);
<yellow>	WARN_ON(VALID_PAGE(vcpu->arch.root_mmu.root.hpa));</yellow>
<blue>	kvm_mmu_free_roots(kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);</blue>
<yellow>	WARN_ON(VALID_PAGE(vcpu->arch.guest_mmu.root.hpa));</yellow>
<blue>	vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);</blue>
}

static bool is_obsolete_root(struct kvm *kvm, hpa_t root_hpa)
{
	struct kvm_mmu_page *sp;

	if (!VALID_PAGE(root_hpa))
		return false;

	/*
	 * When freeing obsolete roots, treat roots as obsolete if they don&#x27;t
	 * have an associated shadow page.  This does mean KVM will get false
	 * positives and free roots that don&#x27;t strictly need to be freed, but
	 * such false positives are relatively rare:
	 *
	 *  (a) only PAE paging and nested NPT has roots without shadow pages
	 *  (b) remote reloads due to a memslot update obsoletes _all_ roots
	 *  (c) KVM doesn&#x27;t track previous roots for PAE paging, and the guest
	 *      is unlikely to zap an in-use PGD.
	 */
<blue>	sp = to_shadow_page(root_hpa);</blue>
<blue>	return !sp || is_obsolete_sp(kvm, sp);</blue>
}

static void __kvm_mmu_free_obsolete_roots(struct kvm *kvm, struct kvm_mmu *mmu)
{
	unsigned long roots_to_free = 0;
	int i;

<blue>	if (is_obsolete_root(kvm, mmu->root.hpa))</blue>
		roots_to_free |= KVM_MMU_ROOT_CURRENT;

<blue>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {</blue>
<blue>		if (is_obsolete_root(kvm, mmu->prev_roots[i].hpa))</blue>
<blue>			roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);</blue>
	}

<blue>	if (roots_to_free)</blue>
<blue>		kvm_mmu_free_roots(kvm, mmu, roots_to_free);</blue>
<blue>}</blue>

void kvm_mmu_free_obsolete_roots(struct kvm_vcpu *vcpu)
{
<blue>	__kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.root_mmu);</blue>
	__kvm_mmu_free_obsolete_roots(vcpu-&gt;kvm, &amp;vcpu-&gt;arch.guest_mmu);
}

static u64 mmu_pte_write_fetch_gpte(struct kvm_vcpu *vcpu, gpa_t *gpa,
				    int *bytes)
{
	u64 gentry = 0;
	int r;

	/*
	 * Assume that the pte write on a page table of the same type
	 * as the current vcpu paging mode since we update the sptes only
	 * when they have the same mode.
	 */
<blue>	if (is_pae(vcpu) && *bytes == 4) {</blue>
		/* Handle a 32-bit guest writing two halves of a 64-bit gpte */
<blue>		*gpa &= ~(gpa_t)7;</blue>
		*bytes = 8;
	}

<blue>	if (*bytes == 4 || *bytes == 8) {</blue>
<blue>		r = kvm_vcpu_read_guest_atomic(vcpu, *gpa, &gentry, *bytes);</blue>
		if (r)
<yellow>			gentry = 0;</yellow>
	}

<blue>	return gentry;</blue>
}

/*
 * If we&#x27;re seeing too many writes to a page, it may no longer be a page table,
 * or we may be forking, in which case it is better to unmap the page.
 */
static bool detect_write_flooding(struct kvm_mmu_page *sp)
{
	/*
	 * Skip write-flooding detected for the sp whose level is 1, because
	 * it can become unsync, then the guest page is not write-protected.
	 */
<yellow>	if (sp->role.level == PG_LEVEL_4K)</yellow>
		return false;

<yellow>	atomic_inc(&sp->write_flooding_count);</yellow>
	return atomic_read(&amp;sp-&gt;write_flooding_count) &gt;= 3;
}

/*
 * Misaligned accesses are too much trouble to fix up; also, they usually
 * indicate a page is not used as a page table.
 */
static bool detect_write_misaligned(struct kvm_mmu_page *sp, gpa_t gpa,
				    int bytes)
{
	unsigned offset, pte_size, misaligned;

	pgprintk(&quot;misaligned: gpa %llx bytes %d role %x\n&quot;,
		 gpa, bytes, sp-&gt;role.word);

<blue>	offset = offset_in_page(gpa);</blue>
<yellow>	pte_size = sp->role.has_4_byte_gpte ? 4 : 8;</yellow>

	/*
	 * Sometimes, the OS only writes the last one bytes to update status
	 * bits, for example, in linux, andb instruction is used in clear_bit().
	 */
<yellow>	if (!(offset & (pte_size - 1)) && bytes == 1)</yellow>
		return false;

<yellow>	misaligned = (offset ^ (offset + bytes - 1)) & ~(pte_size - 1);</yellow>
	misaligned |= bytes &lt; 4;

	return misaligned;
}

static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
{
	unsigned page_offset, quadrant;
	u64 *spte;
	int level;

	page_offset = offset_in_page(gpa);
<yellow>	level = sp->role.level;</yellow>
	*nspte = 1;
<yellow>	if (sp->role.has_4_byte_gpte) {</yellow>
		page_offset &lt;&lt;= 1;	/* 32-&gt;64 */
		/*
		 * A 32-bit pde maps 4MB while the shadow pdes map
		 * only 2MB.  So we need to double the offset again
		 * and zap two pdes instead of one.
		 */
<yellow>		if (level == PT32_ROOT_LEVEL) {</yellow>
			page_offset &amp;= ~7; /* kill rounding error */
			page_offset &lt;&lt;= 1;
			*nspte = 2;
		}
		quadrant = page_offset &gt;&gt; PAGE_SHIFT;
<yellow>		page_offset &= ~PAGE_MASK;</yellow>
<yellow>		if (quadrant != sp->role.quadrant)</yellow>
			return NULL;
	}

<yellow>	spte = &sp->spt[page_offset / sizeof(*spte)];</yellow>
	return spte;
}

static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
			      const u8 *new, int bytes,
			      struct kvm_page_track_notifier_node *node)
<blue>{</blue>
	gfn_t gfn = gpa &gt;&gt; PAGE_SHIFT;
	struct kvm_mmu_page *sp;
<blue>	LIST_HEAD(invalid_list);</blue>
	u64 entry, gentry, *spte;
	int npte;
	bool flush = false;

	/*
	 * If we don&#x27;t have indirect shadow pages, it means no page is
	 * write-protected, so we can exit simply.
	 */
	if (!READ_ONCE(vcpu-&gt;kvm-&gt;arch.indirect_shadow_pages))
		return;

	pgprintk(&quot;%s: gpa %llx bytes %d\n&quot;, __func__, gpa, bytes);

<blue>	write_lock(&vcpu->kvm->mmu_lock);</blue>

<blue>	gentry = mmu_pte_write_fetch_gpte(vcpu, &gpa, &bytes);</blue>

<blue>	++vcpu->kvm->stat.mmu_pte_write;</blue>

<blue>	for_each_gfn_valid_sp_with_gptes(vcpu->kvm, sp, gfn) {</blue>
<blue>		if (detect_write_misaligned(sp, gpa, bytes) ||</blue>
<yellow>		      detect_write_flooding(sp)) {</yellow>
<yellow>			kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);</yellow>
			++vcpu-&gt;kvm-&gt;stat.mmu_flooded;
			continue;
		}

<yellow>		spte = get_written_sptes(sp, gpa, &npte);</yellow>
		if (!spte)
			continue;

<yellow>		while (npte--) {</yellow>
<yellow>			entry = *spte;</yellow>
			mmu_page_zap_pte(vcpu-&gt;kvm, sp, spte, NULL);
<yellow>			if (gentry && sp->role.level != PG_LEVEL_4K)</yellow>
<yellow>				++vcpu->kvm->stat.mmu_pde_zapped;</yellow>
<yellow>			if (is_shadow_present_pte(entry))</yellow>
				flush = true;
			++spte;
		}
	}
<blue>	kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);</blue>
<blue>	write_unlock(&vcpu->kvm->mmu_lock);</blue>
}

int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
		       void *insn, int insn_len)
<blue>{</blue>
	int r, emulation_type = EMULTYPE_PF;
<blue>	bool direct = vcpu->arch.mmu->root_role.direct;</blue>

<yellow>	if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root.hpa)))</yellow>
		return RET_PF_RETRY;

	r = RET_PF_INVALID;
<blue>	if (unlikely(error_code & PFERR_RSVD_MASK)) {</blue>
<blue>		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);</blue>
		if (r == RET_PF_EMULATE)
			goto emulate;
	}

	if (r == RET_PF_INVALID) {
<blue>		r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,</blue>
					  lower_32_bits(error_code), false);
<blue>		if (KVM_BUG_ON(r == RET_PF_INVALID, vcpu->kvm))</blue>
			return -EIO;
	}

<blue>	if (r < 0)</blue>
		return r;
<blue>	if (r != RET_PF_EMULATE)</blue>
		return 1;

	/*
	 * Before emulating the instruction, check if the error code
	 * was due to a RO violation while translating the guest page.
	 * This can occur when using nested virtualization with nested
	 * paging in both guests. If true, we simply unprotect the page
	 * and resume the guest.
	 */
<blue>	if (vcpu->arch.mmu->root_role.direct &&</blue>
<blue>	    (error_code & PFERR_NESTED_GUEST_PAGE) == PFERR_NESTED_GUEST_PAGE) {</blue>
<yellow>		kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(cr2_or_gpa));</yellow>
		return 1;
	}

	/*
	 * vcpu-&gt;arch.mmu.page_fault returned RET_PF_EMULATE, but we can still
	 * optimistically try to just unprotect the page and let the processor
	 * re-execute the instruction that caused the page fault.  Do not allow
	 * retrying MMIO emulation, as it&#x27;s not only pointless but could also
	 * cause us to enter an infinite loop because the processor will keep
	 * faulting on the non-existent MMIO address.  Retrying an instruction
	 * from a nested guest is also pointless and dangerous as we are only
	 * explicitly shadowing L1&#x27;s page tables, i.e. unprotecting something
	 * for L1 isn&#x27;t going to magically fix whatever issue cause L2 to fail.
	 */
<blue>	if (!mmio_info_in_cache(vcpu, cr2_or_gpa, direct) && !is_guest_mode(vcpu))</blue>
		emulation_type |= EMULTYPE_ALLOW_RETRY_PF;
emulate:
<blue>	return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,</blue>
				       insn_len);
}
EXPORT_SYMBOL_GPL(kvm_mmu_page_fault);

void kvm_mmu_invalidate_gva(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
			    gva_t gva, hpa_t root_hpa)
{
	int i;

	/* It&#x27;s actually a GPA for vcpu-&gt;arch.guest_mmu.  */
<yellow>	if (mmu != &vcpu->arch.guest_mmu) {</yellow>
		/* INVLPG on a non-canonical address is a NOP according to the SDM.  */
<yellow>		if (is_noncanonical_address(gva, vcpu))</yellow>
			return;

<yellow>		static_call(kvm_x86_flush_tlb_gva)(vcpu, gva);</yellow>
	}

<yellow>	if (!mmu->invlpg)</yellow>
		return;

<yellow>	if (root_hpa == INVALID_PAGE) {</yellow>
<yellow>		mmu->invlpg(vcpu, gva, mmu->root.hpa);</yellow>

		/*
		 * INVLPG is required to invalidate any global mappings for the VA,
		 * irrespective of PCID. Since it would take us roughly similar amount
		 * of work to determine whether any of the prev_root mappings of the VA
		 * is marked global, or to just sync it blindly, so we might as well
		 * just always sync it.
		 *
		 * Mappings not reachable via the current cr3 or the prev_roots will be
		 * synced when switching to that cr3, so nothing needs to be done here
		 * for them.
		 */
<yellow>		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)</yellow>
<yellow>			if (VALID_PAGE(mmu->prev_roots[i].hpa))</yellow>
<yellow>				mmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);</yellow>
	} else {
<yellow>		mmu->invlpg(vcpu, gva, root_hpa);</yellow>
	}
<yellow>}</yellow>

void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)
{
<yellow>	kvm_mmu_invalidate_gva(vcpu, vcpu->arch.walk_mmu, gva, INVALID_PAGE);</yellow>
	++vcpu-&gt;stat.invlpg;
}
EXPORT_SYMBOL_GPL(kvm_mmu_invlpg);


void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
{
<yellow>	struct kvm_mmu *mmu = vcpu->arch.mmu;</yellow>
	bool tlb_flush = false;
	uint i;

<yellow>	if (pcid == kvm_get_active_pcid(vcpu)) {</yellow>
<yellow>		if (mmu->invlpg)</yellow>
<yellow>			mmu->invlpg(vcpu, gva, mmu->root.hpa);</yellow>
		tlb_flush = true;
	}

<yellow>	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {</yellow>
<yellow>		if (VALID_PAGE(mmu->prev_roots[i].hpa) &&</yellow>
<yellow>		    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd)) {</yellow>
<yellow>			if (mmu->invlpg)</yellow>
<yellow>				mmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);</yellow>
			tlb_flush = true;
		}
	}

<yellow>	if (tlb_flush)</yellow>
<yellow>		static_call(kvm_x86_flush_tlb_gva)(vcpu, gva);</yellow>

<yellow>	++vcpu->stat.invlpg;</yellow>

	/*
	 * Mappings not reachable via the current cr3 or the prev_roots will be
	 * synced when switching to that cr3, so nothing needs to be done here
	 * for them.
	 */
}

<yellow>void kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,</yellow>
		       int tdp_max_root_level, int tdp_huge_page_level)
{
<yellow>	tdp_enabled = enable_tdp;</yellow>
	tdp_root_level = tdp_forced_root_level;
	max_tdp_level = tdp_max_root_level;

	/*
	 * max_huge_page_level reflects KVM&#x27;s MMU capabilities irrespective
	 * of kernel support, e.g. KVM may be capable of using 1GB pages when
	 * the kernel is not.  But, KVM never creates a page size greater than
	 * what is used by the kernel for any given HVA, i.e. the kernel&#x27;s
	 * capabilities are ultimately consulted by kvm_mmu_hugepage_adjust().
	 */
<yellow>	if (tdp_enabled)</yellow>
<yellow>		max_huge_page_level = tdp_huge_page_level;</yellow>
<yellow>	else if (boot_cpu_has(X86_FEATURE_GBPAGES))</yellow>
		max_huge_page_level = PG_LEVEL_1G;
	else
		max_huge_page_level = PG_LEVEL_2M;
}
EXPORT_SYMBOL_GPL(kvm_configure_mmu);

/* The return value indicates if tlb flush on all vcpus is needed. */
typedef bool (*slot_level_handler) (struct kvm *kvm,
				    struct kvm_rmap_head *rmap_head,
				    const struct kvm_memory_slot *slot);

/* The caller should hold mmu-lock before calling this function. */
static __always_inline bool
slot_handle_level_range(struct kvm *kvm, const struct kvm_memory_slot *memslot,
			slot_level_handler fn, int start_level, int end_level,
			gfn_t start_gfn, gfn_t end_gfn, bool flush_on_yield,
			bool flush)
{
	struct slot_rmap_walk_iterator iterator;

<yellow>	for_each_slot_rmap_range(memslot, start_level, end_level, start_gfn,</yellow>
			end_gfn, &amp;iterator) {
		if (iterator.rmap)
<yellow>			flush |= fn(kvm, iterator.rmap, memslot);</yellow>

<yellow>		if (need_resched() || rwlock_needbreak(&kvm->mmu_lock)) {</yellow>
<yellow>			if (flush && flush_on_yield) {</yellow>
<yellow>				kvm_flush_remote_tlbs_with_address(kvm,</yellow>
						start_gfn,
						iterator.gfn - start_gfn + 1);
				flush = false;
			}
<yellow>			cond_resched_rwlock_write(&kvm->mmu_lock);</yellow>
		}
	}

	return flush;
}

static __always_inline bool
slot_handle_level(struct kvm *kvm, const struct kvm_memory_slot *memslot,
		  slot_level_handler fn, int start_level, int end_level,
		  bool flush_on_yield)
{
<yellow>	return slot_handle_level_range(kvm, memslot, fn, start_level,</yellow>
			end_level, memslot-&gt;base_gfn,
<yellow>			memslot->base_gfn + memslot->npages - 1,</yellow>
			flush_on_yield, false);
}

static __always_inline bool
slot_handle_level_4k(struct kvm *kvm, const struct kvm_memory_slot *memslot,
		     slot_level_handler fn, bool flush_on_yield)
{
<yellow>	return slot_handle_level(kvm, memslot, fn, PG_LEVEL_4K,</yellow>
				 PG_LEVEL_4K, flush_on_yield);
}

static void free_mmu_pages(struct kvm_mmu *mmu)
{
<yellow>	if (!tdp_enabled && mmu->pae_root)</yellow>
<yellow>		set_memory_encrypted((unsigned long)mmu->pae_root, 1);</yellow>
<yellow>	free_page((unsigned long)mmu->pae_root);</yellow>
	free_page((unsigned long)mmu-&gt;pml4_root);
	free_page((unsigned long)mmu-&gt;pml5_root);
}

static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
{
	struct page *page;
	int i;

<blue>	mmu->root.hpa = INVALID_PAGE;</blue>
	mmu-&gt;root.pgd = 0;
	for (i = 0; i &lt; KVM_MMU_NUM_PREV_ROOTS; i++)
		mmu-&gt;prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;

	/* vcpu-&gt;arch.guest_mmu isn&#x27;t used when !tdp_enabled. */
<blue>	if (!tdp_enabled && mmu == &vcpu->arch.guest_mmu)</blue>
		return 0;

	/*
	 * When using PAE paging, the four PDPTEs are treated as &#x27;root&#x27; pages,
	 * while the PDP table is a per-vCPU construct that&#x27;s allocated at MMU
	 * creation.  When emulating 32-bit mode, cr3 is only 32 bits even on
	 * x86_64.  Therefore we need to allocate the PDP table in the first
	 * 4GB of memory, which happens to fit the DMA32 zone.  TDP paging
	 * generally doesn&#x27;t use PAE paging and can skip allocating the PDP
	 * table.  The main exception, handled here, is SVM&#x27;s 32-bit NPT.  The
	 * other exception is for shadowing L1&#x27;s 32-bit or PAE NPT on 64-bit
	 * KVM; that horror is handled on-demand by mmu_alloc_special_roots().
	 */
<blue>	if (tdp_enabled && kvm_mmu_get_tdp_level(vcpu) > PT32E_ROOT_LEVEL)</blue>
		return 0;

<yellow>	page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_DMA32);</yellow>
	if (!page)
		return -ENOMEM;

<yellow>	mmu->pae_root = page_address(page);</yellow>

	/*
	 * CR3 is only 32 bits when PAE paging is used, thus it&#x27;s impossible to
	 * get the CPU to treat the PDPTEs as encrypted.  Decrypt the page so
	 * that KVM&#x27;s writes and the CPU&#x27;s reads get along.  Note, this is
	 * only necessary when using shadow paging, as 64-bit NPT can get at
	 * the C-bit even when shadowing 32-bit NPT, and SME isn&#x27;t supported
	 * by 32-bit kernels (when KVM itself uses 32-bit NPT).
	 */
<yellow>	if (!tdp_enabled)</yellow>
<yellow>		set_memory_decrypted((unsigned long)mmu->pae_root, 1);</yellow>
	else
<yellow>		WARN_ON_ONCE(shadow_me_value);</yellow>

	for (i = 0; i &lt; 4; ++i)
<yellow>		mmu->pae_root[i] = INVALID_PAE_ROOT;</yellow>

	return 0;
<blue>}</blue>

int kvm_mmu_create(struct kvm_vcpu *vcpu)
{
	int ret;

<blue>	vcpu->arch.mmu_pte_list_desc_cache.kmem_cache = pte_list_desc_cache;</blue>
	vcpu-&gt;arch.mmu_pte_list_desc_cache.gfp_zero = __GFP_ZERO;

	vcpu-&gt;arch.mmu_page_header_cache.kmem_cache = mmu_page_header_cache;
	vcpu-&gt;arch.mmu_page_header_cache.gfp_zero = __GFP_ZERO;

	vcpu-&gt;arch.mmu_shadow_page_cache.gfp_zero = __GFP_ZERO;

	vcpu-&gt;arch.mmu = &amp;vcpu-&gt;arch.root_mmu;
	vcpu-&gt;arch.walk_mmu = &amp;vcpu-&gt;arch.root_mmu;

	ret = __kvm_mmu_create(vcpu, &amp;vcpu-&gt;arch.guest_mmu);
	if (ret)
		return ret;

<blue>	ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);</blue>
	if (ret)
		goto fail_allocate_root;

	return ret;
 fail_allocate_root:
<yellow>	free_mmu_pages(&vcpu->arch.guest_mmu);</yellow>
	return ret;
<blue>}</blue>

#define BATCH_ZAP_PAGES	10
static void kvm_zap_obsolete_pages(struct kvm *kvm)
{
	struct kvm_mmu_page *sp, *node;
	int nr_zapped, batch = 0;
	bool unstable;

restart:
<yellow>	list_for_each_entry_safe_reverse(sp, node,</yellow>
	      &amp;kvm-&gt;arch.active_mmu_pages, link) {
		/*
		 * No obsolete valid page exists before a newly created page
		 * since active_mmu_pages is a FIFO list.
		 */
<yellow>		if (!is_obsolete_sp(kvm, sp))</yellow>
			break;

		/*
		 * Invalid pages should never land back on the list of active
		 * pages.  Skip the bogus page, otherwise we&#x27;ll get stuck in an
		 * infinite loop if the page gets put back on the list (again).
		 */
<yellow>		if (WARN_ON(sp->role.invalid))</yellow>
			continue;

		/*
		 * No need to flush the TLB since we&#x27;re only zapping shadow
		 * pages with an obsolete generation number and all vCPUS have
		 * loaded a new root, i.e. the shadow pages being zapped cannot
		 * be in active use by the guest.
		 */
<yellow>		if (batch >= BATCH_ZAP_PAGES &&</yellow>
<yellow>		    cond_resched_rwlock_write(&kvm->mmu_lock)) {</yellow>
			batch = 0;
			goto restart;
		}

<yellow>		unstable = __kvm_mmu_prepare_zap_page(kvm, sp,</yellow>
				&amp;kvm-&gt;arch.zapped_obsolete_pages, &amp;nr_zapped);
		batch += nr_zapped;

		if (unstable)
			goto restart;
	}

	/*
	 * Kick all vCPUs (via remote TLB flush) before freeing the page tables
	 * to ensure KVM is not in the middle of a lockless shadow page table
	 * walk, which may reference the pages.  The remote TLB flush itself is
	 * not required and is simply a convenient way to kick vCPUs as needed.
	 * KVM performs a local TLB flush when allocating a new root (see
	 * kvm_mmu_load()), and the reload in the caller ensure no vCPUs are
	 * running with an obsolete MMU.
	 */
<yellow>	kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);</yellow>
}

/*
 * Fast invalidate all shadow pages and use lock-break technique
 * to zap obsolete pages.
 *
 * It&#x27;s required when memslot is being deleted or VM is being
 * destroyed, in these cases, we should ensure that KVM MMU does
 * not use any resource of the being-deleted slot or all slots
 * after calling the function.
 */
static void kvm_mmu_zap_all_fast(struct kvm *kvm)
<yellow>{</yellow>
	lockdep_assert_held(&amp;kvm-&gt;slots_lock);

<yellow>	write_lock(&kvm->mmu_lock);</yellow>
<yellow>	trace_kvm_mmu_zap_all_fast(kvm);</yellow>

	/*
	 * Toggle mmu_valid_gen between &#x27;0&#x27; and &#x27;1&#x27;.  Because slots_lock is
	 * held for the entire duration of zapping obsolete pages, it&#x27;s
	 * impossible for there to be multiple invalid generations associated
	 * with *valid* shadow pages at any given time, i.e. there is exactly
	 * one valid generation and (at most) one invalid generation.
	 */
<yellow>	kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;</yellow>

	/*
	 * In order to ensure all vCPUs drop their soon-to-be invalid roots,
	 * invalidating TDP MMU roots must be done while holding mmu_lock for
	 * write and in the same critical section as making the reload request,
	 * e.g. before kvm_zap_obsolete_pages() could drop mmu_lock and yield.
	 */
<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
<yellow>		kvm_tdp_mmu_invalidate_all_roots(kvm);</yellow>

	/*
	 * Notify all vcpus to reload its shadow page table and flush TLB.
	 * Then all vcpus will switch to new shadow page table with the new
	 * mmu_valid_gen.
	 *
	 * Note: we need to do this under the protection of mmu_lock,
	 * otherwise, vcpu would purge shadow page but miss tlb flush.
	 */
<yellow>	kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);</yellow>

<yellow>	kvm_zap_obsolete_pages(kvm);</yellow>

<yellow>	write_unlock(&kvm->mmu_lock);</yellow>

	/*
	 * Zap the invalidated TDP MMU roots, all SPTEs must be dropped before
	 * returning to the caller, e.g. if the zap is in response to a memslot
	 * deletion, mmu_notifier callbacks will be unable to reach the SPTEs
	 * associated with the deleted memslot once the update completes, and
	 * Deferring the zap until the final reference to the root is put would
	 * lead to use-after-free.
	 */
<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
<yellow>		kvm_tdp_mmu_zap_invalidated_roots(kvm);</yellow>
}

static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
{
<yellow>	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));</yellow>
}

static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
			struct kvm_memory_slot *slot,
			struct kvm_page_track_notifier_node *node)
{
<yellow>	kvm_mmu_zap_all_fast(kvm);</yellow>
}

int kvm_mmu_init_vm(struct kvm *kvm)
{
	struct kvm_page_track_notifier_node *node = &amp;kvm-&gt;arch.mmu_sp_tracker;
	int r;

<blue>	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);</blue>
	INIT_LIST_HEAD(&amp;kvm-&gt;arch.zapped_obsolete_pages);
	INIT_LIST_HEAD(&amp;kvm-&gt;arch.lpage_disallowed_mmu_pages);
	spin_lock_init(&amp;kvm-&gt;arch.mmu_unsync_pages_lock);

	r = kvm_mmu_init_tdp_mmu(kvm);
	if (r &lt; 0)
		return r;

<blue>	node->track_write = kvm_mmu_pte_write;</blue>
	node-&gt;track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
	kvm_page_track_register_notifier(kvm, node);

	kvm-&gt;arch.split_page_header_cache.kmem_cache = mmu_page_header_cache;
	kvm-&gt;arch.split_page_header_cache.gfp_zero = __GFP_ZERO;

	kvm-&gt;arch.split_shadow_page_cache.gfp_zero = __GFP_ZERO;

	kvm-&gt;arch.split_desc_cache.kmem_cache = pte_list_desc_cache;
	kvm-&gt;arch.split_desc_cache.gfp_zero = __GFP_ZERO;

	return 0;
<blue>}</blue>

static void mmu_free_vm_memory_caches(struct kvm *kvm)
{
	kvm_mmu_free_memory_cache(&amp;kvm-&gt;arch.split_desc_cache);
	kvm_mmu_free_memory_cache(&amp;kvm-&gt;arch.split_page_header_cache);
	kvm_mmu_free_memory_cache(&amp;kvm-&gt;arch.split_shadow_page_cache);
}

void kvm_mmu_uninit_vm(struct kvm *kvm)
{
<yellow>	struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;</yellow>

	kvm_page_track_unregister_notifier(kvm, node);

	kvm_mmu_uninit_tdp_mmu(kvm);

	mmu_free_vm_memory_caches(kvm);
}

static bool kvm_rmap_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
<yellow>{</yellow>
	const struct kvm_memory_slot *memslot;
	struct kvm_memslots *slots;
	struct kvm_memslot_iter iter;
	bool flush = false;
	gfn_t start, end;
	int i;

<yellow>	if (!kvm_memslots_have_rmaps(kvm))</yellow>
		return flush;

<yellow>	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {</yellow>
<yellow>		slots = __kvm_memslots(kvm, i);</yellow>

<yellow>		kvm_for_each_memslot_in_gfn_range(&iter, slots, gfn_start, gfn_end) {</yellow>
			memslot = iter.slot;
<yellow>			start = max(gfn_start, memslot->base_gfn);</yellow>
			end = min(gfn_end, memslot-&gt;base_gfn + memslot-&gt;npages);
<yellow>			if (WARN_ON_ONCE(start >= end))</yellow>
				continue;

<yellow>			flush = slot_handle_level_range(kvm, memslot, __kvm_zap_rmap,</yellow>
							PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
							start, end - 1, true, flush);
		}
	}

	return flush;
}

/*
 * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
 * (not including it)
 */
void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
<yellow>{</yellow>
	bool flush;
	int i;

<yellow>	if (WARN_ON_ONCE(gfn_end <= gfn_start))</yellow>
		return;

<yellow>	write_lock(&kvm->mmu_lock);</yellow>

	kvm_mmu_invalidate_begin(kvm, 0, -1ul);

	flush = kvm_rmap_zap_gfn_range(kvm, gfn_start, gfn_end);

<yellow>	if (is_tdp_mmu_enabled(kvm)) {</yellow>
		for (i = 0; i &lt; KVM_ADDRESS_SPACE_NUM; i++)
<yellow>			flush = kvm_tdp_mmu_zap_leafs(kvm, i, gfn_start,</yellow>
						      gfn_end, true, flush);
	}

<yellow>	if (flush)</yellow>
<yellow>		kvm_flush_remote_tlbs_with_address(kvm, gfn_start,</yellow>
						   gfn_end - gfn_start);

<yellow>	kvm_mmu_invalidate_end(kvm, 0, -1ul);</yellow>

	write_unlock(&amp;kvm-&gt;mmu_lock);
}

static bool slot_rmap_write_protect(struct kvm *kvm,
				    struct kvm_rmap_head *rmap_head,
				    const struct kvm_memory_slot *slot)
{
<yellow>	return rmap_write_protect(rmap_head, false);</yellow>
}

void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
				      const struct kvm_memory_slot *memslot,
				      int start_level)
<yellow>{</yellow>
<yellow>	if (kvm_memslots_have_rmaps(kvm)) {</yellow>
<yellow>		write_lock(&kvm->mmu_lock);</yellow>
<yellow>		slot_handle_level(kvm, memslot, slot_rmap_write_protect,</yellow>
				  start_level, KVM_MAX_HUGEPAGE_LEVEL, false);
<yellow>		write_unlock(&kvm->mmu_lock);</yellow>
	}

<yellow>	if (is_tdp_mmu_enabled(kvm)) {</yellow>
<yellow>		read_lock(&kvm->mmu_lock);</yellow>
		kvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);
		read_unlock(&amp;kvm-&gt;mmu_lock);
	}
}

static inline bool need_topup(struct kvm_mmu_memory_cache *cache, int min)
{
<yellow>	return kvm_mmu_memory_cache_nr_free_objects(cache) < min;</yellow>
}

static bool need_topup_split_caches_or_resched(struct kvm *kvm)
{
<yellow>	if (need_resched() || rwlock_needbreak(&kvm->mmu_lock))</yellow>
		return true;

	/*
	 * In the worst case, SPLIT_DESC_CACHE_MIN_NR_OBJECTS descriptors are needed
	 * to split a single huge page. Calculating how many are actually needed
	 * is possible but not worth the complexity.
	 */
<yellow>	return need_topup(&kvm->arch.split_desc_cache, SPLIT_DESC_CACHE_MIN_NR_OBJECTS) ||</yellow>
<yellow>	       need_topup(&kvm->arch.split_page_header_cache, 1) ||</yellow>
<yellow>	       need_topup(&kvm->arch.split_shadow_page_cache, 1);</yellow>
}

static int topup_split_caches(struct kvm *kvm)
{
	/*
	 * Allocating rmap list entries when splitting huge pages for nested
	 * MMUs is uncommon as KVM needs to use a list if and only if there is
	 * more than one rmap entry for a gfn, i.e. requires an L1 gfn to be
	 * aliased by multiple L2 gfns and/or from multiple nested roots with
	 * different roles.  Aliasing gfns when using TDP is atypical for VMMs;
	 * a few gfns are often aliased during boot, e.g. when remapping BIOS,
	 * but aliasing rarely occurs post-boot or for many gfns.  If there is
	 * only one rmap entry, rmap-&gt;val points directly at that one entry and
	 * doesn&#x27;t need to allocate a list.  Buffer the cache by the default
	 * capacity so that KVM doesn&#x27;t have to drop mmu_lock to topup if KVM
	 * encounters an aliased gfn or two.
	 */
	const int capacity = SPLIT_DESC_CACHE_MIN_NR_OBJECTS +
			     KVM_ARCH_NR_OBJS_PER_MEMORY_CACHE;
	int r;

	lockdep_assert_held(&amp;kvm-&gt;slots_lock);

	r = __kvm_mmu_topup_memory_cache(&amp;kvm-&gt;arch.split_desc_cache, capacity,
					 SPLIT_DESC_CACHE_MIN_NR_OBJECTS);
	if (r)
		return r;

<yellow>	r = kvm_mmu_topup_memory_cache(&kvm->arch.split_page_header_cache, 1);</yellow>
	if (r)
		return r;

<yellow>	return kvm_mmu_topup_memory_cache(&kvm->arch.split_shadow_page_cache, 1);</yellow>
}

static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *huge_sptep)
{
<yellow>	struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);</yellow>
	struct shadow_page_caches caches = {};
	union kvm_mmu_page_role role;
	unsigned int access;
	gfn_t gfn;

	gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
<yellow>	access = kvm_mmu_page_get_access(huge_sp, spte_index(huge_sptep));</yellow>

	/*
	 * Note, huge page splitting always uses direct shadow pages, regardless
	 * of whether the huge page itself is mapped by a direct or indirect
	 * shadow page, since the huge page region itself is being directly
	 * mapped with smaller pages.
	 */
<yellow>	role = kvm_mmu_child_role(huge_sptep, /*direct=*/true, access);</yellow>

	/* Direct SPs do not require a shadowed_info_cache. */
	caches.page_header_cache = &amp;kvm-&gt;arch.split_page_header_cache;
	caches.shadow_page_cache = &amp;kvm-&gt;arch.split_shadow_page_cache;

	/* Safe to pass NULL for vCPU since requesting a direct SP. */
	return __kvm_mmu_get_shadow_page(kvm, NULL, &amp;caches, gfn, role);
}

static void shadow_mmu_split_huge_page(struct kvm *kvm,
				       const struct kvm_memory_slot *slot,
				       u64 *huge_sptep)

{
	struct kvm_mmu_memory_cache *cache = &amp;kvm-&gt;arch.split_desc_cache;
<yellow>	u64 huge_spte = READ_ONCE(*huge_sptep);</yellow>
	struct kvm_mmu_page *sp;
	bool flush = false;
	u64 *sptep, spte;
	gfn_t gfn;
	int index;

<yellow>	sp = shadow_mmu_get_sp_for_split(kvm, huge_sptep);</yellow>

<yellow>	for (index = 0; index < SPTE_ENT_PER_PAGE; index++) {</yellow>
<yellow>		sptep = &sp->spt[index];</yellow>
		gfn = kvm_mmu_page_get_gfn(sp, index);

		/*
		 * The SP may already have populated SPTEs, e.g. if this huge
		 * page is aliased by multiple sptes with the same access
		 * permissions. These entries are guaranteed to map the same
		 * gfn-to-pfn translation since the SP is direct, so no need to
		 * modify them.
		 *
		 * However, if a given SPTE points to a lower level page table,
		 * that lower level page table may only be partially populated.
		 * Installing such SPTEs would effectively unmap a potion of the
		 * huge page. Unmapping guest memory always requires a TLB flush
		 * since a subsequent operation on the unmapped regions would
		 * fail to detect the need to flush.
		 */
		if (is_shadow_present_pte(*sptep)) {
<yellow>			flush |= !is_last_spte(*sptep, sp->role.level);</yellow>
			continue;
		}

<yellow>		spte = make_huge_page_split_spte(kvm, huge_spte, sp->role, index);</yellow>
<yellow>		mmu_spte_set(sptep, spte);</yellow>
		__rmap_add(kvm, cache, slot, sptep, gfn, sp-&gt;role.access);
	}

<yellow>	__link_shadow_page(kvm, cache, huge_sptep, sp, flush);</yellow>
}

static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
					  const struct kvm_memory_slot *slot,
					  u64 *huge_sptep)
{
<yellow>	struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);</yellow>
	int level, r = 0;
	gfn_t gfn;
	u64 spte;

	/* Grab information for the tracepoint before dropping the MMU lock. */
	gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
	level = huge_sp-&gt;role.level;
	spte = *huge_sptep;

<yellow>	if (kvm_mmu_available_pages(kvm) <= KVM_MIN_FREE_MMU_PAGES) {</yellow>
		r = -ENOSPC;
		goto out;
	}

<yellow>	if (need_topup_split_caches_or_resched(kvm)) {</yellow>
<yellow>		write_unlock(&kvm->mmu_lock);</yellow>
		cond_resched();
		/*
		 * If the topup succeeds, return -EAGAIN to indicate that the
		 * rmap iterator should be restarted because the MMU lock was
		 * dropped.
		 */
<yellow>		r = topup_split_caches(kvm) ?: -EAGAIN;</yellow>
<yellow>		write_lock(&kvm->mmu_lock);</yellow>
		goto out;
	}

<yellow>	shadow_mmu_split_huge_page(kvm, slot, huge_sptep);</yellow>

out:
<yellow>	trace_kvm_mmu_split_huge_page(gfn, spte, level, r);</yellow>
	return r;
}

static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
					    struct kvm_rmap_head *rmap_head,
					    const struct kvm_memory_slot *slot)
{
	struct rmap_iterator iter;
	struct kvm_mmu_page *sp;
	u64 *huge_sptep;
	int r;

restart:
<yellow>	for_each_rmap_spte(rmap_head, &iter, huge_sptep) {</yellow>
<yellow>		sp = sptep_to_sp(huge_sptep);</yellow>

		/* TDP MMU is enabled, so rmap only contains nested MMU SPs. */
<yellow>		if (WARN_ON_ONCE(!sp->role.guest_mode))</yellow>
			continue;

		/* The rmaps should never contain non-leaf SPTEs. */
<yellow>		if (WARN_ON_ONCE(!is_large_pte(*huge_sptep)))</yellow>
			continue;

		/* SPs with level &gt;PG_LEVEL_4K should never by unsync. */
<yellow>		if (WARN_ON_ONCE(sp->unsync))</yellow>
			continue;

		/* Don&#x27;t bother splitting huge pages on invalid SPs. */
<yellow>		if (sp->role.invalid)</yellow>
			continue;

<yellow>		r = shadow_mmu_try_split_huge_page(kvm, slot, huge_sptep);</yellow>

		/*
		 * The split succeeded or needs to be retried because the MMU
		 * lock was dropped. Either way, restart the iterator to get it
		 * back into a consistent state.
		 */
<yellow>		if (!r || r == -EAGAIN)</yellow>
			goto restart;

		/* The split failed and shouldn&#x27;t be retried (e.g. -ENOMEM). */
		break;
	}

	return false;
}

static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
						const struct kvm_memory_slot *slot,
						gfn_t start, gfn_t end,
						int target_level)
<yellow>{</yellow>
	int level;

	/*
	 * Split huge pages starting with KVM_MAX_HUGEPAGE_LEVEL and working
	 * down to the target level. This ensures pages are recursively split
	 * all the way to the target level. There&#x27;s no need to split pages
	 * already at the target level.
	 */
<yellow>	for (level = KVM_MAX_HUGEPAGE_LEVEL; level > target_level; level--) {</yellow>
<yellow>		slot_handle_level_range(kvm, slot, shadow_mmu_try_split_huge_pages,</yellow>
					level, level, start, end - 1, true, false);
	}
}

/* Must be called with the mmu_lock held in write-mode. */
<yellow>void kvm_mmu_try_split_huge_pages(struct kvm *kvm,</yellow>
				   const struct kvm_memory_slot *memslot,
				   u64 start, u64 end,
				   int target_level)
{
<yellow>	if (!is_tdp_mmu_enabled(kvm))</yellow>
		return;

<yellow>	if (kvm_memslots_have_rmaps(kvm))</yellow>
<yellow>		kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);</yellow>

<yellow>	kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, false);</yellow>

	/*
	 * A TLB flush is unnecessary at this point for the same resons as in
	 * kvm_mmu_slot_try_split_huge_pages().
	 */
<yellow>}</yellow>

void kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,
					const struct kvm_memory_slot *memslot,
					int target_level)
{
<yellow>	u64 start = memslot->base_gfn;</yellow>
<yellow>	u64 end = start + memslot->npages;</yellow>

<yellow>	if (!is_tdp_mmu_enabled(kvm))</yellow>
		return;

<yellow>	if (kvm_memslots_have_rmaps(kvm)) {</yellow>
<yellow>		write_lock(&kvm->mmu_lock);</yellow>
		kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
		write_unlock(&amp;kvm-&gt;mmu_lock);
	}

<yellow>	read_lock(&kvm->mmu_lock);</yellow>
	kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);
	read_unlock(&amp;kvm-&gt;mmu_lock);

	/*
	 * No TLB flush is necessary here. KVM will flush TLBs after
	 * write-protecting and/or clearing dirty on the newly split SPTEs to
	 * ensure that guest writes are reflected in the dirty log before the
	 * ioctl to enable dirty logging on this memslot completes. Since the
	 * split SPTEs retain the write and dirty bits of the huge SPTE, it is
	 * safe for KVM to decide if a TLB flush is necessary based on the split
	 * SPTEs.
	 */
<yellow>}</yellow>

static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,
					 struct kvm_rmap_head *rmap_head,
					 const struct kvm_memory_slot *slot)
{
	u64 *sptep;
	struct rmap_iterator iter;
	int need_tlb_flush = 0;
	struct kvm_mmu_page *sp;

restart:
<yellow>	for_each_rmap_spte(rmap_head, &iter, sptep) {</yellow>
<yellow>		sp = sptep_to_sp(sptep);</yellow>

		/*
		 * We cannot do huge page mapping for indirect shadow pages,
		 * which are found on the last rmap (level = 1) when not using
		 * tdp; such shadow pages are synced with the page table in
		 * the guest, and the guest page table is using 4K page size
		 * mapping if the indirect sp has level = 1.
		 */
		if (sp-&gt;role.direct &amp;&amp;
<yellow>		    sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn,</yellow>
							       PG_LEVEL_NUM)) {
<yellow>			kvm_zap_one_rmap_spte(kvm, rmap_head, sptep);</yellow>

			if (kvm_available_flush_tlb_with_range())
<yellow>				kvm_flush_remote_tlbs_with_address(kvm, sp->gfn,</yellow>
<yellow>					KVM_PAGES_PER_HPAGE(sp->role.level));</yellow>
			else
				need_tlb_flush = 1;

			goto restart;
		}
	}

<yellow>	return need_tlb_flush;</yellow>
}

static void kvm_rmap_zap_collapsible_sptes(struct kvm *kvm,
					   const struct kvm_memory_slot *slot)
<yellow>{</yellow>
	/*
	 * Note, use KVM_MAX_HUGEPAGE_LEVEL - 1 since there&#x27;s no need to zap
	 * pages that are already mapped at the maximum hugepage level.
	 */
<yellow>	if (slot_handle_level(kvm, slot, kvm_mmu_zap_collapsible_spte,</yellow>
			      PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL - 1, true))
<yellow>		kvm_arch_flush_remote_tlbs_memslot(kvm, slot);</yellow>
<yellow>}</yellow>

void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
				   const struct kvm_memory_slot *slot)
{
<yellow>	if (kvm_memslots_have_rmaps(kvm)) {</yellow>
<yellow>		write_lock(&kvm->mmu_lock);</yellow>
		kvm_rmap_zap_collapsible_sptes(kvm, slot);
		write_unlock(&amp;kvm-&gt;mmu_lock);
	}

<yellow>	if (is_tdp_mmu_enabled(kvm)) {</yellow>
<yellow>		read_lock(&kvm->mmu_lock);</yellow>
		kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);
		read_unlock(&amp;kvm-&gt;mmu_lock);
	}
<yellow>}</yellow>

void kvm_arch_flush_remote_tlbs_memslot(struct kvm *kvm,
					const struct kvm_memory_slot *memslot)
<yellow>{</yellow>
	/*
	 * All current use cases for flushing the TLBs for a specific memslot
	 * related to dirty logging, and many do the TLB flush out of mmu_lock.
	 * The interaction between the various operations on memslot must be
	 * serialized by slots_locks to ensure the TLB flush from one operation
	 * is observed by any other operation on the same memslot.
	 */
	lockdep_assert_held(&amp;kvm-&gt;slots_lock);
<yellow>	kvm_flush_remote_tlbs_with_address(kvm, memslot->base_gfn,</yellow>
<yellow>					   memslot->npages);</yellow>
}

void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
				   const struct kvm_memory_slot *memslot)
<yellow>{</yellow>
<yellow>	if (kvm_memslots_have_rmaps(kvm)) {</yellow>
<yellow>		write_lock(&kvm->mmu_lock);</yellow>
		/*
		 * Clear dirty bits only on 4k SPTEs since the legacy MMU only
		 * support dirty logging at a 4k granularity.
		 */
<yellow>		slot_handle_level_4k(kvm, memslot, __rmap_clear_dirty, false);</yellow>
<yellow>		write_unlock(&kvm->mmu_lock);</yellow>
	}

<yellow>	if (is_tdp_mmu_enabled(kvm)) {</yellow>
<yellow>		read_lock(&kvm->mmu_lock);</yellow>
		kvm_tdp_mmu_clear_dirty_slot(kvm, memslot);
		read_unlock(&amp;kvm-&gt;mmu_lock);
	}

	/*
	 * The caller will flush the TLBs after this function returns.
	 *
	 * It&#x27;s also safe to flush TLBs out of mmu lock here as currently this
	 * function is only used for dirty logging, in which case flushing TLB
	 * out of mmu lock also guarantees no dirty pages will be lost in
	 * dirty_bitmap.
	 */
}

void kvm_mmu_zap_all(struct kvm *kvm)
{
	struct kvm_mmu_page *sp, *node;
<yellow>	LIST_HEAD(invalid_list);</yellow>
	int ign;

	write_lock(&amp;kvm-&gt;mmu_lock);
restart:
<yellow>	list_for_each_entry_safe(sp, node, &kvm->arch.active_mmu_pages, link) {</yellow>
<yellow>		if (WARN_ON(sp->role.invalid))</yellow>
			continue;
<yellow>		if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))</yellow>
			goto restart;
<yellow>		if (cond_resched_rwlock_write(&kvm->mmu_lock))</yellow>
			goto restart;
	}

<yellow>	kvm_mmu_commit_zap_page(kvm, &invalid_list);</yellow>

<yellow>	if (is_tdp_mmu_enabled(kvm))</yellow>
<yellow>		kvm_tdp_mmu_zap_all(kvm);</yellow>

<yellow>	write_unlock(&kvm->mmu_lock);</yellow>
}

void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen)
{
<blue>	WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);</blue>

	gen &amp;= MMIO_SPTE_GEN_MASK;

	/*
	 * Generation numbers are incremented in multiples of the number of
	 * address spaces in order to provide unique generations across all
	 * address spaces.  Strip what is effectively the address space
	 * modifier prior to checking for a wrap of the MMIO generation so
	 * that a wrap in any address space is detected.
	 */
<blue>	gen &= ~((u64)KVM_ADDRESS_SPACE_NUM - 1);</blue>

	/*
	 * The very rare case: if the MMIO generation number has wrapped,
	 * zap all shadow pages.
	 */
	if (unlikely(gen == 0)) {
<yellow>		kvm_debug_ratelimited("kvm: zapping shadow pages for mmio generation wraparound\n");</yellow>
<yellow>		kvm_mmu_zap_all_fast(kvm);</yellow>
	}
<blue>}</blue>

static unsigned long
mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
{
	struct kvm *kvm;
<yellow>	int nr_to_scan = sc->nr_to_scan;</yellow>
	unsigned long freed = 0;

	mutex_lock(&amp;kvm_lock);

<yellow>	list_for_each_entry(kvm, &vm_list, vm_list) {</yellow>
		int idx;
<yellow>		LIST_HEAD(invalid_list);</yellow>

		/*
		 * Never scan more than sc-&gt;nr_to_scan VM instances.
		 * Will not hit this condition practically since we do not try
		 * to shrink more than one VM and it is very unlikely to see
		 * !n_used_mmu_pages so many times.
		 */
		if (!nr_to_scan--)
			break;
		/*
		 * n_used_mmu_pages is accessed without holding kvm-&gt;mmu_lock
		 * here. We may skip a VM instance errorneosly, but we do not
		 * want to shrink a VM that only started to populate its MMU
		 * anyway.
		 */
<yellow>		if (!kvm->arch.n_used_mmu_pages &&</yellow>
<yellow>		    !kvm_has_zapped_obsolete_pages(kvm))</yellow>
			continue;

<yellow>		idx = srcu_read_lock(&kvm->srcu);</yellow>
		write_lock(&amp;kvm-&gt;mmu_lock);

<yellow>		if (kvm_has_zapped_obsolete_pages(kvm)) {</yellow>
<yellow>			kvm_mmu_commit_zap_page(kvm,</yellow>
			      &amp;kvm-&gt;arch.zapped_obsolete_pages);
			goto unlock;
		}

<yellow>		freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);</yellow>

unlock:
		write_unlock(&amp;kvm-&gt;mmu_lock);
<yellow>		srcu_read_unlock(&kvm->srcu, idx);</yellow>

		/*
		 * unfair on small ones
		 * per-vm shrinkers cry out
		 * sadness comes quickly
		 */
		list_move_tail(&amp;kvm-&gt;vm_list, &amp;vm_list);
		break;
	}

<yellow>	mutex_unlock(&kvm_lock);</yellow>
	return freed;
}

static unsigned long
mmu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
{
<blue>	return percpu_counter_read_positive(&kvm_total_used_mmu_pages);</blue>
}

static struct shrinker mmu_shrinker = {
	.count_objects = mmu_shrink_count,
	.scan_objects = mmu_shrink_scan,
	.seeks = DEFAULT_SEEKS * 10,
};

static void mmu_destroy_caches(void)
{
<yellow>	kmem_cache_destroy(pte_list_desc_cache);</yellow>
	kmem_cache_destroy(mmu_page_header_cache);
}

static bool get_nx_auto_mode(void)
{
	/* Return true when CPU has the bug, and mitigations are ON */
<yellow>	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();</yellow>
}

static void __set_nx_huge_pages(bool val)
{
<yellow>	nx_huge_pages = itlb_multihit_kvm_mitigation = val;</yellow>
}

static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
<yellow>{</yellow>
<yellow>	bool old_val = nx_huge_pages;</yellow>
	bool new_val;

	/* In &quot;auto&quot; mode deploy workaround only if CPU has the bug. */
	if (sysfs_streq(val, &quot;off&quot;))
<yellow>		new_val = 0;</yellow>
<yellow>	else if (sysfs_streq(val, "force"))</yellow>
<yellow>		new_val = 1;</yellow>
<yellow>	else if (sysfs_streq(val, "auto"))</yellow>
<yellow>		new_val = get_nx_auto_mode();</yellow>
<yellow>	else if (strtobool(val, &new_val) < 0)</yellow>
		return -EINVAL;

<yellow>	__set_nx_huge_pages(new_val);</yellow>

<yellow>	if (new_val != old_val) {</yellow>
		struct kvm *kvm;

<yellow>		mutex_lock(&kvm_lock);</yellow>

		list_for_each_entry(kvm, &amp;vm_list, vm_list) {
<yellow>			mutex_lock(&kvm->slots_lock);</yellow>
			kvm_mmu_zap_all_fast(kvm);
			mutex_unlock(&amp;kvm-&gt;slots_lock);

			wake_up_process(kvm-&gt;arch.nx_lpage_recovery_thread);
		}
<yellow>		mutex_unlock(&kvm_lock);</yellow>
	}

	return 0;
}

/*
 * nx_huge_pages needs to be resolved to true/false when kvm.ko is loaded, as
 * its default value of -1 is technically undefined behavior for a boolean.
 * Forward the module init call to SPTE code so that it too can handle module
 * params that need to be resolved/snapshot.
 */
void __init kvm_mmu_x86_module_init(void)
{
	if (nx_huge_pages == -1)
		__set_nx_huge_pages(get_nx_auto_mode());

	kvm_mmu_spte_module_init();
}

/*
 * The bulk of the MMU initialization is deferred until the vendor module is
 * loaded as many of the masks/values may be modified by VMX or SVM, i.e. need
 * to be reset when a potentially different vendor module is loaded.
 */
int kvm_mmu_vendor_module_init(void)
{
	int ret = -ENOMEM;

	/*
	 * MMU roles use union aliasing which is, generally speaking, an
	 * undefined behavior. However, we supposedly know how compilers behave
	 * and the current status quo is unlikely to change. Guardians below are
	 * supposed to let us know if the assumption becomes false.
	 */
	BUILD_BUG_ON(sizeof(union kvm_mmu_page_role) != sizeof(u32));
	BUILD_BUG_ON(sizeof(union kvm_mmu_extended_role) != sizeof(u32));
	BUILD_BUG_ON(sizeof(union kvm_cpu_role) != sizeof(u64));

<yellow>	kvm_mmu_reset_all_pte_masks();</yellow>

	pte_list_desc_cache = kmem_cache_create(&quot;pte_list_desc&quot;,
					    sizeof(struct pte_list_desc),
					    0, SLAB_ACCOUNT, NULL);
	if (!pte_list_desc_cache)
		goto out;

<yellow>	mmu_page_header_cache = kmem_cache_create("kvm_mmu_page_header",</yellow>
						  sizeof(struct kvm_mmu_page),
						  0, SLAB_ACCOUNT, NULL);
	if (!mmu_page_header_cache)
		goto out;

<yellow>	if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))</yellow>
		goto out;

<yellow>	ret = register_shrinker(&mmu_shrinker, "x86-mmu");</yellow>
	if (ret)
		goto out_shrinker;

	return 0;

out_shrinker:
<yellow>	percpu_counter_destroy(&kvm_total_used_mmu_pages);</yellow>
out:
<yellow>	mmu_destroy_caches();</yellow>
	return ret;
<yellow>}</yellow>

void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
{
<yellow>	kvm_mmu_unload(vcpu);</yellow>
	free_mmu_pages(&amp;vcpu-&gt;arch.root_mmu);
	free_mmu_pages(&amp;vcpu-&gt;arch.guest_mmu);
	mmu_free_memory_caches(vcpu);
}

void kvm_mmu_vendor_module_exit(void)
{
<yellow>	mmu_destroy_caches();</yellow>
	percpu_counter_destroy(&amp;kvm_total_used_mmu_pages);
	unregister_shrinker(&amp;mmu_shrinker);
}

/*
 * Calculate the effective recovery period, accounting for &#x27;0&#x27; meaning &quot;let KVM
 * select a halving time of 1 hour&quot;.  Returns true if recovery is enabled.
 */
static bool calc_nx_huge_pages_recovery_period(uint *period)
{
	/*
	 * Use READ_ONCE to get the params, this may be called outside of the
	 * param setters, e.g. by the kthread to compute its next timeout.
	 */
<yellow>	bool enabled = READ_ONCE(nx_huge_pages);</yellow>
	uint ratio = READ_ONCE(nx_huge_pages_recovery_ratio);

<yellow>	if (!enabled || !ratio)</yellow>
		return false;

<yellow>	*period = READ_ONCE(nx_huge_pages_recovery_period_ms);</yellow>
	if (!*period) {
		/* Make sure the period is not less than one second.  */
<yellow>		ratio = min(ratio, 3600u);</yellow>
		*period = 60 * 60 * 1000 / ratio;
	}
	return true;
}

<yellow>static int set_nx_huge_pages_recovery_param(const char *val, const struct kernel_param *kp)</yellow>
{
	bool was_recovery_enabled, is_recovery_enabled;
	uint old_period, new_period;
	int err;

<yellow>	was_recovery_enabled = calc_nx_huge_pages_recovery_period(&old_period);</yellow>

<yellow>	err = param_set_uint(val, kp);</yellow>
	if (err)
		return err;

<yellow>	is_recovery_enabled = calc_nx_huge_pages_recovery_period(&new_period);</yellow>

<yellow>	if (is_recovery_enabled &&</yellow>
<yellow>	    (!was_recovery_enabled || old_period > new_period)) {</yellow>
		struct kvm *kvm;

<yellow>		mutex_lock(&kvm_lock);</yellow>

		list_for_each_entry(kvm, &amp;vm_list, vm_list)
<yellow>			wake_up_process(kvm->arch.nx_lpage_recovery_thread);</yellow>

<yellow>		mutex_unlock(&kvm_lock);</yellow>
	}

	return err;
<yellow>}</yellow>

static void kvm_recover_nx_lpages(struct kvm *kvm)
{
<yellow>	unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;</yellow>
	int rcu_idx;
	struct kvm_mmu_page *sp;
	unsigned int ratio;
	LIST_HEAD(invalid_list);
	bool flush = false;
	ulong to_zap;

	rcu_idx = srcu_read_lock(&amp;kvm-&gt;srcu);
	write_lock(&amp;kvm-&gt;mmu_lock);

	/*
	 * Zapping TDP MMU shadow pages, including the remote TLB flush, must
	 * be done under RCU protection, because the pages are freed via RCU
	 * callback.
	 */
	rcu_read_lock();

	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
<yellow>	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;</yellow>
<yellow>	for ( ; to_zap; --to_zap) {</yellow>
<yellow>		if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))</yellow>
			break;

		/*
		 * We use a separate list instead of just using active_mmu_pages
		 * because the number of lpage_disallowed pages is expected to
		 * be relatively small compared to the total.
		 */
<yellow>		sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,</yellow>
				      struct kvm_mmu_page,
				      lpage_disallowed_link);
<yellow>		WARN_ON_ONCE(!sp->lpage_disallowed);</yellow>
<yellow>		if (is_tdp_mmu_page(sp)) {</yellow>
<yellow>			flush |= kvm_tdp_mmu_zap_sp(kvm, sp);</yellow>
		} else {
<yellow>			kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);</yellow>
<yellow>			WARN_ON_ONCE(sp->lpage_disallowed);</yellow>
		}

<yellow>		if (need_resched() || rwlock_needbreak(&kvm->mmu_lock)) {</yellow>
<yellow>			kvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);</yellow>
<yellow>			rcu_read_unlock();</yellow>

			cond_resched_rwlock_write(&amp;kvm-&gt;mmu_lock);
			flush = false;

			rcu_read_lock();
		}
	}
<yellow>	kvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);</yellow>

<yellow>	rcu_read_unlock();</yellow>

	write_unlock(&amp;kvm-&gt;mmu_lock);
<yellow>	srcu_read_unlock(&kvm->srcu, rcu_idx);</yellow>
}

static long get_nx_lpage_recovery_timeout(u64 start_time)
{
	bool enabled;
	uint period;

<yellow>	enabled = calc_nx_huge_pages_recovery_period(&period);</yellow>

<yellow>	return enabled ? start_time + msecs_to_jiffies(period) - get_jiffies_64()</yellow>
		       : MAX_SCHEDULE_TIMEOUT;
}

static int kvm_nx_lpage_recovery_worker(struct kvm *kvm, uintptr_t data)
<yellow>{</yellow>
	u64 start_time;
	long remaining_time;

	while (true) {
<yellow>		start_time = get_jiffies_64();</yellow>
<yellow>		remaining_time = get_nx_lpage_recovery_timeout(start_time);</yellow>

<yellow>		set_current_state(TASK_INTERRUPTIBLE);</yellow>
<yellow>		while (!kthread_should_stop() && remaining_time > 0) {</yellow>
<yellow>			schedule_timeout(remaining_time);</yellow>
<yellow>			remaining_time = get_nx_lpage_recovery_timeout(start_time);</yellow>
<yellow>			set_current_state(TASK_INTERRUPTIBLE);</yellow>
		}

<yellow>		set_current_state(TASK_RUNNING);</yellow>

		if (kthread_should_stop())
			return 0;

<yellow>		kvm_recover_nx_lpages(kvm);</yellow>
	}
}

int kvm_mmu_post_init_vm(struct kvm *kvm)
{
	int err;

<blue>	err = kvm_vm_create_worker_thread(kvm, kvm_nx_lpage_recovery_worker, 0,</blue>
					  &quot;kvm-nx-lpage-recovery&quot;,
					  &amp;kvm-&gt;arch.nx_lpage_recovery_thread);
	if (!err)
<blue>		kthread_unpark(kvm->arch.nx_lpage_recovery_thread);</blue>

	return err;
<blue>}</blue>

void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
{
<yellow>	if (kvm->arch.nx_lpage_recovery_thread)</yellow>
<yellow>		kthread_stop(kvm->arch.nx_lpage_recovery_thread);</yellow>
<yellow>}</yellow>


</code></pre></td></tr></table>
</body>
</html>
