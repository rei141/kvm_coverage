<doctype html>
<html lang="ja">
<head><title>bitops.h</title><meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
      .split {
         height: 100%;
         position: fixed;
         z-index: 1;
         top: 0;
         overflow-x: hidden;
      }

      .tree {
         left: 0;
         width: 20%;
      }

      .right {
         border-left: 2px solid #444;
         right: 0;
         width: 80%;
         /* font-family: 'Courier New', Courier, monospace;
				color: rgb(80, 80, 80); */
      }
</style>

</head>
<body>
   <div class="split tree">
      <ul id="file_list">
      </ul>
   </div>
   <div class="split right">
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line><script>for (let i = 1; i <= 434; i++){
         document.write(i+".\n");
   }
         </script></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">/* SPDX-License-Identifier: GPL-2.0 */
#ifndef _ASM_X86_BITOPS_H
#define _ASM_X86_BITOPS_H

/*
 * Copyright 1992, Linus Torvalds.
 *
 * Note: inlines with more than a single statement should be marked
 * __always_inline to avoid problems with older gcc&#x27;s inlining heuristics.
 */

#ifndef _LINUX_BITOPS_H
#error only &lt;linux/bitops.h&gt; can be included directly
#endif

#include &lt;linux/compiler.h&gt;
#include &lt;asm/alternative.h&gt;
#include &lt;asm/rmwcc.h&gt;
#include &lt;asm/barrier.h&gt;

#if BITS_PER_LONG == 32
# define _BITOPS_LONG_SHIFT 5
#elif BITS_PER_LONG == 64
# define _BITOPS_LONG_SHIFT 6
#else
# error &quot;Unexpected BITS_PER_LONG&quot;
#endif

#define BIT_64(n)			(U64_C(1) &lt;&lt; (n))

/*
 * These have to be done with inline assembly: that way the bit-setting
 * is guaranteed to be atomic. All bit operations return 0 if the bit
 * was cleared before the operation and != 0 if it was not.
 *
 * bit 0 is the LSB of addr; bit 32 is the LSB of (addr+1).
 */

#define RLONG_ADDR(x)			 &quot;m&quot; (*(volatile long *) (x))
#define WBYTE_ADDR(x)			&quot;+m&quot; (*(volatile char *) (x))

#define ADDR				RLONG_ADDR(addr)

/*
 * We do the locked ops that don&#x27;t return the old value as
 * a mask operation on a byte.
 */
#define CONST_MASK_ADDR(nr, addr)	WBYTE_ADDR((void *)(addr) + ((nr)&gt;&gt;3))
#define CONST_MASK(nr)			(1 &lt;&lt; ((nr) &amp; 7))

static __always_inline void
arch_set_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
<yellow>		asm volatile(LOCK_PREFIX "orb %b1,%0"</yellow>
			: CONST_MASK_ADDR(nr, addr)
			: &quot;iq&quot; (CONST_MASK(nr))
			: &quot;memory&quot;);
	} else {
		asm volatile(LOCK_PREFIX __ASM_SIZE(bts) &quot; %1,%0&quot;
			: : RLONG_ADDR(addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);
	}
}

static __always_inline void
arch___set_bit(unsigned long nr, volatile unsigned long *addr)
{
<yellow>	asm volatile(__ASM_SIZE(bts) " %1,%0" : : ADDR, "Ir" (nr) : "memory");</yellow>
}

static __always_inline void
arch_clear_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
		asm volatile(LOCK_PREFIX &quot;andb %b1,%0&quot;
			: CONST_MASK_ADDR(nr, addr)
			: &quot;iq&quot; (~CONST_MASK(nr)));
	} else {
		asm volatile(LOCK_PREFIX __ASM_SIZE(btr) &quot; %1,%0&quot;
			: : RLONG_ADDR(addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);
	}
}

static __always_inline void
arch_clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	barrier();
	arch_clear_bit(nr, addr);
}

static __always_inline void
arch___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(__ASM_SIZE(btr) &quot; %1,%0&quot; : : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
}

static __always_inline bool
arch_clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
{
	bool negative;
	asm volatile(LOCK_PREFIX &quot;andb %2,%1&quot;
		CC_SET(s)
		: CC_OUT(s) (negative), WBYTE_ADDR(addr)
		: &quot;ir&quot; ((char) ~(1 &lt;&lt; nr)) : &quot;memory&quot;);
	return negative;
}
#define arch_clear_bit_unlock_is_negative_byte                                 \
	arch_clear_bit_unlock_is_negative_byte

static __always_inline void
arch___clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	arch___clear_bit(nr, addr);
}

static __always_inline void
arch___change_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(__ASM_SIZE(btc) &quot; %1,%0&quot; : : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
}

static __always_inline void
arch_change_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
		asm volatile(LOCK_PREFIX &quot;xorb %b1,%0&quot;
			: CONST_MASK_ADDR(nr, addr)
			: &quot;iq&quot; (CONST_MASK(nr)));
	} else {
		asm volatile(LOCK_PREFIX __ASM_SIZE(btc) &quot; %1,%0&quot;
			: : RLONG_ADDR(addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);
	}
}

static __always_inline bool
arch_test_and_set_bit(long nr, volatile unsigned long *addr)
{
	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(bts), *addr, c, &quot;Ir&quot;, nr);
}

static __always_inline bool
arch_test_and_set_bit_lock(long nr, volatile unsigned long *addr)
{
	return arch_test_and_set_bit(nr, addr);
}

static __always_inline bool
arch___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm(__ASM_SIZE(bts) &quot; %2,%1&quot;
	    CC_SET(c)
	    : CC_OUT(c) (oldbit)
	    : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
	return oldbit;
}

static __always_inline bool
arch_test_and_clear_bit(long nr, volatile unsigned long *addr)
{
	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btr), *addr, c, &quot;Ir&quot;, nr);
}

/*
 * Note: the operation is performed atomically with respect to
 * the local CPU, but not other CPUs. Portable code should not
 * rely on this behaviour.
 * KVM relies on this behaviour on x86 for modifying memory that is also
 * accessed from a hypervisor on the same CPU if running in a VM: don&#x27;t change
 * this without also updating arch/x86/kernel/kvm.c
 */
static __always_inline bool
arch___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(__ASM_SIZE(btr) &quot; %2,%1&quot;
		     CC_SET(c)
		     : CC_OUT(c) (oldbit)
		     : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
	return oldbit;
}

static __always_inline bool
arch___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(__ASM_SIZE(btc) &quot; %2,%1&quot;
		     CC_SET(c)
		     : CC_OUT(c) (oldbit)
		     : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);

	return oldbit;
}

static __always_inline bool
arch_test_and_change_bit(long nr, volatile unsigned long *addr)
{
	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc), *addr, c, &quot;Ir&quot;, nr);
}

static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL &lt;&lt; (nr &amp; (BITS_PER_LONG-1))) &amp;
<blue>		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;</blue>
}

static __always_inline bool constant_test_bit_acquire(long nr, const volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(&quot;testb %2,%1&quot;
		     CC_SET(nz)
		     : CC_OUT(nz) (oldbit)
		     : &quot;m&quot; (((unsigned char *)addr)[nr &gt;&gt; 3]),
		       &quot;i&quot; (1 &lt;&lt; (nr &amp; 7))
		     :&quot;memory&quot;);

	return oldbit;
}

static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
{
	bool oldbit;

<yellow>	asm volatile(__ASM_SIZE(bt) " %2,%1"</yellow>
		     CC_SET(c)
		     : CC_OUT(c) (oldbit)
		     : &quot;m&quot; (*(unsigned long *)addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);

	return oldbit;
}

static __always_inline bool
arch_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
<blue>	return __builtin_constant_p(nr) ? constant_test_bit(nr, addr) :</blue>
<yellow>					  variable_test_bit(nr, addr);</yellow>
}

static __always_inline bool
arch_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
	return __builtin_constant_p(nr) ? constant_test_bit_acquire(nr, addr) :
					  variable_test_bit(nr, addr);
}

static __always_inline unsigned long variable__ffs(unsigned long word)
{
<yellow>	asm("rep; bsf %1,%0"</yellow>
		: &quot;=r&quot; (word)
		: &quot;rm&quot; (word));
	return word;
}

/**
 * __ffs - find first set bit in word
 * @word: The word to search
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
#define __ffs(word)				\
	(__builtin_constant_p(word) ?		\
	 (unsigned long)__builtin_ctzl(word) :	\
	 variable__ffs(word))

static __always_inline unsigned long variable_ffz(unsigned long word)
{
	asm(&quot;rep; bsf %1,%0&quot;
		: &quot;=r&quot; (word)
<yellow>		: "r" (~word));</yellow>
	return word;
}

/**
 * ffz - find first zero bit in word
 * @word: The word to search
 *
 * Undefined if no zero exists, so code should check against ~0UL first.
 */
#define ffz(word)				\
	(__builtin_constant_p(word) ?		\
	 (unsigned long)__builtin_ctzl(~word) :	\
	 variable_ffz(word))

/*
 * __fls: find last set bit in word
 * @word: The word to search
 *
 * Undefined if no set bit exists, so code should check against 0 first.
 */
static __always_inline unsigned long __fls(unsigned long word)
{
	asm(&quot;bsr %1,%0&quot;
	    : &quot;=r&quot; (word)
	    : &quot;rm&quot; (word));
	return word;
}

#undef ADDR

#ifdef __KERNEL__
static __always_inline int variable_ffs(int x)
{
	int r;

#ifdef CONFIG_X86_64
	/*
	 * AMD64 says BSFL won&#x27;t clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
<yellow>	asm("bsfl %1,%0"</yellow>
	    : &quot;=r&quot; (r)
	    : &quot;rm&quot; (x), &quot;0&quot; (-1));
#elif defined(CONFIG_X86_CMOV)
	asm(&quot;bsfl %1,%0\n\t&quot;
	    &quot;cmovzl %2,%0&quot;
	    : &quot;=&amp;r&quot; (r) : &quot;rm&quot; (x), &quot;r&quot; (-1));
#else
	asm(&quot;bsfl %1,%0\n\t&quot;
	    &quot;jnz 1f\n\t&quot;
	    &quot;movl $-1,%0\n&quot;
	    &quot;1:&quot; : &quot;=r&quot; (r) : &quot;rm&quot; (x));
#endif
	return r + 1;
}

/**
 * ffs - find first set bit in word
 * @x: the word to search
 *
 * This is defined the same way as the libc and compiler builtin ffs
 * routines, therefore differs in spirit from the other bitops.
 *
 * ffs(value) returns 0 if value is 0 or the position of the first
 * set bit if value is nonzero. The first (least significant) bit
 * is at position 1.
 */
#define ffs(x) (__builtin_constant_p(x) ? __builtin_ffs(x) : variable_ffs(x))

/**
 * fls - find last set bit in word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffs, but returns the position of the most significant set bit.
 *
 * fls(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 32.
 */
static __always_inline int fls(unsigned int x)
{
	int r;

#ifdef CONFIG_X86_64
	/*
	 * AMD64 says BSRL won&#x27;t clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
	asm(&quot;bsrl %1,%0&quot;
	    : &quot;=r&quot; (r)
	    : &quot;rm&quot; (x), &quot;0&quot; (-1));
#elif defined(CONFIG_X86_CMOV)
	asm(&quot;bsrl %1,%0\n\t&quot;
	    &quot;cmovzl %2,%0&quot;
	    : &quot;=&amp;r&quot; (r) : &quot;rm&quot; (x), &quot;rm&quot; (-1));
#else
	asm(&quot;bsrl %1,%0\n\t&quot;
	    &quot;jnz 1f\n\t&quot;
	    &quot;movl $-1,%0\n&quot;
	    &quot;1:&quot; : &quot;=r&quot; (r) : &quot;rm&quot; (x));
#endif
	return r + 1;
}

/**
 * fls64 - find last set bit in a 64-bit word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffsll, but returns the position of the most significant set bit.
 *
 * fls64(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 64.
 */
#ifdef CONFIG_X86_64
static __always_inline int fls64(__u64 x)
{
	int bitpos = -1;
	/*
	 * AMD64 says BSRQ won&#x27;t clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before.
	 */
<yellow>	asm("bsrq %1,%q0"</yellow>
	    : &quot;+r&quot; (bitpos)
	    : &quot;rm&quot; (x));
	return bitpos + 1;
}
#else
#include &lt;asm-generic/bitops/fls64.h&gt;
#endif

#include &lt;asm-generic/bitops/sched.h&gt;

#include &lt;asm/arch_hweight.h&gt;

#include &lt;asm-generic/bitops/const_hweight.h&gt;

#include &lt;asm-generic/bitops/instrumented-atomic.h&gt;
#include &lt;asm-generic/bitops/instrumented-non-atomic.h&gt;
#include &lt;asm-generic/bitops/instrumented-lock.h&gt;

#include &lt;asm-generic/bitops/le.h&gt;

#include &lt;asm-generic/bitops/ext2-atomic-setbit.h&gt;

#endif /* __KERNEL__ */
#endif /* _ASM_X86_BITOPS_H */


</code></pre></td></tr></table>
</div><script>const fileList = document.getElementById('file_list')
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/mmu/mmu.c.html">mmu.c 45.7%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/nested.c.html">nested.c 75.0%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/vmx.c.html">vmx.c 50.3%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/x86.c.html">x86.c 47.6%</li>`
</script></body></html>