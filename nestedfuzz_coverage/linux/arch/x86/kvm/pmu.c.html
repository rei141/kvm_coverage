<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.<br>2.<br>3.<br>4.<br>5.<br>6.<br>7.<br>8.<br>9.<br>10.<br>11.<br>12.<br>13.<br>14.<br>15.<br>16.<br>17.<br>18.<br>19.<br>20.<br>21.<br>22.<br>23.<br>24.<br>25.<br>26.<br>27.<br>28.<br>29.<br>30.<br>31.<br>32.<br>33.<br>34.<br>35.<br>36.<br>37.<br>38.<br>39.<br>40.<br>41.<br>42.<br>43.<br>44.<br>45.<br>46.<br>47.<br>48.<br>49.<br>50.<br>51.<br>52.<br>53.<br>54.<br>55.<br>56.<br>57.<br>58.<br>59.<br>60.<br>61.<br>62.<br>63.<br>64.<br>65.<br>66.<br>67.<br>68.<br>69.<br>70.<br>71.<br>72.<br>73.<br>74.<br>75.<br>76.<br>77.<br>78.<br>79.<br>80.<br>81.<br>82.<br>83.<br>84.<br>85.<br>86.<br>87.<br>88.<br>89.<br>90.<br>91.<br>92.<br>93.<br>94.<br>95.<br>96.<br>97.<br>98.<br>99.<br>100.<br>101.<br>102.<br>103.<br>104.<br>105.<br>106.<br>107.<br>108.<br>109.<br>110.<br>111.<br>112.<br>113.<br>114.<br>115.<br>116.<br>117.<br>118.<br>119.<br>120.<br>121.<br>122.<br>123.<br>124.<br>125.<br>126.<br>127.<br>128.<br>129.<br>130.<br>131.<br>132.<br>133.<br>134.<br>135.<br>136.<br>137.<br>138.<br>139.<br>140.<br>141.<br>142.<br>143.<br>144.<br>145.<br>146.<br>147.<br>148.<br>149.<br>150.<br>151.<br>152.<br>153.<br>154.<br>155.<br>156.<br>157.<br>158.<br>159.<br>160.<br>161.<br>162.<br>163.<br>164.<br>165.<br>166.<br>167.<br>168.<br>169.<br>170.<br>171.<br>172.<br>173.<br>174.<br>175.<br>176.<br>177.<br>178.<br>179.<br>180.<br>181.<br>182.<br>183.<br>184.<br>185.<br>186.<br>187.<br>188.<br>189.<br>190.<br>191.<br>192.<br>193.<br>194.<br>195.<br>196.<br>197.<br>198.<br>199.<br>200.<br>201.<br>202.<br>203.<br>204.<br>205.<br>206.<br>207.<br>208.<br>209.<br>210.<br>211.<br>212.<br>213.<br>214.<br>215.<br>216.<br>217.<br>218.<br>219.<br>220.<br>221.<br>222.<br>223.<br>224.<br>225.<br>226.<br>227.<br>228.<br>229.<br>230.<br>231.<br>232.<br>233.<br>234.<br>235.<br>236.<br>237.<br>238.<br>239.<br>240.<br>241.<br>242.<br>243.<br>244.<br>245.<br>246.<br>247.<br>248.<br>249.<br>250.<br>251.<br>252.<br>253.<br>254.<br>255.<br>256.<br>257.<br>258.<br>259.<br>260.<br>261.<br>262.<br>263.<br>264.<br>265.<br>266.<br>267.<br>268.<br>269.<br>270.<br>271.<br>272.<br>273.<br>274.<br>275.<br>276.<br>277.<br>278.<br>279.<br>280.<br>281.<br>282.<br>283.<br>284.<br>285.<br>286.<br>287.<br>288.<br>289.<br>290.<br>291.<br>292.<br>293.<br>294.<br>295.<br>296.<br>297.<br>298.<br>299.<br>300.<br>301.<br>302.<br>303.<br>304.<br>305.<br>306.<br>307.<br>308.<br>309.<br>310.<br>311.<br>312.<br>313.<br>314.<br>315.<br>316.<br>317.<br>318.<br>319.<br>320.<br>321.<br>322.<br>323.<br>324.<br>325.<br>326.<br>327.<br>328.<br>329.<br>330.<br>331.<br>332.<br>333.<br>334.<br>335.<br>336.<br>337.<br>338.<br>339.<br>340.<br>341.<br>342.<br>343.<br>344.<br>345.<br>346.<br>347.<br>348.<br>349.<br>350.<br>351.<br>352.<br>353.<br>354.<br>355.<br>356.<br>357.<br>358.<br>359.<br>360.<br>361.<br>362.<br>363.<br>364.<br>365.<br>366.<br>367.<br>368.<br>369.<br>370.<br>371.<br>372.<br>373.<br>374.<br>375.<br>376.<br>377.<br>378.<br>379.<br>380.<br>381.<br>382.<br>383.<br>384.<br>385.<br>386.<br>387.<br>388.<br>389.<br>390.<br>391.<br>392.<br>393.<br>394.<br>395.<br>396.<br>397.<br>398.<br>399.<br>400.<br>401.<br>402.<br>403.<br>404.<br>405.<br>406.<br>407.<br>408.<br>409.<br>410.<br>411.<br>412.<br>413.<br>414.<br>415.<br>416.<br>417.<br>418.<br>419.<br>420.<br>421.<br>422.<br>423.<br>424.<br>425.<br>426.<br>427.<br>428.<br>429.<br>430.<br>431.<br>432.<br>433.<br>434.<br>435.<br>436.<br>437.<br>438.<br>439.<br>440.<br>441.<br>442.<br>443.<br>444.<br>445.<br>446.<br>447.<br>448.<br>449.<br>450.<br>451.<br>452.<br>453.<br>454.<br>455.<br>456.<br>457.<br>458.<br>459.<br>460.<br>461.<br>462.<br>463.<br>464.<br>465.<br>466.<br>467.<br>468.<br>469.<br>470.<br>471.<br>472.<br>473.<br>474.<br>475.<br>476.<br>477.<br>478.<br>479.<br>480.<br>481.<br>482.<br>483.<br>484.<br>485.<br>486.<br>487.<br>488.<br>489.<br>490.<br>491.<br>492.<br>493.<br>494.<br>495.<br>496.<br>497.<br>498.<br>499.<br>500.<br>501.<br>502.<br>503.<br>504.<br>505.<br>506.<br>507.<br>508.<br>509.<br>510.<br>511.<br>512.<br>513.<br>514.<br>515.<br>516.<br>517.<br>518.<br>519.<br>520.<br>521.<br>522.<br>523.<br>524.<br>525.<br>526.<br>527.<br>528.<br>529.<br>530.<br>531.<br>532.<br>533.<br>534.<br>535.<br>536.<br>537.<br>538.<br>539.<br>540.<br>541.<br>542.<br>543.<br>544.<br>545.<br>546.<br>547.<br>548.<br>549.<br>550.<br>551.<br>552.<br>553.<br>554.<br>555.<br>556.<br>557.<br>558.<br>559.<br>560.<br>561.<br>562.<br>563.<br>564.<br>565.<br>566.<br>567.<br>568.<br>569.<br>570.<br>571.<br>572.<br>573.<br>574.<br>575.<br>576.<br>577.<br>578.<br>579.<br>580.<br>581.<br>582.<br>583.<br>584.<br>585.<br>586.<br>587.<br>588.<br>589.<br>590.<br>591.<br>592.<br>593.<br>594.<br>595.<br>596.<br>597.<br>598.<br>599.<br>600.<br>601.<br>602.<br>603.<br>604.<br>605.<br>606.<br>607.<br>608.<br>609.<br>610.<br>611.<br>612.<br>613.<br>614.<br>615.<br>616.<br>617.<br>618.<br>619.<br>620.<br>621.<br>622.<br>623.<br>624.<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * Kernel-based Virtual Machine -- Performance Monitoring Unit support
 *
 * Copyright 2015 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@redhat.com&gt;
 *   Gleb Natapov &lt;gleb@redhat.com&gt;
 *   Wei Huang    &lt;wei@redhat.com&gt;
 */

#include &lt;linux/types.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/perf_event.h&gt;
#include &lt;linux/bsearch.h&gt;
#include &lt;linux/sort.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &lt;asm/cpu_device_id.h&gt;
#include &quot;x86.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;lapic.h&quot;
#include &quot;pmu.h&quot;

/* This is enough to filter the vast majority of currently defined events. */
#define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300

struct x86_pmu_capability __read_mostly kvm_pmu_cap;
EXPORT_SYMBOL_GPL(kvm_pmu_cap);

static const struct x86_cpu_id vmx_icl_pebs_cpu[] = {
	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D, NULL),
	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X, NULL),
	{}
};

/* NOTE:
 * - Each perf counter is defined as &quot;struct kvm_pmc&quot;;
 * - There are two types of perf counters: general purpose (gp) and fixed.
 *   gp counters are stored in gp_counters[] and fixed counters are stored
 *   in fixed_counters[] respectively. Both of them are part of &quot;struct
 *   kvm_pmu&quot;;
 * - pmu.c understands the difference between gp counters and fixed counters.
 *   However AMD doesn&#x27;t support fixed-counters;
 * - There are three types of index to access perf counters (PMC):
 *     1. MSR (named msr): For example Intel has MSR_IA32_PERFCTRn and AMD
 *        has MSR_K7_PERFCTRn and, for families 15H and later,
 *        MSR_F15H_PERF_CTRn, where MSR_F15H_PERF_CTR[0-3] are
 *        aliased to MSR_K7_PERFCTRn.
 *     2. MSR Index (named idx): This normally is used by RDPMC instruction.
 *        For instance AMD RDPMC instruction uses 0000_0003h in ECX to access
 *        C001_0007h (MSR_K7_PERCTR3). Intel has a similar mechanism, except
 *        that it also supports fixed counters. idx can be used to as index to
 *        gp and fixed counters.
 *     3. Global PMC Index (named pmc): pmc is an index specific to PMU
 *        code. Each pmc, stored in kvm_pmc.idx field, is unique across
 *        all perf counters (both gp and fixed). The mapping relationship
 *        between pmc and perf counters is as the following:
 *        * Intel: [0 .. KVM_INTEL_PMC_MAX_GENERIC-1] &lt;=&gt; gp counters
 *                 [INTEL_PMC_IDX_FIXED .. INTEL_PMC_IDX_FIXED + 2] &lt;=&gt; fixed
 *        * AMD:   [0 .. AMD64_NUM_COUNTERS-1] and, for families 15H
 *          and later, [0 .. AMD64_NUM_COUNTERS_CORE-1] &lt;=&gt; gp counters
 */

static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;

#define KVM_X86_PMU_OP(func)					     \
	DEFINE_STATIC_CALL_NULL(kvm_x86_pmu_##func,			     \
				*(((struct kvm_pmu_ops *)0)-&gt;func));
#define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
#include &lt;asm/kvm-x86-pmu-ops.h&gt;

void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
{
<yellow>	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));</yellow>

#define __KVM_X86_PMU_OP(func) \
	static_call_update(kvm_x86_pmu_##func, kvm_pmu_ops.func);
#define KVM_X86_PMU_OP(func) \
	WARN_ON(!kvm_pmu_ops.func); __KVM_X86_PMU_OP(func)
#define KVM_X86_PMU_OP_OPTIONAL __KVM_X86_PMU_OP
#include &lt;asm/kvm-x86-pmu-ops.h&gt;
#undef __KVM_X86_PMU_OP
}

static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
{
<blue>	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);</blue>
}

static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
{
	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
<yellow>	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);</yellow>

<yellow>	kvm_pmu_deliver_pmi(vcpu);</yellow>
<yellow>}</yellow>

static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	bool skip_pmi = false;

	/* Ignore counters that have been reprogrammed already. */
	if (test_and_set_bit(pmc-&gt;idx, pmu-&gt;reprogram_pmi))
		return;

<yellow>	if (pmc->perf_event && pmc->perf_event->attr.precise_ip) {</yellow>
<yellow>		if (!in_pmi) {</yellow>
			/*
			 * TODO: KVM is currently _choosing_ to not generate records
			 * for emulated instructions, avoiding BUFFER_OVF PMI when
			 * there are no records. Strictly speaking, it should be done
			 * as well in the right context to improve sampling accuracy.
			 */
			skip_pmi = true;
		} else {
			/* Indicate PEBS overflow PMI to guest. */
<yellow>			skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,</yellow>
						      (unsigned long *)&amp;pmu-&gt;global_status);
		}
	} else {
<yellow>		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);</yellow>
	}
<yellow>	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);</yellow>

<yellow>	if (!pmc->intr || skip_pmi)</yellow>
		return;

	/*
	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
	 * can be ejected on a guest mode re-entry. Otherwise we can&#x27;t
	 * be sure that vcpu wasn&#x27;t executing hlt instruction at the
	 * time of vmexit and is not going to re-enter guest mode until
	 * woken up. So we should wake it, but this is impossible from
	 * NMI context. Do it from irq work instead.
	 */
<yellow>	if (in_pmi && !kvm_handling_nmi_from_guest(pmc->vcpu))</yellow>
<yellow>		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);</yellow>
	else
<yellow>		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);</yellow>
<yellow>}</yellow>

static void kvm_perf_overflow(struct perf_event *perf_event,
			      struct perf_sample_data *data,
			      struct pt_regs *regs)
{
<yellow>	struct kvm_pmc *pmc = perf_event->overflow_handler_context;</yellow>

	__kvm_perf_overflow(pmc, true);
}

<yellow>static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,</yellow>
				  u64 config, bool exclude_user,
				  bool exclude_kernel, bool intr)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	struct perf_event *event;
	struct perf_event_attr attr = {
		.type = type,
		.size = sizeof(attr),
		.pinned = true,
		.exclude_idle = true,
		.exclude_host = 1,
		.exclude_user = exclude_user,
		.exclude_kernel = exclude_kernel,
		.config = config,
	};
	bool pebs = test_bit(pmc-&gt;idx, (unsigned long *)&amp;pmu-&gt;pebs_enable);

<yellow>	attr.sample_period = get_sample_period(pmc, pmc->counter);</yellow>

<yellow>	if ((attr.config & HSW_IN_TX_CHECKPOINTED) &&</yellow>
<yellow>	    guest_cpuid_is_intel(pmc->vcpu)) {</yellow>
		/*
		 * HSW_IN_TX_CHECKPOINTED is not supported with nonzero
		 * period. Just clear the sample period so at least
		 * allocating the counter doesn&#x27;t fail.
		 */
<yellow>		attr.sample_period = 0;</yellow>
	}
<yellow>	if (pebs) {</yellow>
		/*
		 * The non-zero precision level of guest event makes the ordinary
		 * guest event becomes a guest PEBS event and triggers the host
		 * PEBS PMI handler to determine whether the PEBS overflow PMI
		 * comes from the host counters or the guest.
		 *
		 * For most PEBS hardware events, the difference in the software
		 * precision levels of guest and host PEBS events will not affect
		 * the accuracy of the PEBS profiling result, because the &quot;event IP&quot;
		 * in the PEBS record is calibrated on the guest side.
		 *
		 * On Icelake everything is fine. Other hardware (GLC+, TNT+) that
		 * could possibly care here is unsupported and needs changes.
		 */
<yellow>		attr.precise_ip = 1;</yellow>
<yellow>		if (x86_match_cpu(vmx_icl_pebs_cpu) && pmc->idx == 32)</yellow>
<yellow>			attr.precise_ip = 3;</yellow>
	}

<yellow>	event = perf_event_create_kernel_counter(&attr, -1, current,</yellow>
						 kvm_perf_overflow, pmc);
	if (IS_ERR(event)) {
<yellow>		pr_debug_ratelimited("kvm_pmu: event creation failed %ld for pmc->idx = %d\n",</yellow>
			    PTR_ERR(event), pmc-&gt;idx);
		return;
	}

<yellow>	pmc->perf_event = event;</yellow>
	pmc_to_pmu(pmc)-&gt;event_count++;
	clear_bit(pmc-&gt;idx, pmc_to_pmu(pmc)-&gt;reprogram_pmi);
	pmc-&gt;is_paused = false;
	pmc-&gt;intr = intr || pebs;
}

static void pmc_pause_counter(struct kvm_pmc *pmc)
{
<yellow>	u64 counter = pmc->counter;</yellow>

<yellow>	if (!pmc->perf_event || pmc->is_paused)</yellow>
		return;

	/* update counter, reset event value to avoid redundant accumulation */
<yellow>	counter += perf_event_pause(pmc->perf_event, true);</yellow>
	pmc-&gt;counter = counter &amp; pmc_bitmask(pmc);
	pmc-&gt;is_paused = true;
}

static bool pmc_resume_counter(struct kvm_pmc *pmc)
{
<yellow>	if (!pmc->perf_event)</yellow>
		return false;

	/* recalibrate sample period and check if it&#x27;s accepted by perf core */
<yellow>	if (perf_event_period(pmc->perf_event,</yellow>
			      get_sample_period(pmc, pmc-&gt;counter)))
		return false;

<yellow>	if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=</yellow>
	    (!!pmc-&gt;perf_event-&gt;attr.precise_ip))
		return false;

	/* reuse perf_event to serve as pmc_reprogram_counter() does*/
<yellow>	perf_event_enable(pmc->perf_event);</yellow>
	pmc-&gt;is_paused = false;

	clear_bit(pmc-&gt;idx, (unsigned long *)&amp;pmc_to_pmu(pmc)-&gt;reprogram_pmi);
	return true;
}

static int cmp_u64(const void *pa, const void *pb)
{
<yellow>	u64 a = *(u64 *)pa;</yellow>
	u64 b = *(u64 *)pb;

	return (a &gt; b) - (a &lt; b);
}

static bool check_pmu_event_filter(struct kvm_pmc *pmc)
{
	struct kvm_pmu_event_filter *filter;
<yellow>	struct kvm *kvm = pmc->vcpu->kvm;</yellow>
	bool allow_event = true;
	__u64 key;
	int idx;

	if (!static_call(kvm_x86_pmu_hw_event_available)(pmc))
		return false;

<yellow>	filter = srcu_dereference(kvm->arch.pmu_event_filter, &kvm->srcu);</yellow>
	if (!filter)
		goto out;

<yellow>	if (pmc_is_gp(pmc)) {</yellow>
<yellow>		key = pmc->eventsel & AMD64_RAW_EVENT_MASK_NB;</yellow>
		if (bsearch(&amp;key, filter-&gt;events, filter-&gt;nevents,
			    sizeof(__u64), cmp_u64))
<yellow>			allow_event = filter->action == KVM_PMU_EVENT_ALLOW;</yellow>
		else
<yellow>			allow_event = filter->action == KVM_PMU_EVENT_DENY;</yellow>
	} else {
<yellow>		idx = pmc->idx - INTEL_PMC_IDX_FIXED;</yellow>
		if (filter-&gt;action == KVM_PMU_EVENT_DENY &amp;&amp;
<yellow>		    test_bit(idx, (ulong *)&filter->fixed_counter_bitmap))</yellow>
			allow_event = false;
<yellow>		if (filter->action == KVM_PMU_EVENT_ALLOW &&</yellow>
<yellow>		    !test_bit(idx, (ulong *)&filter->fixed_counter_bitmap))</yellow>
			allow_event = false;
	}

out:
	return allow_event;
}

void reprogram_counter(struct kvm_pmc *pmc)
<yellow>{</yellow>
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	u64 eventsel = pmc-&gt;eventsel;
	u64 new_config = eventsel;
	u8 fixed_ctr_ctrl;

<yellow>	pmc_pause_counter(pmc);</yellow>

<yellow>	if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))</yellow>
		return;

<yellow>	if (!check_pmu_event_filter(pmc))</yellow>
		return;

<yellow>	if (eventsel & ARCH_PERFMON_EVENTSEL_PIN_CONTROL)</yellow>
<yellow>		printk_once("kvm pmu: pin control bit is ignored\n");</yellow>

<yellow>	if (pmc_is_fixed(pmc)) {</yellow>
<yellow>		fixed_ctr_ctrl = fixed_ctrl_field(pmu->fixed_ctr_ctrl,</yellow>
						  pmc-&gt;idx - INTEL_PMC_IDX_FIXED);
		if (fixed_ctr_ctrl &amp; 0x1)
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_OS;</yellow>
<yellow>		if (fixed_ctr_ctrl & 0x2)</yellow>
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_USR;</yellow>
<yellow>		if (fixed_ctr_ctrl & 0x8)</yellow>
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_INT;</yellow>
<yellow>		new_config = (u64)fixed_ctr_ctrl;</yellow>
	}

<yellow>	if (pmc->current_config == new_config && pmc_resume_counter(pmc))</yellow>
		return;

<yellow>	pmc_release_perf_event(pmc);</yellow>

<yellow>	pmc->current_config = new_config;</yellow>
	pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
			      (eventsel &amp; pmu-&gt;raw_event_mask),
			      !(eventsel &amp; ARCH_PERFMON_EVENTSEL_USR),
			      !(eventsel &amp; ARCH_PERFMON_EVENTSEL_OS),
			      eventsel &amp; ARCH_PERFMON_EVENTSEL_INT);
}
EXPORT_SYMBOL_GPL(reprogram_counter);

void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	int bit;

<yellow>	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {</yellow>
<yellow>		struct kvm_pmc *pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, bit);</yellow>

<yellow>		if (unlikely(!pmc || !pmc->perf_event)) {</yellow>
<yellow>			clear_bit(bit, pmu->reprogram_pmi);</yellow>
			continue;
		}
<yellow>		reprogram_counter(pmc);</yellow>
	}

	/*
	 * Unused perf_events are only released if the corresponding MSRs
	 * weren&#x27;t accessed during the last vCPU time slice. kvm_arch_sched_in
	 * triggers KVM_REQ_PMU if cleanup is needed.
	 */
<yellow>	if (unlikely(pmu->need_cleanup))</yellow>
<yellow>		kvm_pmu_cleanup(vcpu);</yellow>
<yellow>}</yellow>

/* check if idx is a valid index to access PMU */
bool kvm_pmu_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
{
<yellow>	return static_call(kvm_x86_pmu_is_valid_rdpmc_ecx)(vcpu, idx);</yellow>
}

bool is_vmware_backdoor_pmc(u32 pmc_idx)
{
<blue>	switch (pmc_idx) {</blue>
	case VMWARE_BACKDOOR_PMC_HOST_TSC:
	case VMWARE_BACKDOOR_PMC_REAL_TIME:
	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
		return true;
	}
	return false;
}

static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
{
	u64 ctr_val;

<yellow>	switch (idx) {</yellow>
	case VMWARE_BACKDOOR_PMC_HOST_TSC:
<yellow>		ctr_val = rdtsc();</yellow>
		break;
	case VMWARE_BACKDOOR_PMC_REAL_TIME:
<yellow>		ctr_val = ktime_get_boottime_ns();</yellow>
		break;
	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
<yellow>		ctr_val = ktime_get_boottime_ns() +</yellow>
			vcpu-&gt;kvm-&gt;arch.kvmclock_offset;
		break;
	default:
		return 1;
	}

<yellow>	*data = ctr_val;</yellow>
	return 0;
}

int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
<blue>{</blue>
	bool fast_mode = idx &amp; (1u &lt;&lt; 31);
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;
<blue>	u64 mask = fast_mode ? ~0u : ~0ull;</blue>

<blue>	if (!pmu->version)</blue>
		return 1;

<blue>	if (is_vmware_backdoor_pmc(idx))</blue>
<yellow>		return kvm_pmu_rdpmc_vmware(vcpu, idx, data);</yellow>

<blue>	pmc = static_call(kvm_x86_pmu_rdpmc_ecx_to_pmc)(vcpu, idx, &mask);</blue>
	if (!pmc)
		return 1;

<yellow>	if (!(kvm_read_cr4(vcpu) & X86_CR4_PCE) &&</yellow>
<yellow>	    (static_call(kvm_x86_get_cpl)(vcpu) != 0) &&</yellow>
<yellow>	    (kvm_read_cr0(vcpu) & X86_CR0_PE))</yellow>
		return 1;

<yellow>	*data = pmc_read_counter(pmc) & mask;</yellow>
	return 0;
}

<yellow>void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)</yellow>
{
<yellow>	if (lapic_in_kernel(vcpu)) {</yellow>
<yellow>		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);</yellow>
		kvm_apic_local_deliver(vcpu-&gt;arch.apic, APIC_LVTPC);
	}
<yellow>}</yellow>

bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
{
<blue>	return static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr) ||</blue>
<blue>		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);</blue>
<blue>}</blue>

static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc = static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr);

	if (pmc)
<yellow>		__set_bit(pmc->idx, pmu->pmc_in_use);</yellow>
}

int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
<yellow>	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);</yellow>
}

int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
<blue>	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);</blue>
<blue>	return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);</blue>
}

/* refresh PMU settings. This function generally is called when underlying
 * settings are changed (such as changes of PMU CPUID by guest VMs), which
 * should rarely happen.
 */
void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
{
<yellow>	static_call(kvm_x86_pmu_refresh)(vcpu);</yellow>
}

void kvm_pmu_reset(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);

<blue>	irq_work_sync(&pmu->irq_work);</blue>
	static_call(kvm_x86_pmu_reset)(vcpu);
}

void kvm_pmu_init(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);

<blue>	memset(pmu, 0, sizeof(*pmu));</blue>
	static_call(kvm_x86_pmu_init)(vcpu);
	init_irq_work(&amp;pmu-&gt;irq_work, kvm_pmi_trigger_fn);
	pmu-&gt;event_count = 0;
	pmu-&gt;need_cleanup = false;
	kvm_pmu_refresh(vcpu);
}

/* Release perf_events for vPMCs that have been unused for a full time slice.  */
void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
{
<yellow>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</yellow>
	struct kvm_pmc *pmc = NULL;
	DECLARE_BITMAP(bitmask, X86_PMC_IDX_MAX);
	int i;

	pmu-&gt;need_cleanup = false;

	bitmap_andnot(bitmask, pmu-&gt;all_valid_pmc_idx,
		      pmu-&gt;pmc_in_use, X86_PMC_IDX_MAX);

<yellow>	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {</yellow>
<yellow>		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);</yellow>

<yellow>		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))</yellow>
<yellow>			pmc_stop_counter(pmc);</yellow>
	}

<yellow>	static_call_cond(kvm_x86_pmu_cleanup)(vcpu);</yellow>

	bitmap_zero(pmu-&gt;pmc_in_use, X86_PMC_IDX_MAX);
}

void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
{
	kvm_pmu_reset(vcpu);
}

static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
{
	u64 prev_count;

<yellow>	prev_count = pmc->counter;</yellow>
	pmc-&gt;counter = (pmc-&gt;counter + 1) &amp; pmc_bitmask(pmc);

	reprogram_counter(pmc);
	if (pmc-&gt;counter &lt; prev_count)
<yellow>		__kvm_perf_overflow(pmc, false);</yellow>
}

static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
	unsigned int perf_hw_id)
{
<yellow>	return !((pmc->eventsel ^ perf_get_hw_event_config(perf_hw_id)) &</yellow>
		AMD64_RAW_EVENT_MASK_NB);
}

static inline bool cpl_is_matched(struct kvm_pmc *pmc)
{
	bool select_os, select_user;
<yellow>	u64 config = pmc->current_config;</yellow>

	if (pmc_is_gp(pmc)) {
<yellow>		select_os = config & ARCH_PERFMON_EVENTSEL_OS;</yellow>
		select_user = config &amp; ARCH_PERFMON_EVENTSEL_USR;
	} else {
<yellow>		select_os = config & 0x1;</yellow>
		select_user = config &amp; 0x2;
	}

<yellow>	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;</yellow>
}

void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
{
<blue>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</blue>
	struct kvm_pmc *pmc;
	int i;

<blue>	for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {</blue>
<blue>		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);</blue>

<blue>		if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))</blue>
			continue;

		/* Ignore checks for edge detect, pin control, invert and CMASK bits */
<yellow>		if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))</yellow>
<yellow>			kvm_pmu_incr_counter(pmc);</yellow>
	}
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_pmu_trigger_event);

int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
<yellow>{</yellow>
	struct kvm_pmu_event_filter tmp, *filter;
	size_t size;
	int r;

<yellow>	if (copy_from_user(&tmp, argp, sizeof(tmp)))</yellow>
		return -EFAULT;

<yellow>	if (tmp.action != KVM_PMU_EVENT_ALLOW &&</yellow>
	    tmp.action != KVM_PMU_EVENT_DENY)
		return -EINVAL;

<yellow>	if (tmp.flags != 0)</yellow>
		return -EINVAL;

<yellow>	if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)</yellow>
		return -E2BIG;

<yellow>	size = struct_size(filter, events, tmp.nevents);</yellow>
	filter = kmalloc(size, GFP_KERNEL_ACCOUNT);
	if (!filter)
		return -ENOMEM;

	r = -EFAULT;
<yellow>	if (copy_from_user(filter, argp, size))</yellow>
		goto cleanup;

	/* Ensure nevents can&#x27;t be changed between the user copies. */
<yellow>	*filter = tmp;</yellow>

	/*
	 * Sort the in-kernel list so that we can search it with bsearch.
	 */
	sort(&amp;filter-&gt;events, filter-&gt;nevents, sizeof(__u64), cmp_u64, NULL);

	mutex_lock(&amp;kvm-&gt;lock);
	filter = rcu_replace_pointer(kvm-&gt;arch.pmu_event_filter, filter,
				     mutex_is_locked(&amp;kvm-&gt;lock));
	mutex_unlock(&amp;kvm-&gt;lock);

	synchronize_srcu_expedited(&amp;kvm-&gt;srcu);
	r = 0;
cleanup:
<yellow>	kfree(filter);</yellow>
	return r;
}


</code></pre></td></tr></table>
</body>
</html>
