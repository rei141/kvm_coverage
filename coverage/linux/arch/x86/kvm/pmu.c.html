<doctype html>
<html lang="ja">
<head><title>pmu.c</title><meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
      .split {
         height: 100%;
         position: fixed;
         z-index: 1;
         top: 0;
         overflow-x: hidden;
      }

      .tree {
         left: 0;
         width: 20%;
      }

      .right {
         border-left: 2px solid #444;
         right: 0;
         width: 80%;
         /* font-family: 'Courier New', Courier, monospace;
				color: rgb(80, 80, 80); */
      }
</style>

</head>
<body>
   <div class="split tree">
      <ul id="file_list">
      </ul>
   </div>
   <div class="split right">
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line><script>for (let i = 1; i <= 653; i++){
         document.write(i+".\n");
   }
         </script></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * Kernel-based Virtual Machine -- Performance Monitoring Unit support
 *
 * Copyright 2015 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@redhat.com&gt;
 *   Gleb Natapov &lt;gleb@redhat.com&gt;
 *   Wei Huang    &lt;wei@redhat.com&gt;
 */

#include &lt;linux/types.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/perf_event.h&gt;
#include &lt;linux/bsearch.h&gt;
#include &lt;linux/sort.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &lt;asm/cpu_device_id.h&gt;
#include &quot;x86.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;lapic.h&quot;
#include &quot;pmu.h&quot;

/* This is enough to filter the vast majority of currently defined events. */
#define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300

struct x86_pmu_capability __read_mostly kvm_pmu_cap;
EXPORT_SYMBOL_GPL(kvm_pmu_cap);

static const struct x86_cpu_id vmx_icl_pebs_cpu[] = {
	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D, NULL),
	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X, NULL),
	{}
};

/* NOTE:
 * - Each perf counter is defined as &quot;struct kvm_pmc&quot;;
 * - There are two types of perf counters: general purpose (gp) and fixed.
 *   gp counters are stored in gp_counters[] and fixed counters are stored
 *   in fixed_counters[] respectively. Both of them are part of &quot;struct
 *   kvm_pmu&quot;;
 * - pmu.c understands the difference between gp counters and fixed counters.
 *   However AMD doesn&#x27;t support fixed-counters;
 * - There are three types of index to access perf counters (PMC):
 *     1. MSR (named msr): For example Intel has MSR_IA32_PERFCTRn and AMD
 *        has MSR_K7_PERFCTRn and, for families 15H and later,
 *        MSR_F15H_PERF_CTRn, where MSR_F15H_PERF_CTR[0-3] are
 *        aliased to MSR_K7_PERFCTRn.
 *     2. MSR Index (named idx): This normally is used by RDPMC instruction.
 *        For instance AMD RDPMC instruction uses 0000_0003h in ECX to access
 *        C001_0007h (MSR_K7_PERCTR3). Intel has a similar mechanism, except
 *        that it also supports fixed counters. idx can be used to as index to
 *        gp and fixed counters.
 *     3. Global PMC Index (named pmc): pmc is an index specific to PMU
 *        code. Each pmc, stored in kvm_pmc.idx field, is unique across
 *        all perf counters (both gp and fixed). The mapping relationship
 *        between pmc and perf counters is as the following:
 *        * Intel: [0 .. KVM_INTEL_PMC_MAX_GENERIC-1] &lt;=&gt; gp counters
 *                 [INTEL_PMC_IDX_FIXED .. INTEL_PMC_IDX_FIXED + 2] &lt;=&gt; fixed
 *        * AMD:   [0 .. AMD64_NUM_COUNTERS-1] and, for families 15H
 *          and later, [0 .. AMD64_NUM_COUNTERS_CORE-1] &lt;=&gt; gp counters
 */

static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;

#define KVM_X86_PMU_OP(func)					     \
	DEFINE_STATIC_CALL_NULL(kvm_x86_pmu_##func,			     \
				*(((struct kvm_pmu_ops *)0)-&gt;func));
#define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
#include &lt;asm/kvm-x86-pmu-ops.h&gt;

void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
{
<yellow>	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));</yellow>

#define __KVM_X86_PMU_OP(func) \
	static_call_update(kvm_x86_pmu_##func, kvm_pmu_ops.func);
#define KVM_X86_PMU_OP(func) \
	WARN_ON(!kvm_pmu_ops.func); __KVM_X86_PMU_OP(func)
#define KVM_X86_PMU_OP_OPTIONAL __KVM_X86_PMU_OP
#include &lt;asm/kvm-x86-pmu-ops.h&gt;
#undef __KVM_X86_PMU_OP
}

static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
{
<yellow>	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);</yellow>
}

static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
{
	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
<yellow>	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);</yellow>

<yellow>	kvm_pmu_deliver_pmi(vcpu);</yellow>
<yellow>}</yellow>

<yellow>static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)</yellow>
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	bool skip_pmi = false;

<yellow>	if (pmc->perf_event && pmc->perf_event->attr.precise_ip) {</yellow>
<yellow>		if (!in_pmi) {</yellow>
			/*
			 * TODO: KVM is currently _choosing_ to not generate records
			 * for emulated instructions, avoiding BUFFER_OVF PMI when
			 * there are no records. Strictly speaking, it should be done
			 * as well in the right context to improve sampling accuracy.
			 */
			skip_pmi = true;
		} else {
			/* Indicate PEBS overflow PMI to guest. */
<yellow>			skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,</yellow>
						      (unsigned long *)&amp;pmu-&gt;global_status);
		}
	} else {
<yellow>		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);</yellow>
	}

<yellow>	if (!pmc->intr || skip_pmi)</yellow>
		return;

	/*
	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
	 * can be ejected on a guest mode re-entry. Otherwise we can&#x27;t
	 * be sure that vcpu wasn&#x27;t executing hlt instruction at the
	 * time of vmexit and is not going to re-enter guest mode until
	 * woken up. So we should wake it, but this is impossible from
	 * NMI context. Do it from irq work instead.
	 */
<yellow>	if (in_pmi && !kvm_handling_nmi_from_guest(pmc->vcpu))</yellow>
<yellow>		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);</yellow>
	else
<yellow>		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);</yellow>
<yellow>}</yellow>

static void kvm_perf_overflow(struct perf_event *perf_event,
			      struct perf_sample_data *data,
			      struct pt_regs *regs)
{
<yellow>	struct kvm_pmc *pmc = perf_event->overflow_handler_context;</yellow>

	/*
	 * Ignore overflow events for counters that are scheduled to be
	 * reprogrammed, e.g. if a PMI for the previous event races with KVM&#x27;s
	 * handling of a related guest WRMSR.
	 */
	if (test_and_set_bit(pmc-&gt;idx, pmc_to_pmu(pmc)-&gt;reprogram_pmi))
		return;

<yellow>	__kvm_perf_overflow(pmc, true);</yellow>

	kvm_make_request(KVM_REQ_PMU, pmc-&gt;vcpu);
<yellow>}</yellow>

<yellow>static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,</yellow>
				 bool exclude_user, bool exclude_kernel,
				 bool intr)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	struct perf_event *event;
	struct perf_event_attr attr = {
		.type = type,
		.size = sizeof(attr),
		.pinned = true,
		.exclude_idle = true,
		.exclude_host = 1,
		.exclude_user = exclude_user,
		.exclude_kernel = exclude_kernel,
		.config = config,
	};
	bool pebs = test_bit(pmc-&gt;idx, (unsigned long *)&amp;pmu-&gt;pebs_enable);

<yellow>	attr.sample_period = get_sample_period(pmc, pmc->counter);</yellow>

<yellow>	if ((attr.config & HSW_IN_TX_CHECKPOINTED) &&</yellow>
<yellow>	    guest_cpuid_is_intel(pmc->vcpu)) {</yellow>
		/*
		 * HSW_IN_TX_CHECKPOINTED is not supported with nonzero
		 * period. Just clear the sample period so at least
		 * allocating the counter doesn&#x27;t fail.
		 */
<yellow>		attr.sample_period = 0;</yellow>
	}
<yellow>	if (pebs) {</yellow>
		/*
		 * The non-zero precision level of guest event makes the ordinary
		 * guest event becomes a guest PEBS event and triggers the host
		 * PEBS PMI handler to determine whether the PEBS overflow PMI
		 * comes from the host counters or the guest.
		 *
		 * For most PEBS hardware events, the difference in the software
		 * precision levels of guest and host PEBS events will not affect
		 * the accuracy of the PEBS profiling result, because the &quot;event IP&quot;
		 * in the PEBS record is calibrated on the guest side.
		 *
		 * On Icelake everything is fine. Other hardware (GLC+, TNT+) that
		 * could possibly care here is unsupported and needs changes.
		 */
<yellow>		attr.precise_ip = 1;</yellow>
<yellow>		if (x86_match_cpu(vmx_icl_pebs_cpu) && pmc->idx == 32)</yellow>
<yellow>			attr.precise_ip = 3;</yellow>
	}

<yellow>	event = perf_event_create_kernel_counter(&attr, -1, current,</yellow>
						 kvm_perf_overflow, pmc);
	if (IS_ERR(event)) {
<yellow>		pr_debug_ratelimited("kvm_pmu: event creation failed %ld for pmc->idx = %d\n",</yellow>
			    PTR_ERR(event), pmc-&gt;idx);
<yellow>		return PTR_ERR(event);</yellow>
	}

<yellow>	pmc->perf_event = event;</yellow>
	pmc_to_pmu(pmc)-&gt;event_count++;
	pmc-&gt;is_paused = false;
	pmc-&gt;intr = intr || pebs;
	return 0;
}

static void pmc_pause_counter(struct kvm_pmc *pmc)
{
<yellow>	u64 counter = pmc->counter;</yellow>

<yellow>	if (!pmc->perf_event || pmc->is_paused)</yellow>
		return;

	/* update counter, reset event value to avoid redundant accumulation */
<yellow>	counter += perf_event_pause(pmc->perf_event, true);</yellow>
	pmc-&gt;counter = counter &amp; pmc_bitmask(pmc);
	pmc-&gt;is_paused = true;
}

static bool pmc_resume_counter(struct kvm_pmc *pmc)
{
<yellow>	if (!pmc->perf_event)</yellow>
		return false;

	/* recalibrate sample period and check if it&#x27;s accepted by perf core */
<yellow>	if (is_sampling_event(pmc->perf_event) &&</yellow>
<yellow>	    perf_event_period(pmc->perf_event,</yellow>
			      get_sample_period(pmc, pmc-&gt;counter)))
		return false;

<yellow>	if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=</yellow>
	    (!!pmc-&gt;perf_event-&gt;attr.precise_ip))
		return false;

	/* reuse perf_event to serve as pmc_reprogram_counter() does*/
<yellow>	perf_event_enable(pmc->perf_event);</yellow>
	pmc-&gt;is_paused = false;

	return true;
}

static int cmp_u64(const void *pa, const void *pb)
{
<yellow>	u64 a = *(u64 *)pa;</yellow>
	u64 b = *(u64 *)pb;

	return (a &gt; b) - (a &lt; b);
}

static bool check_pmu_event_filter(struct kvm_pmc *pmc)
{
	struct kvm_pmu_event_filter *filter;
<yellow>	struct kvm *kvm = pmc->vcpu->kvm;</yellow>
	bool allow_event = true;
	__u64 key;
	int idx;

	if (!static_call(kvm_x86_pmu_hw_event_available)(pmc))
		return false;

<yellow>	filter = srcu_dereference(kvm->arch.pmu_event_filter, &kvm->srcu);</yellow>
<yellow>	if (!filter)</yellow>
		goto out;

<yellow>	if (pmc_is_gp(pmc)) {</yellow>
<yellow>		key = pmc->eventsel & AMD64_RAW_EVENT_MASK_NB;</yellow>
		if (bsearch(&amp;key, filter-&gt;events, filter-&gt;nevents,
			    sizeof(__u64), cmp_u64))
<yellow>			allow_event = filter->action == KVM_PMU_EVENT_ALLOW;</yellow>
		else
<yellow>			allow_event = filter->action == KVM_PMU_EVENT_DENY;</yellow>
	} else {
<yellow>		idx = pmc->idx - INTEL_PMC_IDX_FIXED;</yellow>
		if (filter-&gt;action == KVM_PMU_EVENT_DENY &amp;&amp;
<yellow>		    test_bit(idx, (ulong *)&filter->fixed_counter_bitmap))</yellow>
			allow_event = false;
<yellow>		if (filter->action == KVM_PMU_EVENT_ALLOW &&</yellow>
<yellow>		    !test_bit(idx, (ulong *)&filter->fixed_counter_bitmap))</yellow>
			allow_event = false;
	}

out:
	return allow_event;
}

static void reprogram_counter(struct kvm_pmc *pmc)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	u64 eventsel = pmc-&gt;eventsel;
	u64 new_config = eventsel;
	u8 fixed_ctr_ctrl;

<yellow>	pmc_pause_counter(pmc);</yellow>

<yellow>	if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))</yellow>
		goto reprogram_complete;

<yellow>	if (!check_pmu_event_filter(pmc))</yellow>
		goto reprogram_complete;

<yellow>	if (pmc->counter < pmc->prev_counter)</yellow>
<yellow>		__kvm_perf_overflow(pmc, false);</yellow>

<yellow>	if (eventsel & ARCH_PERFMON_EVENTSEL_PIN_CONTROL)</yellow>
<yellow>		printk_once("kvm pmu: pin control bit is ignored\n");</yellow>

<yellow>	if (pmc_is_fixed(pmc)) {</yellow>
<yellow>		fixed_ctr_ctrl = fixed_ctrl_field(pmu->fixed_ctr_ctrl,</yellow>
						  pmc-&gt;idx - INTEL_PMC_IDX_FIXED);
		if (fixed_ctr_ctrl &amp; 0x1)
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_OS;</yellow>
<yellow>		if (fixed_ctr_ctrl & 0x2)</yellow>
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_USR;</yellow>
<yellow>		if (fixed_ctr_ctrl & 0x8)</yellow>
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_INT;</yellow>
<yellow>		new_config = (u64)fixed_ctr_ctrl;</yellow>
	}

<yellow>	if (pmc->current_config == new_config && pmc_resume_counter(pmc))</yellow>
		goto reprogram_complete;

<yellow>	pmc_release_perf_event(pmc);</yellow>

<yellow>	pmc->current_config = new_config;</yellow>

	/*
	 * If reprogramming fails, e.g. due to contention, leave the counter&#x27;s
	 * regprogram bit set, i.e. opportunistically try again on the next PMU
	 * refresh.  Don&#x27;t make a new request as doing so can stall the guest
	 * if reprogramming repeatedly fails.
	 */
	if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
				  (eventsel &amp; pmu-&gt;raw_event_mask),
				  !(eventsel &amp; ARCH_PERFMON_EVENTSEL_USR),
				  !(eventsel &amp; ARCH_PERFMON_EVENTSEL_OS),
				  eventsel &amp; ARCH_PERFMON_EVENTSEL_INT))
		return;

reprogram_complete:
<yellow>	clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);</yellow>
	pmc-&gt;prev_counter = 0;
}

void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
<yellow>{</yellow>
<yellow>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</yellow>
	int bit;

<yellow>	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {</yellow>
<yellow>		struct kvm_pmc *pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, bit);</yellow>

		if (unlikely(!pmc)) {
<yellow>			clear_bit(bit, pmu->reprogram_pmi);</yellow>
			continue;
		}

<yellow>		reprogram_counter(pmc);</yellow>
	}

	/*
	 * Unused perf_events are only released if the corresponding MSRs
	 * weren&#x27;t accessed during the last vCPU time slice. kvm_arch_sched_in
	 * triggers KVM_REQ_PMU if cleanup is needed.
	 */
<yellow>	if (unlikely(pmu->need_cleanup))</yellow>
<yellow>		kvm_pmu_cleanup(vcpu);</yellow>
}

/* check if idx is a valid index to access PMU */
bool kvm_pmu_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
{
<yellow>	return static_call(kvm_x86_pmu_is_valid_rdpmc_ecx)(vcpu, idx);</yellow>
}

bool is_vmware_backdoor_pmc(u32 pmc_idx)
{
<yellow>	switch (pmc_idx) {</yellow>
	case VMWARE_BACKDOOR_PMC_HOST_TSC:
	case VMWARE_BACKDOOR_PMC_REAL_TIME:
	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
		return true;
	}
	return false;
}

static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
{
	u64 ctr_val;

<yellow>	switch (idx) {</yellow>
	case VMWARE_BACKDOOR_PMC_HOST_TSC:
<yellow>		ctr_val = rdtsc();</yellow>
		break;
	case VMWARE_BACKDOOR_PMC_REAL_TIME:
<yellow>		ctr_val = ktime_get_boottime_ns();</yellow>
		break;
	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
<yellow>		ctr_val = ktime_get_boottime_ns() +</yellow>
			vcpu-&gt;kvm-&gt;arch.kvmclock_offset;
		break;
	default:
		return 1;
	}

<yellow>	*data = ctr_val;</yellow>
	return 0;
}

int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
<blue>{</blue>
	bool fast_mode = idx &amp; (1u &lt;&lt; 31);
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;
<blue>	u64 mask = fast_mode ? ~0u : ~0ull;</blue>

<blue>	if (!pmu->version)</blue>
		return 1;

<yellow>	if (is_vmware_backdoor_pmc(idx))</yellow>
<yellow>		return kvm_pmu_rdpmc_vmware(vcpu, idx, data);</yellow>

<yellow>	pmc = static_call(kvm_x86_pmu_rdpmc_ecx_to_pmc)(vcpu, idx, &mask);</yellow>
	if (!pmc)
		return 1;

<yellow>	if (!(kvm_read_cr4(vcpu) & X86_CR4_PCE) &&</yellow>
<yellow>	    (static_call(kvm_x86_get_cpl)(vcpu) != 0) &&</yellow>
<yellow>	    (kvm_read_cr0(vcpu) & X86_CR0_PE))</yellow>
		return 1;

<yellow>	*data = pmc_read_counter(pmc) & mask;</yellow>
	return 0;
}

<yellow>void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)</yellow>
{
<yellow>	if (lapic_in_kernel(vcpu)) {</yellow>
<yellow>		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);</yellow>
		kvm_apic_local_deliver(vcpu-&gt;arch.apic, APIC_LVTPC);
	}
<yellow>}</yellow>

bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
{
<blue>	return static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr) ||</blue>
<blue>		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);</blue>
<blue>}</blue>

static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc = static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr);

	if (pmc)
<yellow>		__set_bit(pmc->idx, pmu->pmc_in_use);</yellow>
}

int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
<yellow>	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);</yellow>
}

int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
<yellow>	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);</yellow>
<yellow>	return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);</yellow>
}

/* refresh PMU settings. This function generally is called when underlying
 * settings are changed (such as changes of PMU CPUID by guest VMs), which
 * should rarely happen.
 */
void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
{
<blue>	static_call(kvm_x86_pmu_refresh)(vcpu);</blue>
}

void kvm_pmu_reset(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);

<blue>	irq_work_sync(&pmu->irq_work);</blue>
	static_call(kvm_x86_pmu_reset)(vcpu);
}

void kvm_pmu_init(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);

<blue>	memset(pmu, 0, sizeof(*pmu));</blue>
	static_call(kvm_x86_pmu_init)(vcpu);
	init_irq_work(&amp;pmu-&gt;irq_work, kvm_pmi_trigger_fn);
	pmu-&gt;event_count = 0;
	pmu-&gt;need_cleanup = false;
	kvm_pmu_refresh(vcpu);
}

/* Release perf_events for vPMCs that have been unused for a full time slice.  */
void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
{
<yellow>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</yellow>
	struct kvm_pmc *pmc = NULL;
	DECLARE_BITMAP(bitmask, X86_PMC_IDX_MAX);
	int i;

	pmu-&gt;need_cleanup = false;

	bitmap_andnot(bitmask, pmu-&gt;all_valid_pmc_idx,
		      pmu-&gt;pmc_in_use, X86_PMC_IDX_MAX);

<yellow>	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {</yellow>
<yellow>		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);</yellow>

<yellow>		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))</yellow>
<yellow>			pmc_stop_counter(pmc);</yellow>
	}

<yellow>	static_call_cond(kvm_x86_pmu_cleanup)(vcpu);</yellow>

	bitmap_zero(pmu-&gt;pmc_in_use, X86_PMC_IDX_MAX);
}

void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
{
	kvm_pmu_reset(vcpu);
}

static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
{
<yellow>	pmc->prev_counter = pmc->counter;</yellow>
	pmc-&gt;counter = (pmc-&gt;counter + 1) &amp; pmc_bitmask(pmc);
	kvm_pmu_request_counter_reprogam(pmc);
}

static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
	unsigned int perf_hw_id)
{
<yellow>	return !((pmc->eventsel ^ perf_get_hw_event_config(perf_hw_id)) &</yellow>
		AMD64_RAW_EVENT_MASK_NB);
}

static inline bool cpl_is_matched(struct kvm_pmc *pmc)
{
	bool select_os, select_user;
	u64 config;

<yellow>	if (pmc_is_gp(pmc)) {</yellow>
<yellow>		config = pmc->eventsel;</yellow>
		select_os = config &amp; ARCH_PERFMON_EVENTSEL_OS;
		select_user = config &amp; ARCH_PERFMON_EVENTSEL_USR;
	} else {
<yellow>		config = fixed_ctrl_field(pmc_to_pmu(pmc)->fixed_ctr_ctrl,</yellow>
					  pmc-&gt;idx - INTEL_PMC_IDX_FIXED);
		select_os = config &amp; 0x1;
		select_user = config &amp; 0x2;
	}

<yellow>	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;</yellow>
}

void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
{
<blue>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</blue>
	struct kvm_pmc *pmc;
	int i;

<yellow>	for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {</yellow>
<yellow>		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);</yellow>

<yellow>		if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))</yellow>
			continue;

		/* Ignore checks for edge detect, pin control, invert and CMASK bits */
<yellow>		if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))</yellow>
<yellow>			kvm_pmu_incr_counter(pmc);</yellow>
	}
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_pmu_trigger_event);

int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
<yellow>{</yellow>
	struct kvm_pmu_event_filter tmp, *filter;
	struct kvm_vcpu *vcpu;
	unsigned long i;
	size_t size;
	int r;

<yellow>	if (copy_from_user(&tmp, argp, sizeof(tmp)))</yellow>
		return -EFAULT;

<yellow>	if (tmp.action != KVM_PMU_EVENT_ALLOW &&</yellow>
	    tmp.action != KVM_PMU_EVENT_DENY)
		return -EINVAL;

<yellow>	if (tmp.flags != 0)</yellow>
		return -EINVAL;

<yellow>	if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)</yellow>
		return -E2BIG;

<yellow>	size = struct_size(filter, events, tmp.nevents);</yellow>
	filter = kmalloc(size, GFP_KERNEL_ACCOUNT);
	if (!filter)
		return -ENOMEM;

	r = -EFAULT;
<yellow>	if (copy_from_user(filter, argp, size))</yellow>
		goto cleanup;

	/* Ensure nevents can&#x27;t be changed between the user copies. */
<yellow>	*filter = tmp;</yellow>

	/*
	 * Sort the in-kernel list so that we can search it with bsearch.
	 */
	sort(&amp;filter-&gt;events, filter-&gt;nevents, sizeof(__u64), cmp_u64, NULL);

	mutex_lock(&amp;kvm-&gt;lock);
<yellow>	filter = rcu_replace_pointer(kvm->arch.pmu_event_filter, filter,</yellow>
				     mutex_is_locked(&amp;kvm-&gt;lock));
	synchronize_srcu_expedited(&amp;kvm-&gt;srcu);

	BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)-&gt;reprogram_pmi) &gt;
		     sizeof(((struct kvm_pmu *)0)-&gt;__reprogram_pmi));

	kvm_for_each_vcpu(i, vcpu, kvm)
<yellow>		atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);</yellow>

<yellow>	kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);</yellow>

	mutex_unlock(&amp;kvm-&gt;lock);

	r = 0;
cleanup:
<yellow>	kfree(filter);</yellow>
	return r;
}


</code></pre></td></tr></table>
</div><script>const fileList = document.getElementById('file_list')
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/mmu/mmu.c.html">mmu.c 59.8%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/nested.c.html">nested.c 84.9%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/vmx.c.html">vmx.c 60.0%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/x86.c.html">x86.c 52.0%</li>`
</script></body></html>