<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.
57.
58.
59.
60.
61.
62.
63.
64.
65.
66.
67.
68.
69.
70.
71.
72.
73.
74.
75.
76.
77.
78.
79.
80.
81.
82.
83.
84.
85.
86.
87.
88.
89.
90.
91.
92.
93.
94.
95.
96.
97.
98.
99.
100.
101.
102.
103.
104.
105.
106.
107.
108.
109.
110.
111.
112.
113.
114.
115.
116.
117.
118.
119.
120.
121.
122.
123.
124.
125.
126.
127.
128.
129.
130.
131.
132.
133.
134.
135.
136.
137.
138.
139.
140.
141.
142.
143.
144.
145.
146.
147.
148.
149.
150.
151.
152.
153.
154.
155.
156.
157.
158.
159.
160.
161.
162.
163.
164.
165.
166.
167.
168.
169.
170.
171.
172.
173.
174.
175.
176.
177.
178.
179.
180.
181.
182.
183.
184.
185.
186.
187.
188.
189.
190.
191.
192.
193.
194.
195.
196.
197.
198.
199.
200.
201.
202.
203.
204.
205.
206.
207.
208.
209.
210.
211.
212.
213.
214.
215.
216.
217.
218.
219.
220.
221.
222.
223.
224.
225.
226.
227.
228.
229.
230.
231.
232.
233.
234.
235.
236.
237.
238.
239.
240.
241.
242.
243.
244.
245.
246.
247.
248.
249.
250.
251.
252.
253.
254.
255.
256.
257.
258.
259.
260.
261.
262.
263.
264.
265.
266.
267.
268.
269.
270.
271.
272.
273.
274.
275.
276.
277.
278.
279.
280.
281.
282.
283.
284.
285.
286.
287.
288.
289.
290.
291.
292.
293.
294.
295.
296.
297.
298.
299.
300.
301.
302.
303.
304.
305.
306.
307.
308.
309.
310.
311.
312.
313.
314.
315.
316.
317.
318.
319.
320.
321.
322.
323.
324.
325.
326.
327.
328.
329.
330.
331.
332.
333.
334.
335.
336.
337.
338.
339.
340.
341.
342.
343.
344.
345.
346.
347.
348.
349.
350.
351.
352.
353.
354.
355.
356.
357.
358.
359.
360.
361.
362.
363.
364.
365.
366.
367.
368.
369.
370.
371.
372.
373.
374.
375.
376.
377.
378.
379.
380.
381.
382.
383.
384.
385.
386.
387.
388.
389.
390.
391.
392.
393.
394.
395.
396.
397.
398.
399.
400.
401.
402.
403.
404.
405.
406.
407.
408.
409.
410.
411.
412.
413.
414.
415.
416.
417.
418.
419.
420.
421.
422.
423.
424.
425.
426.
427.
428.
429.
430.
431.
432.
433.
434.
435.
436.
437.
438.
439.
440.
441.
442.
443.
444.
445.
446.
447.
448.
449.
450.
451.
452.
453.
454.
455.
456.
457.
458.
459.
460.
461.
462.
463.
464.
465.
466.
467.
468.
469.
470.
471.
472.
473.
474.
475.
476.
477.
478.
479.
480.
481.
482.
483.
484.
485.
486.
487.
488.
489.
490.
491.
492.
493.
494.
495.
496.
497.
498.
499.
500.
501.
502.
503.
504.
505.
506.
507.
508.
509.
510.
511.
512.
513.
514.
515.
516.
517.
518.
519.
520.
521.
522.
523.
524.
525.
526.
527.
528.
529.
530.
531.
532.
533.
534.
535.
536.
537.
538.
539.
540.
541.
542.
543.
544.
545.
546.
547.
548.
549.
550.
551.
552.
553.
554.
555.
556.
557.
558.
559.
560.
561.
562.
563.
564.
565.
566.
567.
568.
569.
570.
571.
572.
573.
574.
575.
576.
577.
578.
579.
580.
581.
582.
583.
584.
585.
586.
587.
588.
589.
590.
591.
592.
593.
594.
595.
596.
597.
598.
599.
600.
601.
602.
603.
604.
605.
606.
607.
608.
609.
610.
611.
612.
613.
614.
615.
616.
617.
618.
619.
620.
621.
622.
623.
624.
625.
626.
627.
628.
629.
630.
631.
632.
633.
634.
635.
636.
637.
638.
639.
640.
641.
642.
643.
644.
645.
646.
647.
648.
649.
650.
651.
652.
653.
654.
655.
656.
657.
658.
659.
660.
661.
662.
663.
664.
665.
666.
667.
668.
669.
670.
671.
672.
673.
674.
675.
676.
677.
678.
679.
680.
681.
682.
683.
684.
685.
686.
687.
688.
689.
690.
691.
692.
693.
694.
695.
696.
697.
698.
699.
700.
701.
702.
703.
704.
705.
706.
707.
708.
709.
710.
711.
712.
713.
714.
715.
716.
717.
718.
719.
720.
721.
722.
723.
724.
725.
726.
727.
728.
729.
730.
731.
732.
733.
734.
735.
736.
737.
738.
739.
740.
741.
742.
743.
744.
745.
746.
747.
748.
749.
750.
751.
752.
</code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">/*
 * 8253/8254 interval timer emulation
 *
 * Copyright (c) 2003-2004 Fabrice Bellard
 * Copyright (c) 2006 Intel Corporation
 * Copyright (c) 2007 Keir Fraser, XenSource Inc
 * Copyright (c) 2008 Intel Corporation
 * Copyright 2009 Red Hat, Inc. and/or its affiliates.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the &quot;Software&quot;), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 * Authors:
 *   Sheng Yang &lt;sheng.yang@intel.com&gt;
 *   Based on QEMU and Xen.
 */

#define pr_fmt(fmt) &quot;pit: &quot; fmt

#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/slab.h&gt;

#include &quot;ioapic.h&quot;
#include &quot;irq.h&quot;
#include &quot;i8254.h&quot;
#include &quot;x86.h&quot;

#ifndef CONFIG_X86_64
#define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
#else
#define mod_64(x, y) ((x) % (y))
#endif

#define RW_STATE_LSB 1
#define RW_STATE_MSB 2
#define RW_STATE_WORD0 3
#define RW_STATE_WORD1 4

static void pit_set_gate(struct kvm_pit *pit, int channel, u32 val)
{
	struct kvm_kpit_channel_state *c = &amp;pit-&gt;pit_state.channels[channel];

	switch (c-&gt;mode) {
	default:
	case 0:
	case 4:
		/* XXX: just disable/enable counting */
		break;
	case 1:
	case 2:
	case 3:
	case 5:
		/* Restart counting on rising edge. */
<yellow>		if (c->gate < val)</yellow>
<yellow>			c->count_load_time = ktime_get();</yellow>
		break;
	}

<yellow>	c->gate = val;</yellow>
}

static int pit_get_gate(struct kvm_pit *pit, int channel)
{
	return pit-&gt;pit_state.channels[channel].gate;
}

static s64 __kpit_elapsed(struct kvm_pit *pit)
{
	s64 elapsed;
	ktime_t remaining;
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;

<blue>	if (!ps->period)</blue>
		return 0;

	/*
	 * The Counter does not stop when it reaches zero. In
	 * Modes 0, 1, 4, and 5 the Counter ``wraps around&#x27;&#x27; to
	 * the highest count, either FFFF hex for binary counting
	 * or 9999 for BCD counting, and continues counting.
	 * Modes 2 and 3 are periodic; the Counter reloads
	 * itself with the initial count and continues counting
	 * from there.
	 */
<blue>	remaining = hrtimer_get_remaining(&ps->timer);</blue>
	elapsed = ps-&gt;period - ktime_to_ns(remaining);

	return elapsed;
}

<yellow>static s64 kpit_elapsed(struct kvm_pit *pit, struct kvm_kpit_channel_state *c,</yellow>
			int channel)
{
	if (channel == 0)
<blue>		return __kpit_elapsed(pit);</blue>

<yellow>	return ktime_to_ns(ktime_sub(ktime_get(), c->count_load_time));</yellow>
}

static int pit_get_count(struct kvm_pit *pit, int channel)
{
<blue>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</blue>
	s64 d, t;
	int counter;

<blue>	t = kpit_elapsed(pit, c, channel);</blue>
<blue>	d = mul_u64_u32_div(t, KVM_PIT_FREQ, NSEC_PER_SEC);</blue>

	switch (c-&gt;mode) {
	case 0:
	case 1:
	case 4:
	case 5:
<yellow>		counter = (c->count - d) & 0xffff;</yellow>
		break;
	case 3:
		/* XXX: may be incorrect for odd counts */
<blue>		counter = c->count - (mod_64((2 * d), c->count));</blue>
		break;
	default:
<yellow>		counter = c->count - mod_64(d, c->count);</yellow>
		break;
	}
	return counter;
<blue>}</blue>

static int pit_get_out(struct kvm_pit *pit, int channel)
{
<yellow>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</yellow>
	s64 d, t;
	int out;

<yellow>	t = kpit_elapsed(pit, c, channel);</yellow>
<yellow>	d = mul_u64_u32_div(t, KVM_PIT_FREQ, NSEC_PER_SEC);</yellow>

	switch (c-&gt;mode) {
	default:
	case 0:
<yellow>		out = (d >= c->count);</yellow>
		break;
	case 1:
<yellow>		out = (d < c->count);</yellow>
		break;
	case 2:
<yellow>		out = ((mod_64(d, c->count) == 0) && (d != 0));</yellow>
		break;
	case 3:
<yellow>		out = (mod_64(d, c->count) < ((c->count + 1) >> 1));</yellow>
		break;
	case 4:
	case 5:
<yellow>		out = (d == c->count);</yellow>
		break;
	}

	return out;
<yellow>}</yellow>

<blue>static void pit_latch_count(struct kvm_pit *pit, int channel)</blue>
{
<blue>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</blue>

	if (!c-&gt;count_latched) {
<blue>		c->latched_count = pit_get_count(pit, channel);</blue>
		c-&gt;count_latched = c-&gt;rw_mode;
	}
}

static void pit_latch_status(struct kvm_pit *pit, int channel)
{
<yellow>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</yellow>

	if (!c-&gt;status_latched) {
		/* TODO: Return NULL COUNT (bit 6). */
<yellow>		c->status = ((pit_get_out(pit, channel) << 7) |</yellow>
				(c-&gt;rw_mode &lt;&lt; 4) |
				(c-&gt;mode &lt;&lt; 1) |
				c-&gt;bcd);
		c-&gt;status_latched = 1;
	}
}

static inline struct kvm_pit *pit_state_to_pit(struct kvm_kpit_state *ps)
{
	return container_of(ps, struct kvm_pit, pit_state);
}

static void kvm_pit_ack_irq(struct kvm_irq_ack_notifier *kian)
{
	struct kvm_kpit_state *ps = container_of(kian, struct kvm_kpit_state,
						 irq_ack_notifier);
	struct kvm_pit *pit = pit_state_to_pit(ps);

<blue>	atomic_set(&ps->irq_ack, 1);</blue>
	/* irq_ack should be set before pending is read.  Order accesses with
	 * inc(pending) in pit_timer_fn and xchg(irq_ack, 0) in pit_do_work.
	 */
	smp_mb();
<blue>	if (atomic_dec_if_positive(&ps->pending) > 0)</blue>
<blue>		kthread_queue_work(pit->worker, &pit->expired);</blue>
<blue>}</blue>

void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)
{
<blue>	struct kvm_pit *pit = vcpu->kvm->arch.vpit;</blue>
	struct hrtimer *timer;

	/* Somewhat arbitrarily make vcpu0 the owner of the PIT. */
<blue>	if (vcpu->vcpu_id || !pit)</blue>
		return;

	timer = &amp;pit-&gt;pit_state.timer;
<blue>	mutex_lock(&pit->pit_state.lock);</blue>
	if (hrtimer_cancel(timer))
<blue>		hrtimer_start_expires(timer, HRTIMER_MODE_ABS);</blue>
<blue>	mutex_unlock(&pit->pit_state.lock);</blue>
<blue>}</blue>

static void destroy_pit_timer(struct kvm_pit *pit)
{
<blue>	hrtimer_cancel(&pit->pit_state.timer);</blue>
	kthread_flush_work(&amp;pit-&gt;expired);
}

static void pit_do_work(struct kthread_work *work)
<yellow>{</yellow>
	struct kvm_pit *pit = container_of(work, struct kvm_pit, expired);
<yellow>	struct kvm *kvm = pit->kvm;</yellow>
	struct kvm_vcpu *vcpu;
	unsigned long i;
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;

<yellow>	if (atomic_read(&ps->reinject) && !atomic_xchg(&ps->irq_ack, 0))</yellow>
<yellow>		return;</yellow>

<yellow>	kvm_set_irq(kvm, pit->irq_source_id, 0, 1, false);</yellow>
	kvm_set_irq(kvm, pit-&gt;irq_source_id, 0, 0, false);

	/*
	 * Provides NMI watchdog support via Virtual Wire mode.
	 * The route is: PIT -&gt; LVT0 in NMI mode.
	 *
	 * Note: Our Virtual Wire implementation does not follow
	 * the MP specification.  We propagate a PIT interrupt to all
	 * VCPUs and only when LVT0 is in NMI mode.  The interrupt can
	 * also be simultaneously delivered through PIC and IOAPIC.
	 */
	if (atomic_read(&amp;kvm-&gt;arch.vapics_in_nmi_mode) &gt; 0)
<yellow>		kvm_for_each_vcpu(i, vcpu, kvm)</yellow>
<yellow>			kvm_apic_nmi_wd_deliver(vcpu);</yellow>
}

static enum hrtimer_restart pit_timer_fn(struct hrtimer *data)
{
	struct kvm_kpit_state *ps = container_of(data, struct kvm_kpit_state, timer);
	struct kvm_pit *pt = pit_state_to_pit(ps);

<yellow>	if (atomic_read(&ps->reinject))</yellow>
<yellow>		atomic_inc(&ps->pending);</yellow>

<yellow>	kthread_queue_work(pt->worker, &pt->expired);</yellow>

<yellow>	if (ps->is_periodic) {</yellow>
<yellow>		hrtimer_add_expires_ns(&ps->timer, ps->period);</yellow>
		return HRTIMER_RESTART;
	} else
		return HRTIMER_NORESTART;
<yellow>}</yellow>

static inline void kvm_pit_reset_reinject(struct kvm_pit *pit)
{
<blue>	atomic_set(&pit->pit_state.pending, 0);</blue>
	atomic_set(&amp;pit-&gt;pit_state.irq_ack, 1);
}

void kvm_pit_set_reinject(struct kvm_pit *pit, bool reinject)
{
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;
<blue>	struct kvm *kvm = pit->kvm;</blue>

	if (atomic_read(&amp;ps-&gt;reinject) == reinject)
		return;

	/*
	 * AMD SVM AVIC accelerates EOI write and does not trap.
	 * This cause in-kernel PIT re-inject mode to fail
	 * since it checks ps-&gt;irq_ack before kvm_set_irq()
	 * and relies on the ack notifier to timely queue
	 * the pt-&gt;worker work iterm and reinject the missed tick.
	 * So, deactivate APICv when PIT is in reinject mode.
	 */
	if (reinject) {
		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
		/* The initial state is preserved while ps-&gt;reinject == 0. */
<blue>		kvm_pit_reset_reinject(pit);</blue>
<blue>		kvm_register_irq_ack_notifier(kvm, &ps->irq_ack_notifier);</blue>
		kvm_register_irq_mask_notifier(kvm, 0, &amp;pit-&gt;mask_notifier);
	} else {
<yellow>		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);</yellow>
		kvm_unregister_irq_ack_notifier(kvm, &amp;ps-&gt;irq_ack_notifier);
		kvm_unregister_irq_mask_notifier(kvm, 0, &amp;pit-&gt;mask_notifier);
	}

<blue>	atomic_set(&ps->reinject, reinject);</blue>
<blue>}</blue>

static void create_pit_timer(struct kvm_pit *pit, u32 val, int is_period)
{
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;
<blue>	struct kvm *kvm = pit->kvm;</blue>
	s64 interval;

<blue>	if (!ioapic_in_kernel(kvm) ||</blue>
<blue>	    ps->flags & KVM_PIT_FLAGS_HPET_LEGACY)</blue>
		return;

<blue>	interval = mul_u64_u32_div(val, NSEC_PER_SEC, KVM_PIT_FREQ);</blue>

<yellow>	pr_debug("create pit timer, interval is %llu nsec\n", interval);</yellow>

	/* TODO The new value only affected after the retriggered */
<blue>	hrtimer_cancel(&ps->timer);</blue>
	kthread_flush_work(&amp;pit-&gt;expired);
	ps-&gt;period = interval;
	ps-&gt;is_periodic = is_period;

	kvm_pit_reset_reinject(pit);

	/*
	 * Do not allow the guest to program periodic timers with small
	 * interval, since the hrtimers are not throttled by the host
	 * scheduler.
	 */
<blue>	if (ps->is_periodic) {</blue>
<blue>		s64 min_period = min_timer_period_us * 1000LL;</blue>

		if (ps-&gt;period &lt; min_period) {
<yellow>			pr_info_ratelimited(</yellow>
			    &quot;kvm: requested %lld ns &quot;
			    &quot;i8254 timer period limited to %lld ns\n&quot;,
			    ps-&gt;period, min_period);
<yellow>			ps->period = min_period;</yellow>
		}
	}

<blue>	hrtimer_start(&ps->timer, ktime_add_ns(ktime_get(), interval),</blue>
		      HRTIMER_MODE_ABS);
}

static void pit_load_count(struct kvm_pit *pit, int channel, u32 val)
{
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;

<blue>	pr_debug("load_count val is %u, channel is %d\n", val, channel);</blue>

	/*
	 * The largest possible initial count is 0; this is equivalent
	 * to 216 for binary counting and 104 for BCD counting.
	 */
<blue>	if (val == 0)</blue>
		val = 0x10000;

<blue>	ps->channels[channel].count = val;</blue>

	if (channel != 0) {
<blue>		ps->channels[channel].count_load_time = ktime_get();</blue>
		return;
	}

	/* Two types of timer
	 * mode 1 is one shot, mode 2 is period, otherwise del timer */
<blue>	switch (ps->channels[0].mode) {</blue>
	case 0:
	case 1:
        /* FIXME: enhance mode 4 precision */
	case 4:
<yellow>		create_pit_timer(pit, val, 0);</yellow>
		break;
	case 2:
	case 3:
<blue>		create_pit_timer(pit, val, 1);</blue>
		break;
	default:
<blue>		destroy_pit_timer(pit);</blue>
	}
<blue>}</blue>

void kvm_pit_load_count(struct kvm_pit *pit, int channel, u32 val,
		int hpet_legacy_start)
{
	u8 saved_mode;

<blue>	WARN_ON_ONCE(!mutex_is_locked(&pit->pit_state.lock));</blue>

<blue>	if (hpet_legacy_start) {</blue>
		/* save existing mode for later reenablement */
<yellow>		WARN_ON(channel != 0);</yellow>
<yellow>		saved_mode = pit->pit_state.channels[0].mode;</yellow>
		pit-&gt;pit_state.channels[0].mode = 0xff; /* disable timer */
		pit_load_count(pit, channel, val);
		pit-&gt;pit_state.channels[0].mode = saved_mode;
	} else {
<blue>		pit_load_count(pit, channel, val);</blue>
	}
<blue>}</blue>

static inline struct kvm_pit *dev_to_pit(struct kvm_io_device *dev)
{
	return container_of(dev, struct kvm_pit, dev);
}

static inline struct kvm_pit *speaker_to_pit(struct kvm_io_device *dev)
{
<yellow>	return container_of(dev, struct kvm_pit, speaker_dev);</yellow>
}

static inline int pit_in_range(gpa_t addr)
{
	return ((addr &gt;= KVM_PIT_BASE_ADDRESS) &amp;&amp;
		(addr &lt; KVM_PIT_BASE_ADDRESS + KVM_PIT_MEM_LENGTH));
}

static int pit_ioport_write(struct kvm_vcpu *vcpu,
				struct kvm_io_device *this,
			    gpa_t addr, int len, const void *data)
{
	struct kvm_pit *pit = dev_to_pit(this);
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
	int channel, access;
	struct kvm_kpit_channel_state *s;
<blue>	u32 val = *(u32 *) data;</blue>
	if (!pit_in_range(addr))
		return -EOPNOTSUPP;

<blue>	val  &= 0xff;</blue>
	addr &amp;= KVM_PIT_CHANNEL_MASK;

	mutex_lock(&amp;pit_state-&gt;lock);

	if (val != 0)
<blue>		pr_debug("write addr is 0x%x, len is %d, val is 0x%x\n",</blue>
			 (unsigned int)addr, len, val);

<blue>	if (addr == 3) {</blue>
<blue>		channel = val >> 6;</blue>
		if (channel == 3) {
			/* Read-Back Command. */
<yellow>			for (channel = 0; channel < 3; channel++) {</yellow>
<yellow>				if (val & (2 << channel)) {</yellow>
<yellow>					if (!(val & 0x20))</yellow>
<yellow>						pit_latch_count(pit, channel);</yellow>
<yellow>					if (!(val & 0x10))</yellow>
<yellow>						pit_latch_status(pit, channel);</yellow>
				}
			}
		} else {
			/* Select Counter &lt;channel&gt;. */
<blue>			s = &pit_state->channels[channel];</blue>
<blue>			access = (val >> 4) & KVM_PIT_CHANNEL_MASK;</blue>
<blue>			if (access == 0) {</blue>
<blue>				pit_latch_count(pit, channel);</blue>
			} else {
<blue>				s->rw_mode = access;</blue>
				s-&gt;read_state = access;
				s-&gt;write_state = access;
<blue>				s->mode = (val >> 1) & 7;</blue>
				if (s-&gt;mode &gt; 5)
<yellow>					s->mode -= 4;</yellow>
<blue>				s->bcd = val & 1;</blue>
			}
		}
	} else {
		/* Write Count. */
		s = &amp;pit_state-&gt;channels[addr];
<blue>		switch (s->write_state) {</blue>
		default:
		case RW_STATE_LSB:
<blue>			pit_load_count(pit, addr, val);</blue>
			break;
		case RW_STATE_MSB:
<yellow>			pit_load_count(pit, addr, val << 8);</yellow>
			break;
		case RW_STATE_WORD0:
<blue>			s->write_latch = val;</blue>
			s-&gt;write_state = RW_STATE_WORD1;
			break;
		case RW_STATE_WORD1:
<blue>			pit_load_count(pit, addr, s->write_latch | (val << 8));</blue>
			s-&gt;write_state = RW_STATE_WORD0;
			break;
		}
	}

<blue>	mutex_unlock(&pit_state->lock);</blue>
	return 0;
<blue>}</blue>

static int pit_ioport_read(struct kvm_vcpu *vcpu,
			   struct kvm_io_device *this,
			   gpa_t addr, int len, void *data)
<blue>{</blue>
	struct kvm_pit *pit = dev_to_pit(this);
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
	int ret, count;
	struct kvm_kpit_channel_state *s;
<blue>	if (!pit_in_range(addr))</blue>
		return -EOPNOTSUPP;

<blue>	addr &= KVM_PIT_CHANNEL_MASK;</blue>
	if (addr == 3)
		return 0;

	s = &amp;pit_state-&gt;channels[addr];

<blue>	mutex_lock(&pit_state->lock);</blue>

	if (s-&gt;status_latched) {
<yellow>		s->status_latched = 0;</yellow>
		ret = s-&gt;status;
<blue>	} else if (s->count_latched) {</blue>
		switch (s-&gt;count_latched) {
		default:
		case RW_STATE_LSB:
<yellow>			ret = s->latched_count & 0xff;</yellow>
			s-&gt;count_latched = 0;
			break;
		case RW_STATE_MSB:
<yellow>			ret = s->latched_count >> 8;</yellow>
			s-&gt;count_latched = 0;
			break;
		case RW_STATE_WORD0:
			ret = s-&gt;latched_count &amp; 0xff;
<yellow>			s->count_latched = RW_STATE_MSB;</yellow>
			break;
		}
	} else {
<blue>		switch (s->read_state) {</blue>
		default:
		case RW_STATE_LSB:
<yellow>			count = pit_get_count(pit, addr);</yellow>
			ret = count &amp; 0xff;
			break;
		case RW_STATE_MSB:
<yellow>			count = pit_get_count(pit, addr);</yellow>
			ret = (count &gt;&gt; 8) &amp; 0xff;
			break;
		case RW_STATE_WORD0:
<blue>			count = pit_get_count(pit, addr);</blue>
			ret = count &amp; 0xff;
			s-&gt;read_state = RW_STATE_WORD1;
			break;
		case RW_STATE_WORD1:
<blue>			count = pit_get_count(pit, addr);</blue>
			ret = (count &gt;&gt; 8) &amp; 0xff;
			s-&gt;read_state = RW_STATE_WORD0;
			break;
		}
	}

<blue>	if (len > sizeof(ret))</blue>
		len = sizeof(ret);
<blue>	memcpy(data, (char *)&ret, len);</blue>

	mutex_unlock(&amp;pit_state-&gt;lock);
	return 0;
}

static int speaker_ioport_write(struct kvm_vcpu *vcpu,
				struct kvm_io_device *this,
				gpa_t addr, int len, const void *data)
{
	struct kvm_pit *pit = speaker_to_pit(this);
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
<yellow>	u32 val = *(u32 *) data;</yellow>
	if (addr != KVM_SPEAKER_BASE_ADDRESS)
		return -EOPNOTSUPP;

<yellow>	mutex_lock(&pit_state->lock);</yellow>
	if (val &amp; (1 &lt;&lt; 1))
<yellow>		pit_state->flags |= KVM_PIT_FLAGS_SPEAKER_DATA_ON;</yellow>
	else
<yellow>		pit_state->flags &= ~KVM_PIT_FLAGS_SPEAKER_DATA_ON;</yellow>
<yellow>	pit_set_gate(pit, 2, val & 1);</yellow>
	mutex_unlock(&amp;pit_state-&gt;lock);
	return 0;
<yellow>}</yellow>

static int speaker_ioport_read(struct kvm_vcpu *vcpu,
				   struct kvm_io_device *this,
				   gpa_t addr, int len, void *data)
<yellow>{</yellow>
<yellow>	struct kvm_pit *pit = speaker_to_pit(this);</yellow>
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
	unsigned int refresh_clock;
	int ret;
	if (addr != KVM_SPEAKER_BASE_ADDRESS)
		return -EOPNOTSUPP;

	/* Refresh clock toggles at about 15us. We approximate as 2^14ns. */
	refresh_clock = ((unsigned int)ktime_to_ns(ktime_get()) &gt;&gt; 14) &amp; 1;

<yellow>	mutex_lock(&pit_state->lock);</yellow>
	ret = (!!(pit_state-&gt;flags &amp; KVM_PIT_FLAGS_SPEAKER_DATA_ON) &lt;&lt; 1) |
		pit_get_gate(pit, 2) | (pit_get_out(pit, 2) &lt;&lt; 5) |
		(refresh_clock &lt;&lt; 4);
	if (len &gt; sizeof(ret))
		len = sizeof(ret);
<yellow>	memcpy(data, (char *)&ret, len);</yellow>
	mutex_unlock(&amp;pit_state-&gt;lock);
	return 0;
}

static void kvm_pit_reset(struct kvm_pit *pit)
{
	int i;
	struct kvm_kpit_channel_state *c;

	pit-&gt;pit_state.flags = 0;
	for (i = 0; i &lt; 3; i++) {
<blue>		c = &pit->pit_state.channels[i];</blue>
		c-&gt;mode = 0xff;
		c-&gt;gate = (i != 2);
		pit_load_count(pit, i, 0);
	}

<blue>	kvm_pit_reset_reinject(pit);</blue>
}

static void pit_mask_notifer(struct kvm_irq_mask_notifier *kimn, bool mask)
{
	struct kvm_pit *pit = container_of(kimn, struct kvm_pit, mask_notifier);

<blue>	if (!mask)</blue>
<blue>		kvm_pit_reset_reinject(pit);</blue>
<blue>}</blue>

static const struct kvm_io_device_ops pit_dev_ops = {
	.read     = pit_ioport_read,
	.write    = pit_ioport_write,
};

static const struct kvm_io_device_ops speaker_dev_ops = {
	.read     = speaker_ioport_read,
	.write    = speaker_ioport_write,
};

struct kvm_pit *kvm_create_pit(struct kvm *kvm, u32 flags)
{
	struct kvm_pit *pit;
	struct kvm_kpit_state *pit_state;
	struct pid *pid;
	pid_t pid_nr;
	int ret;

<blue>	pit = kzalloc(sizeof(struct kvm_pit), GFP_KERNEL_ACCOUNT);</blue>
	if (!pit)
		return NULL;

<blue>	pit->irq_source_id = kvm_request_irq_source_id(kvm);</blue>
	if (pit-&gt;irq_source_id &lt; 0)
		goto fail_request;

<blue>	mutex_init(&pit->pit_state.lock);</blue>

<blue>	pid = get_pid(task_tgid(current));</blue>
<blue>	pid_nr = pid_vnr(pid);</blue>
	put_pid(pid);

	pit-&gt;worker = kthread_create_worker(0, &quot;kvm-pit/%d&quot;, pid_nr);
	if (IS_ERR(pit-&gt;worker))
		goto fail_kthread;

<blue>	kthread_init_work(&pit->expired, pit_do_work);</blue>

	pit-&gt;kvm = kvm;

	pit_state = &amp;pit-&gt;pit_state;
	hrtimer_init(&amp;pit_state-&gt;timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
	pit_state-&gt;timer.function = pit_timer_fn;

	pit_state-&gt;irq_ack_notifier.gsi = 0;
	pit_state-&gt;irq_ack_notifier.irq_acked = kvm_pit_ack_irq;
	pit-&gt;mask_notifier.func = pit_mask_notifer;

<blue>	kvm_pit_reset(pit);</blue>

	kvm_pit_set_reinject(pit, true);

	mutex_lock(&amp;kvm-&gt;slots_lock);
	kvm_iodevice_init(&amp;pit-&gt;dev, &amp;pit_dev_ops);
	ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
				      KVM_PIT_MEM_LENGTH, &amp;pit-&gt;dev);
	if (ret &lt; 0)
		goto fail_register_pit;

<blue>	if (flags & KVM_PIT_SPEAKER_DUMMY) {</blue>
<yellow>		kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);</yellow>
		ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
					      KVM_SPEAKER_BASE_ADDRESS, 4,
					      &amp;pit-&gt;speaker_dev);
		if (ret &lt; 0)
			goto fail_register_speaker;
	}
<blue>	mutex_unlock(&kvm->slots_lock);</blue>

	return pit;

fail_register_speaker:
<yellow>	kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &pit->dev);</yellow>
fail_register_pit:
<yellow>	mutex_unlock(&kvm->slots_lock);</yellow>
	kvm_pit_set_reinject(pit, false);
	kthread_destroy_worker(pit-&gt;worker);
fail_kthread:
<yellow>	kvm_free_irq_source_id(kvm, pit->irq_source_id);</yellow>
fail_request:
<yellow>	kfree(pit);</yellow>
	return NULL;
<blue>}</blue>

void kvm_free_pit(struct kvm *kvm)
{
<yellow>	struct kvm_pit *pit = kvm->arch.vpit;</yellow>

	if (pit) {
<yellow>		mutex_lock(&kvm->slots_lock);</yellow>
		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &amp;pit-&gt;dev);
		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &amp;pit-&gt;speaker_dev);
		mutex_unlock(&amp;kvm-&gt;slots_lock);
		kvm_pit_set_reinject(pit, false);
		hrtimer_cancel(&amp;pit-&gt;pit_state.timer);
		kthread_destroy_worker(pit-&gt;worker);
		kvm_free_irq_source_id(kvm, pit-&gt;irq_source_id);
		kfree(pit);
	}
<yellow>}</yellow>


</code></pre></td></tr></table>
</body>
</html>
