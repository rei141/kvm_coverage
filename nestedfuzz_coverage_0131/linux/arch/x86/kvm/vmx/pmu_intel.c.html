<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.
57.
58.
59.
60.
61.
62.
63.
64.
65.
66.
67.
68.
69.
70.
71.
72.
73.
74.
75.
76.
77.
78.
79.
80.
81.
82.
83.
84.
85.
86.
87.
88.
89.
90.
91.
92.
93.
94.
95.
96.
97.
98.
99.
100.
101.
102.
103.
104.
105.
106.
107.
108.
109.
110.
111.
112.
113.
114.
115.
116.
117.
118.
119.
120.
121.
122.
123.
124.
125.
126.
127.
128.
129.
130.
131.
132.
133.
134.
135.
136.
137.
138.
139.
140.
141.
142.
143.
144.
145.
146.
147.
148.
149.
150.
151.
152.
153.
154.
155.
156.
157.
158.
159.
160.
161.
162.
163.
164.
165.
166.
167.
168.
169.
170.
171.
172.
173.
174.
175.
176.
177.
178.
179.
180.
181.
182.
183.
184.
185.
186.
187.
188.
189.
190.
191.
192.
193.
194.
195.
196.
197.
198.
199.
200.
201.
202.
203.
204.
205.
206.
207.
208.
209.
210.
211.
212.
213.
214.
215.
216.
217.
218.
219.
220.
221.
222.
223.
224.
225.
226.
227.
228.
229.
230.
231.
232.
233.
234.
235.
236.
237.
238.
239.
240.
241.
242.
243.
244.
245.
246.
247.
248.
249.
250.
251.
252.
253.
254.
255.
256.
257.
258.
259.
260.
261.
262.
263.
264.
265.
266.
267.
268.
269.
270.
271.
272.
273.
274.
275.
276.
277.
278.
279.
280.
281.
282.
283.
284.
285.
286.
287.
288.
289.
290.
291.
292.
293.
294.
295.
296.
297.
298.
299.
300.
301.
302.
303.
304.
305.
306.
307.
308.
309.
310.
311.
312.
313.
314.
315.
316.
317.
318.
319.
320.
321.
322.
323.
324.
325.
326.
327.
328.
329.
330.
331.
332.
333.
334.
335.
336.
337.
338.
339.
340.
341.
342.
343.
344.
345.
346.
347.
348.
349.
350.
351.
352.
353.
354.
355.
356.
357.
358.
359.
360.
361.
362.
363.
364.
365.
366.
367.
368.
369.
370.
371.
372.
373.
374.
375.
376.
377.
378.
379.
380.
381.
382.
383.
384.
385.
386.
387.
388.
389.
390.
391.
392.
393.
394.
395.
396.
397.
398.
399.
400.
401.
402.
403.
404.
405.
406.
407.
408.
409.
410.
411.
412.
413.
414.
415.
416.
417.
418.
419.
420.
421.
422.
423.
424.
425.
426.
427.
428.
429.
430.
431.
432.
433.
434.
435.
436.
437.
438.
439.
440.
441.
442.
443.
444.
445.
446.
447.
448.
449.
450.
451.
452.
453.
454.
455.
456.
457.
458.
459.
460.
461.
462.
463.
464.
465.
466.
467.
468.
469.
470.
471.
472.
473.
474.
475.
476.
477.
478.
479.
480.
481.
482.
483.
484.
485.
486.
487.
488.
489.
490.
491.
492.
493.
494.
495.
496.
497.
498.
499.
500.
501.
502.
503.
504.
505.
506.
507.
508.
509.
510.
511.
512.
513.
514.
515.
516.
517.
518.
519.
520.
521.
522.
523.
524.
525.
526.
527.
528.
529.
530.
531.
532.
533.
534.
535.
536.
537.
538.
539.
540.
541.
542.
543.
544.
545.
546.
547.
548.
549.
550.
551.
552.
553.
554.
555.
556.
557.
558.
559.
560.
561.
562.
563.
564.
565.
566.
567.
568.
569.
570.
571.
572.
573.
574.
575.
576.
577.
578.
579.
580.
581.
582.
583.
584.
585.
586.
587.
588.
589.
590.
591.
592.
593.
594.
595.
596.
597.
598.
599.
600.
601.
602.
603.
604.
605.
606.
607.
608.
609.
610.
611.
612.
613.
614.
615.
616.
617.
618.
619.
620.
621.
622.
623.
624.
625.
626.
627.
628.
629.
630.
631.
632.
633.
634.
635.
636.
637.
638.
639.
640.
641.
642.
643.
644.
645.
646.
647.
648.
649.
650.
651.
652.
653.
654.
655.
656.
657.
658.
659.
660.
661.
662.
663.
664.
665.
666.
667.
668.
669.
670.
671.
672.
673.
674.
675.
676.
677.
678.
679.
680.
681.
682.
683.
684.
685.
686.
687.
688.
689.
690.
691.
692.
693.
694.
695.
696.
697.
698.
699.
700.
701.
702.
703.
704.
705.
706.
707.
708.
709.
710.
711.
712.
713.
714.
715.
716.
717.
718.
719.
720.
721.
722.
723.
724.
725.
726.
727.
728.
729.
730.
731.
732.
733.
734.
735.
736.
737.
738.
739.
740.
741.
742.
743.
744.
745.
746.
747.
748.
749.
750.
751.
752.
753.
754.
755.
756.
757.
758.
759.
760.
761.
762.
763.
764.
765.
766.
767.
768.
769.
770.
771.
772.
773.
774.
775.
776.
777.
778.
779.
780.
781.
782.
783.
784.
785.
786.
787.
788.
789.
790.
791.
792.
793.
794.
795.
796.
797.
798.
799.
800.
801.
802.
803.
804.
805.
806.
807.
808.
809.
810.
811.
812.
813.
814.
815.
</code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * KVM PMU support for Intel CPUs
 *
 * Copyright 2011 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@redhat.com&gt;
 *   Gleb Natapov &lt;gleb@redhat.com&gt;
 */
#include &lt;linux/types.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/perf_event.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &quot;x86.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;lapic.h&quot;
#include &quot;nested.h&quot;
#include &quot;pmu.h&quot;

#define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)

static struct kvm_event_hw_type_mapping intel_arch_events[] = {
	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
	[2] = { 0x3c, 0x01, PERF_COUNT_HW_BUS_CYCLES  },
	[3] = { 0x2e, 0x4f, PERF_COUNT_HW_CACHE_REFERENCES },
	[4] = { 0x2e, 0x41, PERF_COUNT_HW_CACHE_MISSES },
	[5] = { 0xc4, 0x00, PERF_COUNT_HW_BRANCH_INSTRUCTIONS },
	[6] = { 0xc5, 0x00, PERF_COUNT_HW_BRANCH_MISSES },
	/* The above index must match CPUID 0x0A.EBX bit vector */
	[7] = { 0x00, 0x03, PERF_COUNT_HW_REF_CPU_CYCLES },
};

/* mapping between fixed pmc index and intel_arch_events array */
static int fixed_pmc_events[] = {1, 0, 7};

static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
{
	struct kvm_pmc *pmc;
	u8 old_fixed_ctr_ctrl = pmu-&gt;fixed_ctr_ctrl;
	int i;

	pmu-&gt;fixed_ctr_ctrl = data;
<yellow>	for (i = 0; i < pmu->nr_arch_fixed_counters; i++) {</yellow>
<yellow>		u8 new_ctrl = fixed_ctrl_field(data, i);</yellow>
<yellow>		u8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);</yellow>

		if (old_ctrl == new_ctrl)
			continue;

<yellow>		pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);</yellow>

<yellow>		__set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);</yellow>
		reprogram_counter(pmc);
	}
}

<blue>static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)</blue>
{
<blue>	if (pmc_idx < INTEL_PMC_IDX_FIXED) {</blue>
<blue>		return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,</blue>
				  MSR_P6_EVNTSEL0);
	} else {
		u32 idx = pmc_idx - INTEL_PMC_IDX_FIXED;

<blue>		return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);</blue>
	}
<blue>}</blue>

static void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
{
	int bit;
	struct kvm_pmc *pmc;

<blue>	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX) {</blue>
<blue>		pmc = intel_pmc_idx_to_pmc(pmu, bit);</blue>
<blue>		if (pmc)</blue>
<blue>			reprogram_counter(pmc);</blue>
	}
<blue>}</blue>

static bool intel_hw_event_available(struct kvm_pmc *pmc)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	u8 event_select = pmc-&gt;eventsel &amp; ARCH_PERFMON_EVENTSEL_EVENT;
	u8 unit_mask = (pmc-&gt;eventsel &amp; ARCH_PERFMON_EVENTSEL_UMASK) &gt;&gt; 8;
	int i;

<yellow>	for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {</yellow>
<yellow>		if (intel_arch_events[i].eventsel != event_select ||</yellow>
<yellow>		    intel_arch_events[i].unit_mask != unit_mask)</yellow>
			continue;

		/* disable event that reported as not present by cpuid */
<yellow>		if ((i < 7) && !(pmu->available_event_types & (1 << i)))</yellow>
			return false;

		break;
	}

	return true;
<yellow>}</yellow>

/* check if a PMC is enabled by comparing it with globl_ctrl bits. */
static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
{
<blue>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</blue>

	if (!intel_pmu_has_perf_global_ctrl(pmu))
		return true;

<blue>	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);</blue>
<blue>}</blue>

static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	bool fixed = idx &amp; (1u &lt;&lt; 30);

	idx &amp;= ~(3u &lt;&lt; 30);

<yellow>	return fixed ? idx < pmu->nr_arch_fixed_counters</yellow>
<yellow>		     : idx < pmu->nr_arch_gp_counters;</yellow>
<yellow>}</yellow>

static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
					    unsigned int idx, u64 *mask)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
<blue>	bool fixed = idx & (1u << 30);</blue>
	struct kvm_pmc *counters;
	unsigned int num_counters;

	idx &amp;= ~(3u &lt;&lt; 30);
	if (fixed) {
<yellow>		counters = pmu->fixed_counters;</yellow>
		num_counters = pmu-&gt;nr_arch_fixed_counters;
	} else {
<blue>		counters = pmu->gp_counters;</blue>
		num_counters = pmu-&gt;nr_arch_gp_counters;
	}
<blue>	if (idx >= num_counters)</blue>
		return NULL;
<yellow>	*mask &= pmu->counter_bitmask[fixed ? KVM_PMC_FIXED : KVM_PMC_GP];</yellow>
	return &amp;counters[array_index_nospec(idx, num_counters)];
<blue>}</blue>

static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
{
<blue>	if (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))</blue>
		return 0;

<blue>	return vcpu->arch.perf_capabilities;</blue>
}

static inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)
{
<blue>	return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;</blue>
}

static inline struct kvm_pmc *get_fw_gp_pmc(struct kvm_pmu *pmu, u32 msr)
{
<blue>	if (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))</blue>
		return NULL;

<blue>	return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);</blue>
}

static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
{
	struct x86_pmu_lbr *records = vcpu_to_lbr_records(vcpu);
	bool ret = false;

<blue>	if (!intel_pmu_lbr_is_enabled(vcpu))</blue>
		return ret;

	ret = (index == MSR_LBR_SELECT) || (index == MSR_LBR_TOS) ||
<yellow>		(index >= records->from && index < records->from + records->nr) ||</yellow>
<yellow>		(index >= records->to && index < records->to + records->nr);</yellow>

<yellow>	if (!ret && records->info)</yellow>
<yellow>		ret = (index >= records->info && index < records->info + records->nr);</yellow>

	return ret;
<blue>}</blue>

static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	u64 perf_capabilities;
	int ret;

<blue>	switch (msr) {</blue>
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
	case MSR_CORE_PERF_GLOBAL_STATUS:
	case MSR_CORE_PERF_GLOBAL_CTRL:
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		return intel_pmu_has_perf_global_ctrl(pmu);</blue>
		break;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		ret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;</yellow>
		break;
	case MSR_IA32_DS_AREA:
<yellow>		ret = guest_cpuid_has(vcpu, X86_FEATURE_DS);</yellow>
		break;
	case MSR_PEBS_DATA_CFG:
<yellow>		perf_capabilities = vcpu_get_perf_capabilities(vcpu);</yellow>
		ret = (perf_capabilities &amp; PERF_CAP_PEBS_BASELINE) &amp;&amp;
<yellow>			((perf_capabilities & PERF_CAP_PEBS_FORMAT) > 3);</yellow>
		break;
	default:
<blue>		ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||</blue>
<blue>			get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||</blue>
<blue>			get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||</blue>
<blue>			intel_pmu_is_valid_lbr_msr(vcpu, msr);</blue>
		break;
	}

	return ret;
<blue>}</blue>

<blue>static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)</blue>
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;

<blue>	pmc = get_fixed_pmc(pmu, msr);</blue>
<blue>	pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);</blue>
<blue>	pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);</blue>

	return pmc;
<blue>}</blue>

static inline void intel_pmu_release_guest_lbr_event(struct kvm_vcpu *vcpu)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (lbr_desc->event) {</yellow>
<yellow>		perf_event_release_kernel(lbr_desc->event);</yellow>
		lbr_desc-&gt;event = NULL;
		vcpu_to_pmu(vcpu)-&gt;event_count--;
	}
}

int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)
<yellow>{</yellow>
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct perf_event *event;

	/*
	 * The perf_event_attr is constructed in the minimum efficient way:
	 * - set &#x27;pinned = true&#x27; to make it task pinned so that if another
	 *   cpu pinned event reclaims LBR, the event-&gt;oncpu will be set to -1;
	 * - set &#x27;.exclude_host = true&#x27; to record guest branches behavior;
	 *
	 * - set &#x27;.config = INTEL_FIXED_VLBR_EVENT&#x27; to indicates host perf
	 *   schedule the event without a real HW counter but a fake one;
	 *   check is_guest_lbr_event() and __intel_get_event_constraints();
	 *
	 * - set &#x27;sample_type = PERF_SAMPLE_BRANCH_STACK&#x27; and
	 *   &#x27;branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |
	 *   PERF_SAMPLE_BRANCH_USER&#x27; to configure it as a LBR callstack
	 *   event, which helps KVM to save/restore guest LBR records
	 *   during host context switches and reduces quite a lot overhead,
	 *   check branch_user_callstack() and intel_pmu_lbr_sched_task();
	 */
<yellow>	struct perf_event_attr attr = {</yellow>
		.type = PERF_TYPE_RAW,
		.size = sizeof(attr),
		.config = INTEL_FIXED_VLBR_EVENT,
		.sample_type = PERF_SAMPLE_BRANCH_STACK,
		.pinned = true,
		.exclude_host = true,
		.branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |
					PERF_SAMPLE_BRANCH_USER,
	};

	if (unlikely(lbr_desc-&gt;event)) {
<yellow>		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);</yellow>
		return 0;
	}

<yellow>	event = perf_event_create_kernel_counter(&attr, -1,</yellow>
						current, NULL, NULL);
	if (IS_ERR(event)) {
<yellow>		pr_debug_ratelimited("%s: failed %ld\n",</yellow>
					__func__, PTR_ERR(event));
<yellow>		return PTR_ERR(event);</yellow>
	}
<yellow>	lbr_desc->event = event;</yellow>
	pmu-&gt;event_count++;
	__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu-&gt;pmc_in_use);
	return 0;
}

/*
 * It&#x27;s safe to access LBR msrs from guest when they have not
 * been passthrough since the host would help restore or reset
 * the LBR msrs records when the guest LBR event is scheduled in.
 */
static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
				     struct msr_data *msr_info, bool read)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
<yellow>	u32 index = msr_info->index;</yellow>

	if (!intel_pmu_is_valid_lbr_msr(vcpu, index))
		return false;

<yellow>	if (!lbr_desc->event && intel_pmu_create_guest_lbr_event(vcpu) < 0)</yellow>
		goto dummy;

	/*
	 * Disable irq to ensure the LBR feature doesn&#x27;t get reclaimed by the
	 * host at the time the value is read from the msr, and this avoids the
	 * host LBR value to be leaked to the guest. If LBR has been reclaimed,
	 * return 0 on guest reads.
	 */
<yellow>	local_irq_disable();</yellow>
	if (lbr_desc-&gt;event-&gt;state == PERF_EVENT_STATE_ACTIVE) {
<yellow>		if (read)</yellow>
<yellow>			rdmsrl(index, msr_info->data);</yellow>
		else
<yellow>			wrmsrl(index, msr_info->data);</yellow>
<yellow>		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);</yellow>
		local_irq_enable();
		return true;
	}
<yellow>	clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);</yellow>
	local_irq_enable();

dummy:
<yellow>	if (read)</yellow>
<yellow>		msr_info->data = 0;</yellow>
	return true;
<yellow>}</yellow>

static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;
<blue>	u32 msr = msr_info->index;</blue>

	switch (msr) {
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
<blue>		msr_info->data = pmu->fixed_ctr_ctrl;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_STATUS:
<blue>		msr_info->data = pmu->global_status;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		msr_info->data = pmu->global_ctrl;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		msr_info->data = 0;</blue>
		return 0;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		msr_info->data = pmu->pebs_enable;</yellow>
		return 0;
	case MSR_IA32_DS_AREA:
<yellow>		msr_info->data = pmu->ds_area;</yellow>
		return 0;
	case MSR_PEBS_DATA_CFG:
<yellow>		msr_info->data = pmu->pebs_data_cfg;</yellow>
		return 0;
	default:
<blue>		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||</blue>
<blue>		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {</blue>
<blue>			u64 val = pmc_read_counter(pmc);</blue>
			msr_info-&gt;data =
				val &amp; pmu-&gt;counter_bitmask[KVM_PMC_GP];
			return 0;
<blue>		} else if ((pmc = get_fixed_pmc(pmu, msr))) {</blue>
<blue>			u64 val = pmc_read_counter(pmc);</blue>
			msr_info-&gt;data =
				val &amp; pmu-&gt;counter_bitmask[KVM_PMC_FIXED];
			return 0;
<blue>		} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {</blue>
<blue>			msr_info->data = pmc->eventsel;</blue>
			return 0;
<yellow>		} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, true))</yellow>
			return 0;
	}

	return 1;
}

static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
<blue>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</blue>
	struct kvm_pmc *pmc;
	u32 msr = msr_info-&gt;index;
	u64 data = msr_info-&gt;data;
	u64 reserved_bits, diff;

	switch (msr) {
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
<blue>		if (pmu->fixed_ctr_ctrl == data)</blue>
			return 0;
<yellow>		if (!(data & pmu->fixed_ctr_ctrl_mask)) {</yellow>
<yellow>			reprogram_fixed_counters(pmu, data);</yellow>
			return 0;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_STATUS:
<blue>		if (msr_info->host_initiated) {</blue>
<blue>			pmu->global_status = data;</blue>
			return 0;
		}
		break; /* RO MSR */
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		if (pmu->global_ctrl == data)</blue>
			return 0;
<blue>		if (kvm_valid_perf_global_ctrl(pmu, data)) {</blue>
			diff = pmu-&gt;global_ctrl ^ data;
<blue>			pmu->global_ctrl = data;</blue>
			reprogram_counters(pmu, diff);
			return 0;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		if (!(data & pmu->global_ovf_ctrl_mask)) {</blue>
<blue>			if (!msr_info->host_initiated)</blue>
<yellow>				pmu->global_status &= ~data;</yellow>
			return 0;
		}
		break;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		if (pmu->pebs_enable == data)</yellow>
			return 0;
<yellow>		if (!(data & pmu->pebs_enable_mask)) {</yellow>
			diff = pmu-&gt;pebs_enable ^ data;
<yellow>			pmu->pebs_enable = data;</yellow>
			reprogram_counters(pmu, diff);
			return 0;
		}
		break;
	case MSR_IA32_DS_AREA:
<yellow>		if (msr_info->host_initiated && data && !guest_cpuid_has(vcpu, X86_FEATURE_DS))</yellow>
			return 1;
<yellow>		if (is_noncanonical_address(data, vcpu))</yellow>
			return 1;
<yellow>		pmu->ds_area = data;</yellow>
		return 0;
	case MSR_PEBS_DATA_CFG:
<yellow>		if (pmu->pebs_data_cfg == data)</yellow>
			return 0;
<yellow>		if (!(data & pmu->pebs_data_cfg_mask)) {</yellow>
			pmu-&gt;pebs_data_cfg = data;
<yellow>			return 0;</yellow>
		}
		break;
	default:
<blue>		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||</blue>
<blue>		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {</blue>
<blue>			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&</blue>
<yellow>			    (data & ~pmu->counter_bitmask[KVM_PMC_GP]))</yellow>
				return 1;
<blue>			if (!msr_info->host_initiated &&</blue>
			    !(msr &amp; MSR_PMC_FULL_WIDTH_BIT))
<yellow>				data = (s64)(s32)data;</yellow>
<blue>			pmc->counter += data - pmc_read_counter(pmc);</blue>
<yellow>			pmc_update_sample_period(pmc);</yellow>
			return 0;
<blue>		} else if ((pmc = get_fixed_pmc(pmu, msr))) {</blue>
<blue>			pmc->counter += data - pmc_read_counter(pmc);</blue>
<yellow>			pmc_update_sample_period(pmc);</yellow>
			return 0;
<blue>		} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {</blue>
<blue>			if (data == pmc->eventsel)</blue>
				return 0;
<yellow>			reserved_bits = pmu->reserved_bits;</yellow>
			if ((pmc-&gt;idx == 2) &amp;&amp;
<yellow>			    (pmu->raw_event_mask & HSW_IN_TX_CHECKPOINTED))</yellow>
<yellow>				reserved_bits ^= HSW_IN_TX_CHECKPOINTED;</yellow>
<yellow>			if (!(data & reserved_bits)) {</yellow>
<yellow>				pmc->eventsel = data;</yellow>
				reprogram_counter(pmc);
				return 0;
			}
<yellow>		} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, false))</yellow>
			return 0;
	}

	return 1;
}

static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
{
	size_t size = ARRAY_SIZE(fixed_pmc_events);
	struct kvm_pmc *pmc;
	u32 event;
	int i;

	for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++) {
<blue>		pmc = &pmu->fixed_counters[i];</blue>
		event = fixed_pmc_events[array_index_nospec(i, size)];
		pmc-&gt;eventsel = (intel_arch_events[event].unit_mask &lt;&lt; 8) |
			intel_arch_events[event].eventsel;
	}
}

static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
	struct kvm_cpuid_entry2 *entry;
	union cpuid10_eax eax;
	union cpuid10_edx edx;
	u64 perf_capabilities;
	u64 counter_mask;
	int i;

<blue>	pmu->nr_arch_gp_counters = 0;</blue>
	pmu-&gt;nr_arch_fixed_counters = 0;
	pmu-&gt;counter_bitmask[KVM_PMC_GP] = 0;
	pmu-&gt;counter_bitmask[KVM_PMC_FIXED] = 0;
	pmu-&gt;version = 0;
	pmu-&gt;reserved_bits = 0xffffffff00200000ull;
	pmu-&gt;raw_event_mask = X86_RAW_EVENT_MASK;
	pmu-&gt;global_ctrl_mask = ~0ull;
	pmu-&gt;global_ovf_ctrl_mask = ~0ull;
	pmu-&gt;fixed_ctr_ctrl_mask = ~0ull;
	pmu-&gt;pebs_enable_mask = ~0ull;
	pmu-&gt;pebs_data_cfg_mask = ~0ull;

	entry = kvm_find_cpuid_entry(vcpu, 0xa);
<blue>	if (!entry || !vcpu->kvm->arch.enable_pmu)</blue>
		return;
<blue>	eax.full = entry->eax;</blue>
	edx.full = entry-&gt;edx;

	pmu-&gt;version = eax.split.version_id;
	if (!pmu-&gt;version)
		return;

<blue>	pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters,</blue>
					 kvm_pmu_cap.num_counters_gp);
	eax.split.bit_width = min_t(int, eax.split.bit_width,
				    kvm_pmu_cap.bit_width_gp);
<blue>	pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;</blue>
	eax.split.mask_length = min_t(int, eax.split.mask_length,
				      kvm_pmu_cap.events_mask_len);
<blue>	pmu->available_event_types = ~entry->ebx &</blue>
					((1ull &lt;&lt; eax.split.mask_length) - 1);

	if (pmu-&gt;version == 1) {
<yellow>		pmu->nr_arch_fixed_counters = 0;</yellow>
	} else {
		pmu-&gt;nr_arch_fixed_counters =
<blue>			min3(ARRAY_SIZE(fixed_pmc_events),</blue>
			     (size_t) edx.split.num_counters_fixed,
			     (size_t)kvm_pmu_cap.num_counters_fixed);
		edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed,
						  kvm_pmu_cap.bit_width_fixed);
		pmu-&gt;counter_bitmask[KVM_PMC_FIXED] =
<blue>			((u64)1 << edx.split.bit_width_fixed) - 1;</blue>
<blue>		setup_fixed_pmc_eventsel(pmu);</blue>
	}

	for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++)
<blue>		pmu->fixed_ctr_ctrl_mask &= ~(0xbull << (i * 4));</blue>
<blue>	counter_mask = ~(((1ull << pmu->nr_arch_gp_counters) - 1) |</blue>
<blue>		(((1ull << pmu->nr_arch_fixed_counters) - 1) << INTEL_PMC_IDX_FIXED));</blue>
	pmu-&gt;global_ctrl_mask = counter_mask;
	pmu-&gt;global_ovf_ctrl_mask = pmu-&gt;global_ctrl_mask
			&amp; ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF |
			    MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
	if (vmx_pt_mode_is_host_guest())
<yellow>		pmu->global_ovf_ctrl_mask &=</yellow>
				~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;

<blue>	entry = kvm_find_cpuid_entry_index(vcpu, 7, 0);</blue>
	if (entry &amp;&amp;
<blue>	    (boot_cpu_has(X86_FEATURE_HLE) || boot_cpu_has(X86_FEATURE_RTM)) &&</blue>
<yellow>	    (entry->ebx & (X86_FEATURE_HLE|X86_FEATURE_RTM))) {</yellow>
<yellow>		pmu->reserved_bits ^= HSW_IN_TX;</yellow>
		pmu-&gt;raw_event_mask |= (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED);
	}

<blue>	bitmap_set(pmu->all_valid_pmc_idx,</blue>
		0, pmu-&gt;nr_arch_gp_counters);
	bitmap_set(pmu-&gt;all_valid_pmc_idx,
		INTEL_PMC_MAX_GENERIC, pmu-&gt;nr_arch_fixed_counters);

<blue>	perf_capabilities = vcpu_get_perf_capabilities(vcpu);</blue>
<blue>	if (cpuid_model_is_consistent(vcpu) &&</blue>
<blue>	    (perf_capabilities & PMU_CAP_LBR_FMT))</blue>
<blue>		x86_perf_get_lbr(&lbr_desc->records);</blue>
	else
<blue>		lbr_desc->records.nr = 0;</blue>

	if (lbr_desc-&gt;records.nr)
<blue>		bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);</blue>

<blue>	if (perf_capabilities & PERF_CAP_PEBS_FORMAT) {</blue>
<yellow>		if (perf_capabilities & PERF_CAP_PEBS_BASELINE) {</yellow>
<yellow>			pmu->pebs_enable_mask = counter_mask;</yellow>
			pmu-&gt;reserved_bits &amp;= ~ICL_EVENTSEL_ADAPTIVE;
			for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++) {
<yellow>				pmu->fixed_ctr_ctrl_mask &=</yellow>
<yellow>					~(1ULL << (INTEL_PMC_IDX_FIXED + i * 4));</yellow>
			}
<yellow>			pmu->pebs_data_cfg_mask = ~0xff00000full;</yellow>
		} else {
			pmu-&gt;pebs_enable_mask =
<yellow>				~((1ull << pmu->nr_arch_gp_counters) - 1);</yellow>
		}
	}
<blue>}</blue>

static void intel_pmu_init(struct kvm_vcpu *vcpu)
{
	int i;
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

	for (i = 0; i &lt; KVM_INTEL_PMC_MAX_GENERIC; i++) {
<blue>		pmu->gp_counters[i].type = KVM_PMC_GP;</blue>
		pmu-&gt;gp_counters[i].vcpu = vcpu;
		pmu-&gt;gp_counters[i].idx = i;
		pmu-&gt;gp_counters[i].current_config = 0;
	}

	for (i = 0; i &lt; KVM_PMC_MAX_FIXED; i++) {
<blue>		pmu->fixed_counters[i].type = KVM_PMC_FIXED;</blue>
		pmu-&gt;fixed_counters[i].vcpu = vcpu;
		pmu-&gt;fixed_counters[i].idx = i + INTEL_PMC_IDX_FIXED;
		pmu-&gt;fixed_counters[i].current_config = 0;
	}

<blue>	vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();</blue>
	lbr_desc-&gt;records.nr = 0;
	lbr_desc-&gt;event = NULL;
	lbr_desc-&gt;msr_passthrough = false;
}

static void intel_pmu_reset(struct kvm_vcpu *vcpu)
<blue>{</blue>
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc = NULL;
	int i;

	for (i = 0; i &lt; KVM_INTEL_PMC_MAX_GENERIC; i++) {
<blue>		pmc = &pmu->gp_counters[i];</blue>

<yellow>		pmc_stop_counter(pmc);</yellow>
<blue>		pmc->counter = pmc->eventsel = 0;</blue>
	}

	for (i = 0; i &lt; KVM_PMC_MAX_FIXED; i++) {
<blue>		pmc = &pmu->fixed_counters[i];</blue>

<yellow>		pmc_stop_counter(pmc);</yellow>
<blue>		pmc->counter = 0;</blue>
	}

<blue>	pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;</blue>

<yellow>	intel_pmu_release_guest_lbr_event(vcpu);</yellow>
}

/*
 * Emulate LBR_On_PMI behavior for 1 &lt; pmu.version &lt; 4.
 *
 * If Freeze_LBR_On_PMI = 1, the LBR is frozen on PMI and
 * the KVM emulates to clear the LBR bit (bit 0) in IA32_DEBUGCTL.
 *
 * Guest needs to re-enable LBR to resume branches recording.
 */
static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
{
<yellow>	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);</yellow>

<yellow>	if (data & DEBUGCTLMSR_FREEZE_LBRS_ON_PMI) {</yellow>
<yellow>		data &= ~DEBUGCTLMSR_LBR;</yellow>
<yellow>		vmcs_write64(GUEST_IA32_DEBUGCTL, data);</yellow>
	}
}

static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
{
<yellow>	u8 version = vcpu_to_pmu(vcpu)->version;</yellow>

	if (!intel_pmu_lbr_is_enabled(vcpu))
		return;

<yellow>	if (version > 1 && version < 4)</yellow>
<yellow>		intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);</yellow>
<yellow>}</yellow>

static void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)
{
	struct x86_pmu_lbr *lbr = vcpu_to_lbr_records(vcpu);
	int i;

<yellow>	for (i = 0; i < lbr->nr; i++) {</yellow>
<yellow>		vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);</yellow>
		vmx_set_intercept_for_msr(vcpu, lbr-&gt;to + i, MSR_TYPE_RW, set);
		if (lbr-&gt;info)
<yellow>			vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);</yellow>
	}

<yellow>	vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);</yellow>
	vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
<yellow>}</yellow>

<yellow>static inline void vmx_disable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)</yellow>
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (!lbr_desc->msr_passthrough)</yellow>
		return;

<yellow>	vmx_update_intercept_for_lbr_msrs(vcpu, true);</yellow>
	lbr_desc-&gt;msr_passthrough = false;
}

static inline void vmx_enable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (lbr_desc->msr_passthrough)</yellow>
		return;

<yellow>	vmx_update_intercept_for_lbr_msrs(vcpu, false);</yellow>
	lbr_desc-&gt;msr_passthrough = true;
}

/*
 * Higher priority host perf events (e.g. cpu pinned) could reclaim the
 * pmu resources (e.g. LBR) that were assigned to the guest. This is
 * usually done via ipi calls (more details in perf_install_in_context).
 *
 * Before entering the non-root mode (with irq disabled here), double
 * confirm that the pmu features enabled to the guest are not reclaimed
 * by higher priority host events. Otherwise, disallow vcpu&#x27;s access to
 * the reclaimed features.
 */
void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (!lbr_desc->event) {</yellow>
<yellow>		vmx_disable_lbr_msrs_passthrough(vcpu);</yellow>
<yellow>		if (vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR)</yellow>
			goto warn;
<yellow>		if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))</yellow>
			goto warn;
		return;
	}

<yellow>	if (lbr_desc->event->state < PERF_EVENT_STATE_ACTIVE) {</yellow>
<yellow>		vmx_disable_lbr_msrs_passthrough(vcpu);</yellow>
<yellow>		__clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);</yellow>
		goto warn;
	} else
<yellow>		vmx_enable_lbr_msrs_passthrough(vcpu);</yellow>

	return;

warn:
<yellow>	pr_warn_ratelimited("kvm: vcpu-%d: fail to passthrough LBR.\n",</yellow>
		vcpu-&gt;vcpu_id);
<yellow>}</yellow>

static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
{
<yellow>	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))</yellow>
<yellow>		intel_pmu_release_guest_lbr_event(vcpu);</yellow>
<yellow>}</yellow>

void intel_pmu_cross_mapped_check(struct kvm_pmu *pmu)
{
	struct kvm_pmc *pmc = NULL;
	int bit, hw_idx;

<yellow>	for_each_set_bit(bit, (unsigned long *)&pmu->global_ctrl,</yellow>
			 X86_PMC_IDX_MAX) {
<yellow>		pmc = intel_pmc_idx_to_pmc(pmu, bit);</yellow>

<yellow>		if (!pmc || !pmc_speculative_in_use(pmc) ||</yellow>
<yellow>		    !intel_pmc_is_enabled(pmc) || !pmc->perf_event)</yellow>
			continue;

		/*
		 * A negative index indicates the event isn&#x27;t mapped to a
		 * physical counter in the host, e.g. due to contention.
		 */
<yellow>		hw_idx = pmc->perf_event->hw.idx;</yellow>
<yellow>		if (hw_idx != pmc->idx && hw_idx > -1)</yellow>
<yellow>			pmu->host_cross_mapped_mask |= BIT_ULL(hw_idx);</yellow>
	}
<yellow>}</yellow>

struct kvm_pmu_ops intel_pmu_ops __initdata = {
	.hw_event_available = intel_hw_event_available,
	.pmc_is_enabled = intel_pmc_is_enabled,
	.pmc_idx_to_pmc = intel_pmc_idx_to_pmc,
	.rdpmc_ecx_to_pmc = intel_rdpmc_ecx_to_pmc,
	.msr_idx_to_pmc = intel_msr_idx_to_pmc,
	.is_valid_rdpmc_ecx = intel_is_valid_rdpmc_ecx,
	.is_valid_msr = intel_is_valid_msr,
	.get_msr = intel_pmu_get_msr,
	.set_msr = intel_pmu_set_msr,
	.refresh = intel_pmu_refresh,
	.init = intel_pmu_init,
	.reset = intel_pmu_reset,
	.deliver_pmi = intel_pmu_deliver_pmi,
	.cleanup = intel_pmu_cleanup,
};


</code></pre></td></tr></table>
</body>
</html>
