<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.
57.
58.
59.
60.
61.
62.
63.
64.
65.
66.
67.
68.
69.
70.
71.
72.
73.
74.
75.
76.
77.
78.
79.
80.
81.
82.
83.
84.
85.
86.
87.
88.
89.
90.
91.
92.
93.
94.
95.
96.
97.
98.
99.
100.
101.
102.
103.
104.
105.
106.
107.
108.
109.
110.
111.
112.
113.
114.
115.
116.
117.
118.
119.
120.
121.
122.
123.
124.
125.
126.
127.
128.
129.
130.
131.
132.
133.
134.
135.
136.
137.
138.
139.
140.
141.
142.
143.
144.
145.
146.
147.
148.
149.
150.
151.
152.
153.
154.
155.
156.
157.
158.
159.
160.
161.
162.
163.
164.
165.
166.
167.
168.
169.
170.
171.
172.
173.
174.
175.
176.
177.
178.
179.
180.
181.
182.
183.
184.
185.
186.
187.
188.
189.
190.
191.
192.
193.
194.
195.
196.
197.
198.
199.
200.
201.
202.
203.
204.
205.
206.
207.
208.
209.
210.
211.
212.
213.
214.
215.
216.
217.
218.
219.
220.
221.
222.
223.
224.
225.
226.
227.
228.
229.
230.
231.
232.
233.
234.
235.
236.
237.
238.
239.
240.
241.
242.
243.
244.
245.
246.
247.
248.
249.
250.
251.
252.
253.
254.
255.
256.
257.
258.
259.
260.
261.
262.
263.
264.
265.
266.
267.
268.
269.
270.
271.
272.
273.
274.
275.
276.
277.
278.
279.
280.
281.
282.
283.
284.
285.
286.
287.
288.
289.
290.
291.
292.
293.
294.
295.
296.
297.
298.
299.
300.
301.
302.
303.
304.
305.
306.
307.
308.
309.
310.
311.
312.
313.
314.
315.
316.
317.
318.
319.
320.
321.
322.
323.
324.
325.
326.
327.
328.
329.
330.
331.
332.
333.
334.
335.
336.
337.
338.
339.
340.
341.
342.
343.
344.
345.
346.
347.
348.
349.
350.
351.
352.
353.
354.
355.
356.
357.
358.
359.
360.
361.
362.
363.
364.
365.
366.
367.
368.
369.
370.
371.
372.
373.
374.
375.
376.
377.
378.
379.
380.
381.
382.
383.
384.
385.
386.
387.
388.
389.
390.
391.
392.
393.
394.
395.
396.
397.
398.
399.
400.
401.
402.
403.
404.
405.
406.
407.
408.
409.
410.
411.
412.
413.
414.
415.
416.
417.
418.
419.
420.
421.
422.
423.
424.
425.
426.
427.
428.
429.
430.
431.
432.
433.
434.
435.
436.
437.
438.
439.
440.
441.
442.
443.
444.
445.
446.
447.
448.
449.
450.
451.
452.
453.
454.
455.
456.
457.
458.
459.
460.
461.
462.
463.
464.
465.
466.
467.
468.
469.
470.
471.
472.
473.
474.
475.
476.
477.
478.
479.
480.
481.
482.
483.
484.
485.
486.
487.
488.
489.
490.
491.
492.
493.
494.
495.
496.
497.
498.
499.
500.
501.
502.
503.
504.
505.
506.
507.
508.
509.
510.
511.
512.
513.
514.
515.
516.
517.
518.
519.
520.
521.
522.
523.
524.
525.
526.
527.
528.
529.
530.
531.
532.
533.
534.
535.
536.
537.
538.
539.
540.
541.
542.
543.
544.
545.
546.
547.
548.
549.
550.
551.
552.
553.
554.
555.
556.
557.
558.
559.
560.
561.
562.
563.
564.
565.
566.
567.
568.
569.
570.
571.
572.
573.
574.
575.
576.
577.
578.
579.
580.
581.
582.
583.
584.
585.
586.
587.
588.
589.
590.
591.
592.
593.
594.
595.
596.
597.
598.
599.
600.
601.
602.
603.
604.
605.
606.
607.
608.
609.
610.
611.
612.
613.
614.
615.
616.
617.
618.
619.
620.
621.
622.
623.
624.
</code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * Kernel-based Virtual Machine -- Performance Monitoring Unit support
 *
 * Copyright 2015 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@redhat.com&gt;
 *   Gleb Natapov &lt;gleb@redhat.com&gt;
 *   Wei Huang    &lt;wei@redhat.com&gt;
 */

#include &lt;linux/types.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/perf_event.h&gt;
#include &lt;linux/bsearch.h&gt;
#include &lt;linux/sort.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &lt;asm/cpu_device_id.h&gt;
#include &quot;x86.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;lapic.h&quot;
#include &quot;pmu.h&quot;

/* This is enough to filter the vast majority of currently defined events. */
#define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300

struct x86_pmu_capability __read_mostly kvm_pmu_cap;
EXPORT_SYMBOL_GPL(kvm_pmu_cap);

static const struct x86_cpu_id vmx_icl_pebs_cpu[] = {
	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D, NULL),
	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X, NULL),
	{}
};

/* NOTE:
 * - Each perf counter is defined as &quot;struct kvm_pmc&quot;;
 * - There are two types of perf counters: general purpose (gp) and fixed.
 *   gp counters are stored in gp_counters[] and fixed counters are stored
 *   in fixed_counters[] respectively. Both of them are part of &quot;struct
 *   kvm_pmu&quot;;
 * - pmu.c understands the difference between gp counters and fixed counters.
 *   However AMD doesn&#x27;t support fixed-counters;
 * - There are three types of index to access perf counters (PMC):
 *     1. MSR (named msr): For example Intel has MSR_IA32_PERFCTRn and AMD
 *        has MSR_K7_PERFCTRn and, for families 15H and later,
 *        MSR_F15H_PERF_CTRn, where MSR_F15H_PERF_CTR[0-3] are
 *        aliased to MSR_K7_PERFCTRn.
 *     2. MSR Index (named idx): This normally is used by RDPMC instruction.
 *        For instance AMD RDPMC instruction uses 0000_0003h in ECX to access
 *        C001_0007h (MSR_K7_PERCTR3). Intel has a similar mechanism, except
 *        that it also supports fixed counters. idx can be used to as index to
 *        gp and fixed counters.
 *     3. Global PMC Index (named pmc): pmc is an index specific to PMU
 *        code. Each pmc, stored in kvm_pmc.idx field, is unique across
 *        all perf counters (both gp and fixed). The mapping relationship
 *        between pmc and perf counters is as the following:
 *        * Intel: [0 .. KVM_INTEL_PMC_MAX_GENERIC-1] &lt;=&gt; gp counters
 *                 [INTEL_PMC_IDX_FIXED .. INTEL_PMC_IDX_FIXED + 2] &lt;=&gt; fixed
 *        * AMD:   [0 .. AMD64_NUM_COUNTERS-1] and, for families 15H
 *          and later, [0 .. AMD64_NUM_COUNTERS_CORE-1] &lt;=&gt; gp counters
 */

static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;

#define KVM_X86_PMU_OP(func)					     \
	DEFINE_STATIC_CALL_NULL(kvm_x86_pmu_##func,			     \
				*(((struct kvm_pmu_ops *)0)-&gt;func));
#define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
#include &lt;asm/kvm-x86-pmu-ops.h&gt;

void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
{
<yellow>	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));</yellow>

#define __KVM_X86_PMU_OP(func) \
	static_call_update(kvm_x86_pmu_##func, kvm_pmu_ops.func);
#define KVM_X86_PMU_OP(func) \
	WARN_ON(!kvm_pmu_ops.func); __KVM_X86_PMU_OP(func)
#define KVM_X86_PMU_OP_OPTIONAL __KVM_X86_PMU_OP
#include &lt;asm/kvm-x86-pmu-ops.h&gt;
#undef __KVM_X86_PMU_OP
}

static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
{
<blue>	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);</blue>
}

static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
{
	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
<yellow>	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);</yellow>

<yellow>	kvm_pmu_deliver_pmi(vcpu);</yellow>
<yellow>}</yellow>

static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	bool skip_pmi = false;

	/* Ignore counters that have been reprogrammed already. */
	if (test_and_set_bit(pmc-&gt;idx, pmu-&gt;reprogram_pmi))
		return;

<yellow>	if (pmc->perf_event && pmc->perf_event->attr.precise_ip) {</yellow>
<yellow>		if (!in_pmi) {</yellow>
			/*
			 * TODO: KVM is currently _choosing_ to not generate records
			 * for emulated instructions, avoiding BUFFER_OVF PMI when
			 * there are no records. Strictly speaking, it should be done
			 * as well in the right context to improve sampling accuracy.
			 */
			skip_pmi = true;
		} else {
			/* Indicate PEBS overflow PMI to guest. */
<yellow>			skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,</yellow>
						      (unsigned long *)&amp;pmu-&gt;global_status);
		}
	} else {
<yellow>		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);</yellow>
	}
<yellow>	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);</yellow>

<yellow>	if (!pmc->intr || skip_pmi)</yellow>
		return;

	/*
	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
	 * can be ejected on a guest mode re-entry. Otherwise we can&#x27;t
	 * be sure that vcpu wasn&#x27;t executing hlt instruction at the
	 * time of vmexit and is not going to re-enter guest mode until
	 * woken up. So we should wake it, but this is impossible from
	 * NMI context. Do it from irq work instead.
	 */
<yellow>	if (in_pmi && !kvm_handling_nmi_from_guest(pmc->vcpu))</yellow>
<yellow>		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);</yellow>
	else
<yellow>		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);</yellow>
<yellow>}</yellow>

static void kvm_perf_overflow(struct perf_event *perf_event,
			      struct perf_sample_data *data,
			      struct pt_regs *regs)
{
<yellow>	struct kvm_pmc *pmc = perf_event->overflow_handler_context;</yellow>

	__kvm_perf_overflow(pmc, true);
}

<yellow>static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,</yellow>
				  u64 config, bool exclude_user,
				  bool exclude_kernel, bool intr)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	struct perf_event *event;
	struct perf_event_attr attr = {
		.type = type,
		.size = sizeof(attr),
		.pinned = true,
		.exclude_idle = true,
		.exclude_host = 1,
		.exclude_user = exclude_user,
		.exclude_kernel = exclude_kernel,
		.config = config,
	};
	bool pebs = test_bit(pmc-&gt;idx, (unsigned long *)&amp;pmu-&gt;pebs_enable);

<yellow>	attr.sample_period = get_sample_period(pmc, pmc->counter);</yellow>

<yellow>	if ((attr.config & HSW_IN_TX_CHECKPOINTED) &&</yellow>
<yellow>	    guest_cpuid_is_intel(pmc->vcpu)) {</yellow>
		/*
		 * HSW_IN_TX_CHECKPOINTED is not supported with nonzero
		 * period. Just clear the sample period so at least
		 * allocating the counter doesn&#x27;t fail.
		 */
<yellow>		attr.sample_period = 0;</yellow>
	}
<yellow>	if (pebs) {</yellow>
		/*
		 * The non-zero precision level of guest event makes the ordinary
		 * guest event becomes a guest PEBS event and triggers the host
		 * PEBS PMI handler to determine whether the PEBS overflow PMI
		 * comes from the host counters or the guest.
		 *
		 * For most PEBS hardware events, the difference in the software
		 * precision levels of guest and host PEBS events will not affect
		 * the accuracy of the PEBS profiling result, because the &quot;event IP&quot;
		 * in the PEBS record is calibrated on the guest side.
		 *
		 * On Icelake everything is fine. Other hardware (GLC+, TNT+) that
		 * could possibly care here is unsupported and needs changes.
		 */
<yellow>		attr.precise_ip = 1;</yellow>
<yellow>		if (x86_match_cpu(vmx_icl_pebs_cpu) && pmc->idx == 32)</yellow>
<yellow>			attr.precise_ip = 3;</yellow>
	}

<yellow>	event = perf_event_create_kernel_counter(&attr, -1, current,</yellow>
						 kvm_perf_overflow, pmc);
	if (IS_ERR(event)) {
<yellow>		pr_debug_ratelimited("kvm_pmu: event creation failed %ld for pmc->idx = %d\n",</yellow>
			    PTR_ERR(event), pmc-&gt;idx);
		return;
	}

<yellow>	pmc->perf_event = event;</yellow>
	pmc_to_pmu(pmc)-&gt;event_count++;
	clear_bit(pmc-&gt;idx, pmc_to_pmu(pmc)-&gt;reprogram_pmi);
	pmc-&gt;is_paused = false;
	pmc-&gt;intr = intr || pebs;
}

static void pmc_pause_counter(struct kvm_pmc *pmc)
{
<yellow>	u64 counter = pmc->counter;</yellow>

<yellow>	if (!pmc->perf_event || pmc->is_paused)</yellow>
		return;

	/* update counter, reset event value to avoid redundant accumulation */
<yellow>	counter += perf_event_pause(pmc->perf_event, true);</yellow>
	pmc-&gt;counter = counter &amp; pmc_bitmask(pmc);
	pmc-&gt;is_paused = true;
}

static bool pmc_resume_counter(struct kvm_pmc *pmc)
{
<yellow>	if (!pmc->perf_event)</yellow>
		return false;

	/* recalibrate sample period and check if it&#x27;s accepted by perf core */
<yellow>	if (perf_event_period(pmc->perf_event,</yellow>
			      get_sample_period(pmc, pmc-&gt;counter)))
		return false;

<yellow>	if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=</yellow>
	    (!!pmc-&gt;perf_event-&gt;attr.precise_ip))
		return false;

	/* reuse perf_event to serve as pmc_reprogram_counter() does*/
<yellow>	perf_event_enable(pmc->perf_event);</yellow>
	pmc-&gt;is_paused = false;

	clear_bit(pmc-&gt;idx, (unsigned long *)&amp;pmc_to_pmu(pmc)-&gt;reprogram_pmi);
	return true;
}

static int cmp_u64(const void *pa, const void *pb)
{
<yellow>	u64 a = *(u64 *)pa;</yellow>
	u64 b = *(u64 *)pb;

	return (a &gt; b) - (a &lt; b);
}

static bool check_pmu_event_filter(struct kvm_pmc *pmc)
{
	struct kvm_pmu_event_filter *filter;
<yellow>	struct kvm *kvm = pmc->vcpu->kvm;</yellow>
	bool allow_event = true;
	__u64 key;
	int idx;

	if (!static_call(kvm_x86_pmu_hw_event_available)(pmc))
		return false;

<yellow>	filter = srcu_dereference(kvm->arch.pmu_event_filter, &kvm->srcu);</yellow>
	if (!filter)
		goto out;

<yellow>	if (pmc_is_gp(pmc)) {</yellow>
<yellow>		key = pmc->eventsel & AMD64_RAW_EVENT_MASK_NB;</yellow>
		if (bsearch(&amp;key, filter-&gt;events, filter-&gt;nevents,
			    sizeof(__u64), cmp_u64))
<yellow>			allow_event = filter->action == KVM_PMU_EVENT_ALLOW;</yellow>
		else
<yellow>			allow_event = filter->action == KVM_PMU_EVENT_DENY;</yellow>
	} else {
<yellow>		idx = pmc->idx - INTEL_PMC_IDX_FIXED;</yellow>
		if (filter-&gt;action == KVM_PMU_EVENT_DENY &amp;&amp;
<yellow>		    test_bit(idx, (ulong *)&filter->fixed_counter_bitmap))</yellow>
			allow_event = false;
<yellow>		if (filter->action == KVM_PMU_EVENT_ALLOW &&</yellow>
<yellow>		    !test_bit(idx, (ulong *)&filter->fixed_counter_bitmap))</yellow>
			allow_event = false;
	}

out:
	return allow_event;
}

void reprogram_counter(struct kvm_pmc *pmc)
<blue>{</blue>
<blue>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</blue>
	u64 eventsel = pmc-&gt;eventsel;
	u64 new_config = eventsel;
	u8 fixed_ctr_ctrl;

<yellow>	pmc_pause_counter(pmc);</yellow>

<blue>	if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))</blue>
		return;

<yellow>	if (!check_pmu_event_filter(pmc))</yellow>
		return;

<yellow>	if (eventsel & ARCH_PERFMON_EVENTSEL_PIN_CONTROL)</yellow>
<yellow>		printk_once("kvm pmu: pin control bit is ignored\n");</yellow>

<yellow>	if (pmc_is_fixed(pmc)) {</yellow>
<yellow>		fixed_ctr_ctrl = fixed_ctrl_field(pmu->fixed_ctr_ctrl,</yellow>
						  pmc-&gt;idx - INTEL_PMC_IDX_FIXED);
		if (fixed_ctr_ctrl &amp; 0x1)
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_OS;</yellow>
<yellow>		if (fixed_ctr_ctrl & 0x2)</yellow>
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_USR;</yellow>
<yellow>		if (fixed_ctr_ctrl & 0x8)</yellow>
<yellow>			eventsel |= ARCH_PERFMON_EVENTSEL_INT;</yellow>
<yellow>		new_config = (u64)fixed_ctr_ctrl;</yellow>
	}

<yellow>	if (pmc->current_config == new_config && pmc_resume_counter(pmc))</yellow>
		return;

<yellow>	pmc_release_perf_event(pmc);</yellow>

<yellow>	pmc->current_config = new_config;</yellow>
	pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
			      (eventsel &amp; pmu-&gt;raw_event_mask),
			      !(eventsel &amp; ARCH_PERFMON_EVENTSEL_USR),
			      !(eventsel &amp; ARCH_PERFMON_EVENTSEL_OS),
			      eventsel &amp; ARCH_PERFMON_EVENTSEL_INT);
}
EXPORT_SYMBOL_GPL(reprogram_counter);

void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	int bit;

<yellow>	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {</yellow>
<yellow>		struct kvm_pmc *pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, bit);</yellow>

<yellow>		if (unlikely(!pmc || !pmc->perf_event)) {</yellow>
<yellow>			clear_bit(bit, pmu->reprogram_pmi);</yellow>
			continue;
		}
<yellow>		reprogram_counter(pmc);</yellow>
	}

	/*
	 * Unused perf_events are only released if the corresponding MSRs
	 * weren&#x27;t accessed during the last vCPU time slice. kvm_arch_sched_in
	 * triggers KVM_REQ_PMU if cleanup is needed.
	 */
<yellow>	if (unlikely(pmu->need_cleanup))</yellow>
<yellow>		kvm_pmu_cleanup(vcpu);</yellow>
<yellow>}</yellow>

/* check if idx is a valid index to access PMU */
bool kvm_pmu_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
{
<yellow>	return static_call(kvm_x86_pmu_is_valid_rdpmc_ecx)(vcpu, idx);</yellow>
}

bool is_vmware_backdoor_pmc(u32 pmc_idx)
{
<blue>	switch (pmc_idx) {</blue>
	case VMWARE_BACKDOOR_PMC_HOST_TSC:
	case VMWARE_BACKDOOR_PMC_REAL_TIME:
	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
		return true;
	}
	return false;
}

static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
{
	u64 ctr_val;

<yellow>	switch (idx) {</yellow>
	case VMWARE_BACKDOOR_PMC_HOST_TSC:
<yellow>		ctr_val = rdtsc();</yellow>
		break;
	case VMWARE_BACKDOOR_PMC_REAL_TIME:
<yellow>		ctr_val = ktime_get_boottime_ns();</yellow>
		break;
	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
<yellow>		ctr_val = ktime_get_boottime_ns() +</yellow>
			vcpu-&gt;kvm-&gt;arch.kvmclock_offset;
		break;
	default:
		return 1;
	}

<yellow>	*data = ctr_val;</yellow>
	return 0;
}

int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
<blue>{</blue>
	bool fast_mode = idx &amp; (1u &lt;&lt; 31);
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;
<yellow>	u64 mask = fast_mode ? ~0u : ~0ull;</yellow>

<blue>	if (!pmu->version)</blue>
		return 1;

<blue>	if (is_vmware_backdoor_pmc(idx))</blue>
<yellow>		return kvm_pmu_rdpmc_vmware(vcpu, idx, data);</yellow>

<blue>	pmc = static_call(kvm_x86_pmu_rdpmc_ecx_to_pmc)(vcpu, idx, &mask);</blue>
	if (!pmc)
		return 1;

<yellow>	if (!(kvm_read_cr4(vcpu) & X86_CR4_PCE) &&</yellow>
<yellow>	    (static_call(kvm_x86_get_cpl)(vcpu) != 0) &&</yellow>
<yellow>	    (kvm_read_cr0(vcpu) & X86_CR0_PE))</yellow>
		return 1;

<yellow>	*data = pmc_read_counter(pmc) & mask;</yellow>
	return 0;
}

<yellow>void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)</yellow>
{
<yellow>	if (lapic_in_kernel(vcpu)) {</yellow>
<yellow>		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);</yellow>
		kvm_apic_local_deliver(vcpu-&gt;arch.apic, APIC_LVTPC);
	}
<yellow>}</yellow>

bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
{
<blue>	return static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr) ||</blue>
<blue>		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);</blue>
<blue>}</blue>

static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc = static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr);

	if (pmc)
<blue>		__set_bit(pmc->idx, pmu->pmc_in_use);</blue>
}

int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
<blue>	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);</blue>
}

int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
<blue>	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);</blue>
<blue>	return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);</blue>
}

/* refresh PMU settings. This function generally is called when underlying
 * settings are changed (such as changes of PMU CPUID by guest VMs), which
 * should rarely happen.
 */
void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
{
<blue>	static_call(kvm_x86_pmu_refresh)(vcpu);</blue>
}

void kvm_pmu_reset(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);

<blue>	irq_work_sync(&pmu->irq_work);</blue>
	static_call(kvm_x86_pmu_reset)(vcpu);
}

void kvm_pmu_init(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);

<blue>	memset(pmu, 0, sizeof(*pmu));</blue>
	static_call(kvm_x86_pmu_init)(vcpu);
	init_irq_work(&amp;pmu-&gt;irq_work, kvm_pmi_trigger_fn);
	pmu-&gt;event_count = 0;
	pmu-&gt;need_cleanup = false;
	kvm_pmu_refresh(vcpu);
}

/* Release perf_events for vPMCs that have been unused for a full time slice.  */
void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
{
<yellow>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</yellow>
	struct kvm_pmc *pmc = NULL;
	DECLARE_BITMAP(bitmask, X86_PMC_IDX_MAX);
	int i;

	pmu-&gt;need_cleanup = false;

	bitmap_andnot(bitmask, pmu-&gt;all_valid_pmc_idx,
		      pmu-&gt;pmc_in_use, X86_PMC_IDX_MAX);

<yellow>	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {</yellow>
<yellow>		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);</yellow>

<yellow>		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))</yellow>
<yellow>			pmc_stop_counter(pmc);</yellow>
	}

<yellow>	static_call_cond(kvm_x86_pmu_cleanup)(vcpu);</yellow>

	bitmap_zero(pmu-&gt;pmc_in_use, X86_PMC_IDX_MAX);
}

void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
{
	kvm_pmu_reset(vcpu);
}

static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
{
	u64 prev_count;

<yellow>	prev_count = pmc->counter;</yellow>
	pmc-&gt;counter = (pmc-&gt;counter + 1) &amp; pmc_bitmask(pmc);

	reprogram_counter(pmc);
	if (pmc-&gt;counter &lt; prev_count)
<yellow>		__kvm_perf_overflow(pmc, false);</yellow>
}

static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
	unsigned int perf_hw_id)
{
<yellow>	return !((pmc->eventsel ^ perf_get_hw_event_config(perf_hw_id)) &</yellow>
		AMD64_RAW_EVENT_MASK_NB);
}

static inline bool cpl_is_matched(struct kvm_pmc *pmc)
{
	bool select_os, select_user;
<yellow>	u64 config = pmc->current_config;</yellow>

	if (pmc_is_gp(pmc)) {
<yellow>		select_os = config & ARCH_PERFMON_EVENTSEL_OS;</yellow>
		select_user = config &amp; ARCH_PERFMON_EVENTSEL_USR;
	} else {
<yellow>		select_os = config & 0x1;</yellow>
		select_user = config &amp; 0x2;
	}

<yellow>	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;</yellow>
}

void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
{
<blue>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</blue>
	struct kvm_pmc *pmc;
	int i;

<blue>	for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {</blue>
<blue>		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);</blue>

<blue>		if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))</blue>
			continue;

		/* Ignore checks for edge detect, pin control, invert and CMASK bits */
<yellow>		if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))</yellow>
<yellow>			kvm_pmu_incr_counter(pmc);</yellow>
	}
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_pmu_trigger_event);

int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
<yellow>{</yellow>
	struct kvm_pmu_event_filter tmp, *filter;
	size_t size;
	int r;

<yellow>	if (copy_from_user(&tmp, argp, sizeof(tmp)))</yellow>
		return -EFAULT;

<yellow>	if (tmp.action != KVM_PMU_EVENT_ALLOW &&</yellow>
	    tmp.action != KVM_PMU_EVENT_DENY)
		return -EINVAL;

<yellow>	if (tmp.flags != 0)</yellow>
		return -EINVAL;

<yellow>	if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)</yellow>
		return -E2BIG;

<yellow>	size = struct_size(filter, events, tmp.nevents);</yellow>
	filter = kmalloc(size, GFP_KERNEL_ACCOUNT);
	if (!filter)
		return -ENOMEM;

	r = -EFAULT;
<yellow>	if (copy_from_user(filter, argp, size))</yellow>
		goto cleanup;

	/* Ensure nevents can&#x27;t be changed between the user copies. */
<yellow>	*filter = tmp;</yellow>

	/*
	 * Sort the in-kernel list so that we can search it with bsearch.
	 */
	sort(&amp;filter-&gt;events, filter-&gt;nevents, sizeof(__u64), cmp_u64, NULL);

	mutex_lock(&amp;kvm-&gt;lock);
	filter = rcu_replace_pointer(kvm-&gt;arch.pmu_event_filter, filter,
				     mutex_is_locked(&amp;kvm-&gt;lock));
	mutex_unlock(&amp;kvm-&gt;lock);

	synchronize_srcu_expedited(&amp;kvm-&gt;srcu);
	r = 0;
cleanup:
<yellow>	kfree(filter);</yellow>
	return r;
}


</code></pre></td></tr></table>
</body>
</html>
