<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>831<br>832<br>833<br>834<br>835<br>836<br>837<br>838<br>839<br>840<br>841<br>842<br>843<br>844<br>845<br>846<br>847<br>848<br>849<br>850<br>851<br>852<br>853<br>854<br>855<br>856<br>857<br>858<br>859<br>860<br>861<br>862<br>863<br>864<br>865<br>866<br>867<br>868<br>869<br>870<br>871<br>872<br>873<br>874<br>875<br>876<br>877<br>878<br>879<br>880<br>881<br>882<br>883<br>884<br>885<br>886<br>887<br>888<br>889<br>890<br>891<br>892<br>893<br>894<br>895<br>896<br>897<br>898<br>899<br>900<br>901<br>902<br>903<br>904<br>905<br>906<br>907<br>908<br>909<br>910<br>911<br>912<br>913<br>914<br>915<br>916<br>917<br>918<br>919<br>920<br>921<br>922<br>923<br>924<br>925<br>926<br>927<br>928<br>929<br>930<br>931<br>932<br>933<br>934<br>935<br>936<br>937<br>938<br>939<br>940<br>941<br>942<br>943<br>944<br>945<br>946<br>947<br>948<br>949<br>950<br>951<br>952<br>953<br>954<br>955<br>956<br>957<br>958<br>959<br>960<br>961<br>962<br>963<br>964<br>965<br>966<br>967<br>968<br>969<br>970<br>971<br>972<br>973<br>974<br>975<br>976<br>977<br>978<br>979<br>980<br>981<br>982<br>983<br>984<br>985<br>986<br>987<br>988<br>989<br>990<br>991<br>992<br>993<br>994<br>995<br>996<br>997<br>998<br>999<br>1000<br>1001<br>1002<br>1003<br>1004<br>1005<br>1006<br>1007<br>1008<br>1009<br>1010<br>1011<br>1012<br>1013<br>1014<br>1015<br>1016<br>1017<br>1018<br>1019<br>1020<br>1021<br>1022<br>1023<br>1024<br>1025<br>1026<br>1027<br>1028<br>1029<br>1030<br>1031<br>1032<br>1033<br>1034<br>1035<br>1036<br>1037<br>1038<br>1039<br>1040<br>1041<br>1042<br>1043<br>1044<br>1045<br>1046<br>1047<br>1048<br>1049<br>1050<br>1051<br>1052<br>1053<br>1054<br>1055<br>1056<br>1057<br>1058<br>1059<br>1060<br>1061<br>1062<br>1063<br>1064<br>1065<br>1066<br>1067<br>1068<br>1069<br>1070<br>1071<br>1072<br>1073<br>1074<br>1075<br>1076<br>1077<br>1078<br>1079<br>1080<br>1081<br>1082<br>1083<br>1084<br>1085<br>1086<br>1087<br>1088<br>1089<br>1090<br>1091<br>1092<br>1093<br>1094<br>1095<br>1096<br>1097<br>1098<br>1099<br>1100<br>1101<br>1102<br>1103<br>1104<br>1105<br>1106<br>1107<br>1108<br>1109<br>1110<br>1111<br>1112<br>1113<br>1114<br>1115<br>1116<br>1117<br>1118<br>1119<br>1120<br>1121<br>1122<br>1123<br>1124<br>1125<br>1126<br>1127<br>1128<br>1129<br>1130<br>1131<br>1132<br>1133<br>1134<br>1135<br>1136<br>1137<br>1138<br>1139<br>1140<br>1141<br>1142<br>1143<br>1144<br>1145<br>1146<br>1147<br>1148<br>1149<br>1150<br>1151<br>1152<br>1153<br>1154<br>1155<br>1156<br>1157<br>1158<br>1159<br>1160<br>1161<br>1162<br>1163<br>1164<br>1165<br>1166<br>1167<br>1168<br>1169<br>1170<br>1171<br>1172<br>1173<br>1174<br>1175<br>1176<br>1177<br>1178<br>1179<br>1180<br>1181<br>1182<br>1183<br>1184<br>1185<br>1186<br>1187<br>1188<br>1189<br>1190<br>1191<br>1192<br>1193<br>1194<br>1195<br>1196<br>1197<br>1198<br>1199<br>1200<br>1201<br>1202<br>1203<br>1204<br>1205<br>1206<br>1207<br>1208<br>1209<br>1210<br>1211<br>1212<br>1213<br>1214<br>1215<br>1216<br>1217<br>1218<br>1219<br>1220<br>1221<br>1222<br>1223<br>1224<br>1225<br>1226<br>1227<br>1228<br>1229<br>1230<br>1231<br>1232<br>1233<br>1234<br>1235<br>1236<br>1237<br>1238<br>1239<br>1240<br>1241<br>1242<br>1243<br>1244<br>1245<br>1246<br>1247<br>1248<br>1249<br>1250<br>1251<br>1252<br>1253<br>1254<br>1255<br>1256<br>1257<br>1258<br>1259<br>1260<br>1261<br>1262<br>1263<br>1264<br>1265<br>1266<br>1267<br>1268<br>1269<br>1270<br>1271<br>1272<br>1273<br>1274<br>1275<br>1276<br>1277<br>1278<br>1279<br>1280<br>1281<br>1282<br>1283<br>1284<br>1285<br>1286<br>1287<br>1288<br>1289<br>1290<br>1291<br>1292<br>1293<br>1294<br>1295<br>1296<br>1297<br>1298<br>1299<br>1300<br>1301<br>1302<br>1303<br>1304<br>1305<br>1306<br>1307<br>1308<br>1309<br>1310<br>1311<br>1312<br>1313<br>1314<br>1315<br>1316<br>1317<br>1318<br>1319<br>1320<br>1321<br>1322<br>1323<br>1324<br>1325<br>1326<br>1327<br>1328<br>1329<br>1330<br>1331<br>1332<br>1333<br>1334<br>1335<br>1336<br>1337<br>1338<br>1339<br>1340<br>1341<br>1342<br>1343<br>1344<br>1345<br>1346<br>1347<br>1348<br>1349<br>1350<br>1351<br>1352<br>1353<br>1354<br>1355<br>1356<br>1357<br>1358<br>1359<br>1360<br>1361<br>1362<br>1363<br>1364<br>1365<br>1366<br>1367<br>1368<br>1369<br>1370<br>1371<br>1372<br>1373<br>1374<br>1375<br>1376<br>1377<br>1378<br>1379<br>1380<br>1381<br>1382<br>1383<br>1384<br>1385<br>1386<br>1387<br>1388<br>1389<br>1390<br>1391<br>1392<br>1393<br>1394<br>1395<br>1396<br>1397<br>1398<br>1399<br>1400<br>1401<br>1402<br>1403<br>1404<br>1405<br>1406<br>1407<br>1408<br>1409<br>1410<br>1411<br>1412<br>1413<br>1414<br>1415<br>1416<br>1417<br>1418<br>1419<br>1420<br>1421<br>1422<br>1423<br>1424<br>1425<br>1426<br>1427<br>1428<br>1429<br>1430<br>1431<br>1432<br>1433<br>1434<br>1435<br>1436<br>1437<br>1438<br>1439<br>1440<br>1441<br>1442<br>1443<br>1444<br>1445<br>1446<br>1447<br>1448<br>1449<br>1450<br>1451<br>1452<br>1453<br>1454<br>1455<br>1456<br>1457<br>1458<br>1459<br>1460<br>1461<br>1462<br>1463<br>1464<br>1465<br>1466<br>1467<br>1468<br>1469<br>1470<br>1471<br>1472<br>1473<br>1474<br>1475<br>1476<br>1477<br>1478<br>1479<br>1480<br>1481<br>1482<br>1483<br>1484<br>1485<br>1486<br>1487<br>1488<br>1489<br>1490<br>1491<br>1492<br>1493<br>1494<br>1495<br>1496<br>1497<br>1498<br>1499<br>1500<br>1501<br>1502<br>1503<br>1504<br>1505<br>1506<br>1507<br>1508<br>1509<br>1510<br>1511<br>1512<br>1513<br>1514<br>1515<br>1516<br>1517<br>1518<br>1519<br>1520<br>1521<br>1522<br>1523<br>1524<br>1525<br>1526<br>1527<br>1528<br>1529<br>1530<br>1531<br>1532<br>1533<br>1534<br>1535<br>1536<br>1537<br>1538<br>1539<br>1540<br>1541<br>1542<br>1543<br>1544<br>1545<br>1546<br>1547<br>1548<br>1549<br>1550<br>1551<br>1552<br>1553<br>1554<br>1555<br>1556<br>1557<br>1558<br>1559<br>1560<br>1561<br>1562<br>1563<br>1564<br>1565<br>1566<br>1567<br>1568<br>1569<br>1570<br>1571<br>1572<br>1573<br>1574<br>1575<br>1576<br>1577<br>1578<br>1579<br>1580<br>1581<br>1582<br>1583<br>1584<br>1585<br>1586<br>1587<br>1588<br>1589<br>1590<br>1591<br>1592<br>1593<br>1594<br>1595<br>1596<br>1597<br>1598<br>1599<br>1600<br>1601<br>1602<br>1603<br>1604<br>1605<br>1606<br>1607<br>1608<br>1609<br>1610<br>1611<br>1612<br>1613<br>1614<br>1615<br>1616<br>1617<br>1618<br>1619<br>1620<br>1621<br>1622<br>1623<br>1624<br>1625<br>1626<br>1627<br>1628<br>1629<br>1630<br>1631<br>1632<br>1633<br>1634<br>1635<br>1636<br>1637<br>1638<br>1639<br>1640<br>1641<br>1642<br>1643<br>1644<br>1645<br>1646<br>1647<br>1648<br>1649<br>1650<br>1651<br>1652<br>1653<br>1654<br>1655<br>1656<br>1657<br>1658<br>1659<br>1660<br>1661<br>1662<br>1663<br>1664<br>1665<br>1666<br>1667<br>1668<br>1669<br>1670<br>1671<br>1672<br>1673<br>1674<br>1675<br>1676<br>1677<br>1678<br>1679<br>1680<br>1681<br>1682<br>1683<br>1684<br>1685<br>1686<br>1687<br>1688<br>1689<br>1690<br>1691<br>1692<br>1693<br>1694<br>1695<br>1696<br>1697<br>1698<br>1699<br>1700<br>1701<br>1702<br>1703<br>1704<br>1705<br>1706<br>1707<br>1708<br>1709<br>1710<br>1711<br>1712<br>1713<br>1714<br>1715<br>1716<br>1717<br>1718<br>1719<br>1720<br>1721<br>1722<br>1723<br>1724<br>1725<br>1726<br>1727<br>1728<br>1729<br>1730<br>1731<br>1732<br>1733<br>1734<br>1735<br>1736<br>1737<br>1738<br>1739<br>1740<br>1741<br>1742<br>1743<br>1744<br>1745<br>1746<br>1747<br>1748<br>1749<br>1750<br>1751<br>1752<br>1753<br>1754<br>1755<br>1756<br>1757<br>1758<br>1759<br>1760<br>1761<br>1762<br>1763<br>1764<br>1765<br>1766<br>1767<br>1768<br>1769<br>1770<br>1771<br>1772<br>1773<br>1774<br>1775<br>1776<br>1777<br>1778<br>1779<br>1780<br>1781<br>1782<br>1783<br>1784<br>1785<br>1786<br>1787<br>1788<br>1789<br>1790<br>1791<br>1792<br>1793<br>1794<br>1795<br>1796<br>1797<br>1798<br>1799<br>1800<br>1801<br>1802<br>1803<br>1804<br>1805<br>1806<br>1807<br>1808<br>1809<br>1810<br>1811<br>1812<br>1813<br>1814<br>1815<br>1816<br>1817<br>1818<br>1819<br>1820<br>1821<br>1822<br>1823<br>1824<br>1825<br>1826<br>1827<br>1828<br>1829<br>1830<br>1831<br>1832<br>1833<br>1834<br>1835<br>1836<br>1837<br>1838<br>1839<br>1840<br>1841<br>1842<br>1843<br>1844<br>1845<br>1846<br>1847<br>1848<br>1849<br>1850<br>1851<br>1852<br>1853<br>1854<br>1855<br>1856<br>1857<br>1858<br>1859<br>1860<br>1861<br>1862<br>1863<br>1864<br>1865<br>1866<br>1867<br>1868<br>1869<br>1870<br>1871<br>1872<br>1873<br>1874<br>1875<br>1876<br>1877<br>1878<br>1879<br>1880<br>1881<br>1882<br>1883<br>1884<br>1885<br>1886<br>1887<br>1888<br>1889<br>1890<br>1891<br>1892<br>1893<br>1894<br>1895<br>1896<br>1897<br>1898<br>1899<br>1900<br>1901<br>1902<br>1903<br>1904<br>1905<br>1906<br>1907<br>1908<br>1909<br>1910<br>1911<br>1912<br>1913<br>1914<br>1915<br>1916<br>1917<br>1918<br>1919<br>1920<br>1921<br>1922<br>1923<br>1924<br>1925<br>1926<br>1927<br>1928<br>1929<br>1930<br>1931<br>1932<br>1933<br>1934<br>1935<br>1936<br>1937<br>1938<br>1939<br>1940<br>1941<br>1942<br>1943<br>1944<br>1945<br>1946<br>1947<br>1948<br>1949<br>1950<br>1951<br>1952<br>1953<br>1954<br>1955<br>1956<br>1957<br>1958<br>1959<br>1960<br>1961<br>1962<br>1963<br>1964<br>1965<br>1966<br>1967<br>1968<br>1969<br>1970<br>1971<br>1972<br>1973<br>1974<br>1975<br>1976<br>1977<br>1978<br>1979<br>1980<br>1981<br>1982<br>1983<br>1984<br>1985<br>1986<br>1987<br>1988<br>1989<br>1990<br>1991<br>1992<br>1993<br>1994<br>1995<br>1996<br>1997<br>1998<br>1999<br>2000<br>2001<br>2002<br>2003<br>2004<br>2005<br>2006<br>2007<br>2008<br>2009<br>2010<br>2011<br>2012<br>2013<br>2014<br>2015<br>2016<br>2017<br>2018<br>2019<br>2020<br>2021<br>2022<br>2023<br>2024<br>2025<br>2026<br>2027<br>2028<br>2029<br>2030<br>2031<br>2032<br>2033<br>2034<br>2035<br>2036<br>2037<br>2038<br>2039<br>2040<br>2041<br>2042<br>2043<br>2044<br>2045<br>2046<br>2047<br>2048<br>2049<br>2050<br>2051<br>2052<br>2053<br>2054<br>2055<br>2056<br>2057<br>2058<br>2059<br>2060<br>2061<br>2062<br>2063<br>2064<br>2065<br>2066<br>2067<br>2068<br>2069<br>2070<br>2071<br>2072<br>2073<br>2074<br>2075<br>2076<br>2077<br>2078<br>2079<br>2080<br>2081<br>2082<br>2083<br>2084<br>2085<br>2086<br>2087<br>2088<br>2089<br>2090<br>2091<br>2092<br>2093<br>2094<br>2095<br>2096<br>2097<br>2098<br>2099<br>2100<br>2101<br>2102<br>2103<br>2104<br>2105<br>2106<br>2107<br>2108<br>2109<br>2110<br>2111<br>2112<br>2113<br>2114<br>2115<br>2116<br>2117<br>2118<br>2119<br>2120<br>2121<br>2122<br>2123<br>2124<br>2125<br>2126<br>2127<br>2128<br>2129<br>2130<br>2131<br>2132<br>2133<br>2134<br>2135<br>2136<br>2137<br>2138<br>2139<br>2140<br>2141<br>2142<br>2143<br>2144<br>2145<br>2146<br>2147<br>2148<br>2149<br>2150<br>2151<br>2152<br>2153<br>2154<br>2155<br>2156<br>2157<br>2158<br>2159<br>2160<br>2161<br>2162<br>2163<br>2164<br>2165<br>2166<br>2167<br>2168<br>2169<br>2170<br>2171<br>2172<br>2173<br>2174<br>2175<br>2176<br>2177<br>2178<br>2179<br>2180<br>2181<br>2182<br>2183<br>2184<br>2185<br>2186<br>2187<br>2188<br>2189<br>2190<br>2191<br>2192<br>2193<br>2194<br>2195<br>2196<br>2197<br>2198<br>2199<br>2200<br>2201<br>2202<br>2203<br>2204<br>2205<br>2206<br>2207<br>2208<br>2209<br>2210<br>2211<br>2212<br>2213<br>2214<br>2215<br>2216<br>2217<br>2218<br>2219<br>2220<br>2221<br>2222<br>2223<br>2224<br>2225<br>2226<br>2227<br>2228<br>2229<br>2230<br>2231<br>2232<br>2233<br>2234<br>2235<br>2236<br>2237<br>2238<br>2239<br>2240<br>2241<br>2242<br>2243<br>2244<br>2245<br>2246<br>2247<br>2248<br>2249<br>2250<br>2251<br>2252<br>2253<br>2254<br>2255<br>2256<br>2257<br>2258<br>2259<br>2260<br>2261<br>2262<br>2263<br>2264<br>2265<br>2266<br>2267<br>2268<br>2269<br>2270<br>2271<br>2272<br>2273<br>2274<br>2275<br>2276<br>2277<br>2278<br>2279<br>2280<br>2281<br>2282<br>2283<br>2284<br>2285<br>2286<br>2287<br>2288<br>2289<br>2290<br>2291<br>2292<br>2293<br>2294<br>2295<br>2296<br>2297<br>2298<br>2299<br>2300<br>2301<br>2302<br>2303<br>2304<br>2305<br>2306<br>2307<br>2308<br>2309<br>2310<br>2311<br>2312<br>2313<br>2314<br>2315<br>2316<br>2317<br>2318<br>2319<br>2320<br>2321<br>2322<br>2323<br>2324<br>2325<br>2326<br>2327<br>2328<br>2329<br>2330<br>2331<br>2332<br>2333<br>2334<br>2335<br>2336<br>2337<br>2338<br>2339<br>2340<br>2341<br>2342<br>2343<br>2344<br>2345<br>2346<br>2347<br>2348<br>2349<br>2350<br>2351<br>2352<br>2353<br>2354<br>2355<br>2356<br>2357<br>2358<br>2359<br>2360<br>2361<br>2362<br>2363<br>2364<br>2365<br>2366<br>2367<br>2368<br>2369<br>2370<br>2371<br>2372<br>2373<br>2374<br>2375<br>2376<br>2377<br>2378<br>2379<br>2380<br>2381<br>2382<br>2383<br>2384<br>2385<br>2386<br>2387<br>2388<br>2389<br>2390<br>2391<br>2392<br>2393<br>2394<br>2395<br>2396<br>2397<br>2398<br>2399<br>2400<br>2401<br>2402<br>2403<br>2404<br>2405<br>2406<br>2407<br>2408<br>2409<br>2410<br>2411<br>2412<br>2413<br>2414<br>2415<br>2416<br>2417<br>2418<br>2419<br>2420<br>2421<br>2422<br>2423<br>2424<br>2425<br>2426<br>2427<br>2428<br>2429<br>2430<br>2431<br>2432<br>2433<br>2434<br>2435<br>2436<br>2437<br>2438<br>2439<br>2440<br>2441<br>2442<br>2443<br>2444<br>2445<br>2446<br>2447<br>2448<br>2449<br>2450<br>2451<br>2452<br>2453<br>2454<br>2455<br>2456<br>2457<br>2458<br>2459<br>2460<br>2461<br>2462<br>2463<br>2464<br>2465<br>2466<br>2467<br>2468<br>2469<br>2470<br>2471<br>2472<br>2473<br>2474<br>2475<br>2476<br>2477<br>2478<br>2479<br>2480<br>2481<br>2482<br>2483<br>2484<br>2485<br>2486<br>2487<br>2488<br>2489<br>2490<br>2491<br>2492<br>2493<br>2494<br>2495<br>2496<br>2497<br>2498<br>2499<br>2500<br>2501<br>2502<br>2503<br>2504<br>2505<br>2506<br>2507<br>2508<br>2509<br>2510<br>2511<br>2512<br>2513<br>2514<br>2515<br>2516<br>2517<br>2518<br>2519<br>2520<br>2521<br>2522<br>2523<br>2524<br>2525<br>2526<br>2527<br>2528<br>2529<br>2530<br>2531<br>2532<br>2533<br>2534<br>2535<br>2536<br>2537<br>2538<br>2539<br>2540<br>2541<br>2542<br>2543<br>2544<br>2545<br>2546<br>2547<br>2548<br>2549<br>2550<br>2551<br>2552<br>2553<br>2554<br>2555<br>2556<br>2557<br>2558<br>2559<br>2560<br>2561<br>2562<br>2563<br>2564<br>2565<br>2566<br>2567<br>2568<br>2569<br>2570<br>2571<br>2572<br>2573<br>2574<br>2575<br>2576<br>2577<br>2578<br>2579<br>2580<br>2581<br>2582<br>2583<br>2584<br>2585<br>2586<br>2587<br>2588<br>2589<br>2590<br>2591<br>2592<br>2593<br>2594<br>2595<br>2596<br>2597<br>2598<br>2599<br>2600<br>2601<br>2602<br>2603<br>2604<br>2605<br>2606<br>2607<br>2608<br>2609<br>2610<br>2611<br>2612<br>2613<br>2614<br>2615<br>2616<br>2617<br>2618<br>2619<br>2620<br>2621<br>2622<br>2623<br>2624<br>2625<br>2626<br>2627<br>2628<br>2629<br>2630<br>2631<br>2632<br>2633<br>2634<br>2635<br>2636<br>2637<br>2638<br>2639<br>2640<br>2641<br>2642<br>2643<br>2644<br>2645<br>2646<br>2647<br>2648<br>2649<br>2650<br>2651<br>2652<br>2653<br>2654<br>2655<br>2656<br>2657<br>2658<br>2659<br>2660<br>2661<br>2662<br>2663<br>2664<br>2665<br>2666<br>2667<br>2668<br>2669<br>2670<br>2671<br>2672<br>2673<br>2674<br>2675<br>2676<br>2677<br>2678<br>2679<br>2680<br>2681<br>2682<br>2683<br>2684<br>2685<br>2686<br>2687<br>2688<br>2689<br>2690<br>2691<br>2692<br>2693<br>2694<br>2695<br>2696<br>2697<br>2698<br>2699<br>2700<br>2701<br>2702<br>2703<br>2704<br>2705<br>2706<br>2707<br>2708<br>2709<br>2710<br>2711<br>2712<br>2713<br>2714<br>2715<br>2716<br>2717<br>2718<br>2719<br>2720<br>2721<br>2722<br>2723<br>2724<br>2725<br>2726<br>2727<br>2728<br>2729<br>2730<br>2731<br>2732<br>2733<br>2734<br>2735<br>2736<br>2737<br>2738<br>2739<br>2740<br>2741<br>2742<br>2743<br>2744<br>2745<br>2746<br>2747<br>2748<br>2749<br>2750<br>2751<br>2752<br>2753<br>2754<br>2755<br>2756<br>2757<br>2758<br>2759<br>2760<br>2761<br>2762<br>2763<br>2764<br>2765<br>2766<br>2767<br>2768<br>2769<br>2770<br>2771<br>2772<br>2773<br>2774<br>2775<br>2776<br>2777<br>2778<br>2779<br>2780<br>2781<br>2782<br>2783<br>2784<br>2785<br>2786<br>2787<br>2788<br>2789<br>2790<br>2791<br>2792<br>2793<br>2794<br>2795<br>2796<br>2797<br>2798<br>2799<br>2800<br>2801<br>2802<br>2803<br>2804<br>2805<br>2806<br>2807<br>2808<br>2809<br>2810<br>2811<br>2812<br>2813<br>2814<br>2815<br>2816<br>2817<br>2818<br>2819<br>2820<br>2821<br>2822<br>2823<br>2824<br>2825<br>2826<br>2827<br>2828<br>2829<br>2830<br>2831<br>2832<br>2833<br>2834<br>2835<br>2836<br>2837<br>2838<br>2839<br>2840<br>2841<br>2842<br>2843<br>2844<br>2845<br>2846<br>2847<br>2848<br>2849<br>2850<br>2851<br>2852<br>2853<br>2854<br>2855<br>2856<br>2857<br>2858<br>2859<br>2860<br>2861<br>2862<br>2863<br>2864<br>2865<br>2866<br>2867<br>2868<br>2869<br>2870<br>2871<br>2872<br>2873<br>2874<br>2875<br>2876<br>2877<br>2878<br>2879<br>2880<br>2881<br>2882<br>2883<br>2884<br>2885<br>2886<br>2887<br>2888<br>2889<br>2890<br>2891<br>2892<br>2893<br>2894<br>2895<br>2896<br>2897<br>2898<br>2899<br>2900<br>2901<br>2902<br>2903<br>2904<br>2905<br>2906<br>2907<br>2908<br>2909<br>2910<br>2911<br>2912<br>2913<br>2914<br>2915<br>2916<br>2917<br>2918<br>2919<br>2920<br>2921<br>2922<br>2923<br>2924<br>2925<br>2926<br>2927<br>2928<br>2929<br>2930<br>2931<br>2932<br>2933<br>2934<br>2935<br>2936<br>2937<br>2938<br>2939<br>2940<br>2941<br>2942<br>2943<br>2944<br>2945<br>2946<br>2947<br>2948<br>2949<br>2950<br>2951<br>2952<br>2953<br>2954<br>2955<br>2956<br>2957<br>2958<br>2959<br>2960<br>2961<br>2962<br>2963<br>2964<br>2965<br>2966<br>2967<br>2968<br>2969<br>2970<br>2971<br>2972<br>2973<br>2974<br>2975<br>2976<br>2977<br>2978<br>2979<br>2980<br>2981<br>2982<br>2983<br>2984<br>2985<br>2986<br>2987<br>2988<br>2989<br>2990<br>2991<br>2992<br>2993<br>2994<br>2995<br>2996<br>2997<br>2998<br>2999<br>3000<br>3001<br>3002<br>3003<br>3004<br>3005<br>3006<br>3007<br>3008<br>3009<br>3010<br>3011<br>3012<br>3013<br>3014<br>3015<br>3016<br>3017<br>3018<br>3019<br>3020<br>3021<br>3022<br>3023<br>3024<br>3025<br>3026<br>3027<br>3028<br>3029<br>3030<br>3031<br>3032<br>3033<br>3034<br>3035<br>3036<br>3037<br>3038<br>3039<br>3040<br>3041<br>3042<br>3043<br>3044<br>3045<br>3046<br>3047<br>3048<br>3049<br>3050<br>3051<br>3052<br>3053<br>3054<br>3055<br>3056<br>3057<br>3058<br>3059<br>3060<br>3061<br>3062<br>3063<br>3064<br>3065<br>3066<br>3067<br>3068<br>3069<br>3070<br>3071<br>3072<br>3073<br>3074<br>3075<br>3076<br>3077<br>3078<br>3079<br>3080<br>3081<br>3082<br>3083<br>3084<br>3085<br>3086<br>3087<br>3088<br>3089<br>3090<br>3091<br>3092<br>3093<br>3094<br>3095<br>3096<br>3097<br>3098<br>3099<br>3100<br>3101<br>3102<br>3103<br>3104<br>3105<br>3106<br>3107<br>3108<br>3109<br>3110<br>3111<br>3112<br>3113<br>3114<br>3115<br>3116<br>3117<br>3118<br>3119<br>3120<br>3121<br>3122<br>3123<br>3124<br>3125<br>3126<br>3127<br>3128<br>3129<br>3130<br>3131<br>3132<br>3133<br>3134<br>3135<br>3136<br>3137<br>3138<br>3139<br>3140<br>3141<br>3142<br>3143<br>3144<br>3145<br>3146<br>3147<br>3148<br>3149<br>3150<br>3151<br>3152<br>3153<br>3154<br>3155<br>3156<br>3157<br>3158<br>3159<br>3160<br>3161<br>3162<br>3163<br>3164<br>3165<br>3166<br>3167<br>3168<br>3169<br>3170<br>3171<br>3172<br>3173<br>3174<br>3175<br>3176<br>3177<br>3178<br>3179<br>3180<br>3181<br>3182<br>3183<br>3184<br>3185<br>3186<br>3187<br>3188<br>3189<br>3190<br>3191<br>3192<br>3193<br>3194<br>3195<br>3196<br>3197<br>3198<br>3199<br>3200<br>3201<br>3202<br>3203<br>3204<br>3205<br>3206<br>3207<br>3208<br>3209<br>3210<br>3211<br>3212<br>3213<br>3214<br>3215<br>3216<br>3217<br>3218<br>3219<br>3220<br>3221<br>3222<br>3223<br>3224<br>3225<br>3226<br>3227<br>3228<br>3229<br>3230<br>3231<br>3232<br>3233<br>3234<br>3235<br>3236<br>3237<br>3238<br>3239<br>3240<br>3241<br>3242<br>3243<br>3244<br>3245<br>3246<br>3247<br>3248<br>3249<br>3250<br>3251<br>3252<br>3253<br>3254<br>3255<br>3256<br>3257<br>3258<br>3259<br>3260<br>3261<br>3262<br>3263<br>3264<br>3265<br>3266<br>3267<br>3268<br>3269<br>3270<br>3271<br>3272<br>3273<br>3274<br>3275<br>3276<br>3277<br>3278<br>3279<br>3280<br>3281<br>3282<br>3283<br>3284<br>3285<br>3286<br>3287<br>3288<br>3289<br>3290<br>3291<br>3292<br>3293<br>3294<br>3295<br>3296<br>3297<br>3298<br>3299<br>3300<br>3301<br>3302<br>3303<br>3304<br>3305<br>3306<br>3307<br>3308<br>3309<br>3310<br>3311<br>3312<br>3313<br>3314<br>3315<br>3316<br>3317<br>3318<br>3319<br>3320<br>3321<br>3322<br>3323<br>3324<br>3325<br>3326<br>3327<br>3328<br>3329<br>3330<br>3331<br>3332<br>3333<br>3334<br>3335<br>3336<br>3337<br>3338<br>3339<br>3340<br>3341<br>3342<br>3343<br>3344<br>3345<br>3346<br>3347<br>3348<br>3349<br>3350<br>3351<br>3352<br>3353<br>3354<br>3355<br>3356<br>3357<br>3358<br>3359<br>3360<br>3361<br>3362<br>3363<br>3364<br>3365<br>3366<br>3367<br>3368<br>3369<br>3370<br>3371<br>3372<br>3373<br>3374<br>3375<br>3376<br>3377<br>3378<br>3379<br>3380<br>3381<br>3382<br>3383<br>3384<br>3385<br>3386<br>3387<br>3388<br>3389<br>3390<br>3391<br>3392<br>3393<br>3394<br>3395<br>3396<br>3397<br>3398<br>3399<br>3400<br>3401<br>3402<br>3403<br>3404<br>3405<br>3406<br>3407<br>3408<br>3409<br>3410<br>3411<br>3412<br>3413<br>3414<br>3415<br>3416<br>3417<br>3418<br>3419<br>3420<br>3421<br>3422<br>3423<br>3424<br>3425<br>3426<br>3427<br>3428<br>3429<br>3430<br>3431<br>3432<br>3433<br>3434<br>3435<br>3436<br>3437<br>3438<br>3439<br>3440<br>3441<br>3442<br>3443<br>3444<br>3445<br>3446<br>3447<br>3448<br>3449<br>3450<br>3451<br>3452<br>3453<br>3454<br>3455<br>3456<br>3457<br>3458<br>3459<br>3460<br>3461<br>3462<br>3463<br>3464<br>3465<br>3466<br>3467<br>3468<br>3469<br>3470<br>3471<br>3472<br>3473<br>3474<br>3475<br>3476<br>3477<br>3478<br>3479<br>3480<br>3481<br>3482<br>3483<br>3484<br>3485<br>3486<br>3487<br>3488<br>3489<br>3490<br>3491<br>3492<br>3493<br>3494<br>3495<br>3496<br>3497<br>3498<br>3499<br>3500<br>3501<br>3502<br>3503<br>3504<br>3505<br>3506<br>3507<br>3508<br>3509<br>3510<br>3511<br>3512<br>3513<br>3514<br>3515<br>3516<br>3517<br>3518<br>3519<br>3520<br>3521<br>3522<br>3523<br>3524<br>3525<br>3526<br>3527<br>3528<br>3529<br>3530<br>3531<br>3532<br>3533<br>3534<br>3535<br>3536<br>3537<br>3538<br>3539<br>3540<br>3541<br>3542<br>3543<br>3544<br>3545<br>3546<br>3547<br>3548<br>3549<br>3550<br>3551<br>3552<br>3553<br>3554<br>3555<br>3556<br>3557<br>3558<br>3559<br>3560<br>3561<br>3562<br>3563<br>3564<br>3565<br>3566<br>3567<br>3568<br>3569<br>3570<br>3571<br>3572<br>3573<br>3574<br>3575<br>3576<br>3577<br>3578<br>3579<br>3580<br>3581<br>3582<br>3583<br>3584<br>3585<br>3586<br>3587<br>3588<br>3589<br>3590<br>3591<br>3592<br>3593<br>3594<br>3595<br>3596<br>3597<br>3598<br>3599<br>3600<br>3601<br>3602<br>3603<br>3604<br>3605<br>3606<br>3607<br>3608<br>3609<br>3610<br>3611<br>3612<br>3613<br>3614<br>3615<br>3616<br>3617<br>3618<br>3619<br>3620<br>3621<br>3622<br>3623<br>3624<br>3625<br>3626<br>3627<br>3628<br>3629<br>3630<br>3631<br>3632<br>3633<br>3634<br>3635<br>3636<br>3637<br>3638<br>3639<br>3640<br>3641<br>3642<br>3643<br>3644<br>3645<br>3646<br>3647<br>3648<br>3649<br>3650<br>3651<br>3652<br>3653<br>3654<br>3655<br>3656<br>3657<br>3658<br>3659<br>3660<br>3661<br>3662<br>3663<br>3664<br>3665<br>3666<br>3667<br>3668<br>3669<br>3670<br>3671<br>3672<br>3673<br>3674<br>3675<br>3676<br>3677<br>3678<br>3679<br>3680<br>3681<br>3682<br>3683<br>3684<br>3685<br>3686<br>3687<br>3688<br>3689<br>3690<br>3691<br>3692<br>3693<br>3694<br>3695<br>3696<br>3697<br>3698<br>3699<br>3700<br>3701<br>3702<br>3703<br>3704<br>3705<br>3706<br>3707<br>3708<br>3709<br>3710<br>3711<br>3712<br>3713<br>3714<br>3715<br>3716<br>3717<br>3718<br>3719<br>3720<br>3721<br>3722<br>3723<br>3724<br>3725<br>3726<br>3727<br>3728<br>3729<br>3730<br>3731<br>3732<br>3733<br>3734<br>3735<br>3736<br>3737<br>3738<br>3739<br>3740<br>3741<br>3742<br>3743<br>3744<br>3745<br>3746<br>3747<br>3748<br>3749<br>3750<br>3751<br>3752<br>3753<br>3754<br>3755<br>3756<br>3757<br>3758<br>3759<br>3760<br>3761<br>3762<br>3763<br>3764<br>3765<br>3766<br>3767<br>3768<br>3769<br>3770<br>3771<br>3772<br>3773<br>3774<br>3775<br>3776<br>3777<br>3778<br>3779<br>3780<br>3781<br>3782<br>3783<br>3784<br>3785<br>3786<br>3787<br>3788<br>3789<br>3790<br>3791<br>3792<br>3793<br>3794<br>3795<br>3796<br>3797<br>3798<br>3799<br>3800<br>3801<br>3802<br>3803<br>3804<br>3805<br>3806<br>3807<br>3808<br>3809<br>3810<br>3811<br>3812<br>3813<br>3814<br>3815<br>3816<br>3817<br>3818<br>3819<br>3820<br>3821<br>3822<br>3823<br>3824<br>3825<br>3826<br>3827<br>3828<br>3829<br>3830<br>3831<br>3832<br>3833<br>3834<br>3835<br>3836<br>3837<br>3838<br>3839<br>3840<br>3841<br>3842<br>3843<br>3844<br>3845<br>3846<br>3847<br>3848<br>3849<br>3850<br>3851<br>3852<br>3853<br>3854<br>3855<br>3856<br>3857<br>3858<br>3859<br>3860<br>3861<br>3862<br>3863<br>3864<br>3865<br>3866<br>3867<br>3868<br>3869<br>3870<br>3871<br>3872<br>3873<br>3874<br>3875<br>3876<br>3877<br>3878<br>3879<br>3880<br>3881<br>3882<br>3883<br>3884<br>3885<br>3886<br>3887<br>3888<br>3889<br>3890<br>3891<br>3892<br>3893<br>3894<br>3895<br>3896<br>3897<br>3898<br>3899<br>3900<br>3901<br>3902<br>3903<br>3904<br>3905<br>3906<br>3907<br>3908<br>3909<br>3910<br>3911<br>3912<br>3913<br>3914<br>3915<br>3916<br>3917<br>3918<br>3919<br>3920<br>3921<br>3922<br>3923<br>3924<br>3925<br>3926<br>3927<br>3928<br>3929<br>3930<br>3931<br>3932<br>3933<br>3934<br>3935<br>3936<br>3937<br>3938<br>3939<br>3940<br>3941<br>3942<br>3943<br>3944<br>3945<br>3946<br>3947<br>3948<br>3949<br>3950<br>3951<br>3952<br>3953<br>3954<br>3955<br>3956<br>3957<br>3958<br>3959<br>3960<br>3961<br>3962<br>3963<br>3964<br>3965<br>3966<br>3967<br>3968<br>3969<br>3970<br>3971<br>3972<br>3973<br>3974<br>3975<br>3976<br>3977<br>3978<br>3979<br>3980<br>3981<br>3982<br>3983<br>3984<br>3985<br>3986<br>3987<br>3988<br>3989<br>3990<br>3991<br>3992<br>3993<br>3994<br>3995<br>3996<br>3997<br>3998<br>3999<br>4000<br>4001<br>4002<br>4003<br>4004<br>4005<br>4006<br>4007<br>4008<br>4009<br>4010<br>4011<br>4012<br>4013<br>4014<br>4015<br>4016<br>4017<br>4018<br>4019<br>4020<br>4021<br>4022<br>4023<br>4024<br>4025<br>4026<br>4027<br>4028<br>4029<br>4030<br>4031<br>4032<br>4033<br>4034<br>4035<br>4036<br>4037<br>4038<br>4039<br>4040<br>4041<br>4042<br>4043<br>4044<br>4045<br>4046<br>4047<br>4048<br>4049<br>4050<br>4051<br>4052<br>4053<br>4054<br>4055<br>4056<br>4057<br>4058<br>4059<br>4060<br>4061<br>4062<br>4063<br>4064<br>4065<br>4066<br>4067<br>4068<br>4069<br>4070<br>4071<br>4072<br>4073<br>4074<br>4075<br>4076<br>4077<br>4078<br>4079<br>4080<br>4081<br>4082<br>4083<br>4084<br>4085<br>4086<br>4087<br>4088<br>4089<br>4090<br>4091<br>4092<br>4093<br>4094<br>4095<br>4096<br>4097<br>4098<br>4099<br>4100<br>4101<br>4102<br>4103<br>4104<br>4105<br>4106<br>4107<br>4108<br>4109<br>4110<br>4111<br>4112<br>4113<br>4114<br>4115<br>4116<br>4117<br>4118<br>4119<br>4120<br>4121<br>4122<br>4123<br>4124<br>4125<br>4126<br>4127<br>4128<br>4129<br>4130<br>4131<br>4132<br>4133<br>4134<br>4135<br>4136<br>4137<br>4138<br>4139<br>4140<br>4141<br>4142<br>4143<br>4144<br>4145<br>4146<br>4147<br>4148<br>4149<br>4150<br>4151<br>4152<br>4153<br>4154<br>4155<br>4156<br>4157<br>4158<br>4159<br>4160<br>4161<br>4162<br>4163<br>4164<br>4165<br>4166<br>4167<br>4168<br>4169<br>4170<br>4171<br>4172<br>4173<br>4174<br>4175<br>4176<br>4177<br>4178<br>4179<br>4180<br>4181<br>4182<br>4183<br>4184<br>4185<br>4186<br>4187<br>4188<br>4189<br>4190<br>4191<br>4192<br>4193<br>4194<br>4195<br>4196<br>4197<br>4198<br>4199<br>4200<br>4201<br>4202<br>4203<br>4204<br>4205<br>4206<br>4207<br>4208<br>4209<br>4210<br>4211<br>4212<br>4213<br>4214<br>4215<br>4216<br>4217<br>4218<br>4219<br>4220<br>4221<br>4222<br>4223<br>4224<br>4225<br>4226<br>4227<br>4228<br>4229<br>4230<br>4231<br>4232<br>4233<br>4234<br>4235<br>4236<br>4237<br>4238<br>4239<br>4240<br>4241<br>4242<br>4243<br>4244<br>4245<br>4246<br>4247<br>4248<br>4249<br>4250<br>4251<br>4252<br>4253<br>4254<br>4255<br>4256<br>4257<br>4258<br>4259<br>4260<br>4261<br>4262<br>4263<br>4264<br>4265<br>4266<br>4267<br>4268<br>4269<br>4270<br>4271<br>4272<br>4273<br>4274<br>4275<br>4276<br>4277<br>4278<br>4279<br>4280<br>4281<br>4282<br>4283<br>4284<br>4285<br>4286<br>4287<br>4288<br>4289<br>4290<br>4291<br>4292<br>4293<br>4294<br>4295<br>4296<br>4297<br>4298<br>4299<br>4300<br>4301<br>4302<br>4303<br>4304<br>4305<br>4306<br>4307<br>4308<br>4309<br>4310<br>4311<br>4312<br>4313<br>4314<br>4315<br>4316<br>4317<br>4318<br>4319<br>4320<br>4321<br>4322<br>4323<br>4324<br>4325<br>4326<br>4327<br>4328<br>4329<br>4330<br>4331<br>4332<br>4333<br>4334<br>4335<br>4336<br>4337<br>4338<br>4339<br>4340<br>4341<br>4342<br>4343<br>4344<br>4345<br>4346<br>4347<br>4348<br>4349<br>4350<br>4351<br>4352<br>4353<br>4354<br>4355<br>4356<br>4357<br>4358<br>4359<br>4360<br>4361<br>4362<br>4363<br>4364<br>4365<br>4366<br>4367<br>4368<br>4369<br>4370<br>4371<br>4372<br>4373<br>4374<br>4375<br>4376<br>4377<br>4378<br>4379<br>4380<br>4381<br>4382<br>4383<br>4384<br>4385<br>4386<br>4387<br>4388<br>4389<br>4390<br>4391<br>4392<br>4393<br>4394<br>4395<br>4396<br>4397<br>4398<br>4399<br>4400<br>4401<br>4402<br>4403<br>4404<br>4405<br>4406<br>4407<br>4408<br>4409<br>4410<br>4411<br>4412<br>4413<br>4414<br>4415<br>4416<br>4417<br>4418<br>4419<br>4420<br>4421<br>4422<br>4423<br>4424<br>4425<br>4426<br>4427<br>4428<br>4429<br>4430<br>4431<br>4432<br>4433<br>4434<br>4435<br>4436<br>4437<br>4438<br>4439<br>4440<br>4441<br>4442<br>4443<br>4444<br>4445<br>4446<br>4447<br>4448<br>4449<br>4450<br>4451<br>4452<br>4453<br>4454<br>4455<br>4456<br>4457<br>4458<br>4459<br>4460<br>4461<br>4462<br>4463<br>4464<br>4465<br>4466<br>4467<br>4468<br>4469<br>4470<br>4471<br>4472<br>4473<br>4474<br>4475<br>4476<br>4477<br>4478<br>4479<br>4480<br>4481<br>4482<br>4483<br>4484<br>4485<br>4486<br>4487<br>4488<br>4489<br>4490<br>4491<br>4492<br>4493<br>4494<br>4495<br>4496<br>4497<br>4498<br>4499<br>4500<br>4501<br>4502<br>4503<br>4504<br>4505<br>4506<br>4507<br>4508<br>4509<br>4510<br>4511<br>4512<br>4513<br>4514<br>4515<br>4516<br>4517<br>4518<br>4519<br>4520<br>4521<br>4522<br>4523<br>4524<br>4525<br>4526<br>4527<br>4528<br>4529<br>4530<br>4531<br>4532<br>4533<br>4534<br>4535<br>4536<br>4537<br>4538<br>4539<br>4540<br>4541<br>4542<br>4543<br>4544<br>4545<br>4546<br>4547<br>4548<br>4549<br>4550<br>4551<br>4552<br>4553<br>4554<br>4555<br>4556<br>4557<br>4558<br>4559<br>4560<br>4561<br>4562<br>4563<br>4564<br>4565<br>4566<br>4567<br>4568<br>4569<br>4570<br>4571<br>4572<br>4573<br>4574<br>4575<br>4576<br>4577<br>4578<br>4579<br>4580<br>4581<br>4582<br>4583<br>4584<br>4585<br>4586<br>4587<br>4588<br>4589<br>4590<br>4591<br>4592<br>4593<br>4594<br>4595<br>4596<br>4597<br>4598<br>4599<br>4600<br>4601<br>4602<br>4603<br>4604<br>4605<br>4606<br>4607<br>4608<br>4609<br>4610<br>4611<br>4612<br>4613<br>4614<br>4615<br>4616<br>4617<br>4618<br>4619<br>4620<br>4621<br>4622<br>4623<br>4624<br>4625<br>4626<br>4627<br>4628<br>4629<br>4630<br>4631<br>4632<br>4633<br>4634<br>4635<br>4636<br>4637<br>4638<br>4639<br>4640<br>4641<br>4642<br>4643<br>4644<br>4645<br>4646<br>4647<br>4648<br>4649<br>4650<br>4651<br>4652<br>4653<br>4654<br>4655<br>4656<br>4657<br>4658<br>4659<br>4660<br>4661<br>4662<br>4663<br>4664<br>4665<br>4666<br>4667<br>4668<br>4669<br>4670<br>4671<br>4672<br>4673<br>4674<br>4675<br>4676<br>4677<br>4678<br>4679<br>4680<br>4681<br>4682<br>4683<br>4684<br>4685<br>4686<br>4687<br>4688<br>4689<br>4690<br>4691<br>4692<br>4693<br>4694<br>4695<br>4696<br>4697<br>4698<br>4699<br>4700<br>4701<br>4702<br>4703<br>4704<br>4705<br>4706<br>4707<br>4708<br>4709<br>4710<br>4711<br>4712<br>4713<br>4714<br>4715<br>4716<br>4717<br>4718<br>4719<br>4720<br>4721<br>4722<br>4723<br>4724<br>4725<br>4726<br>4727<br>4728<br>4729<br>4730<br>4731<br>4732<br>4733<br>4734<br>4735<br>4736<br>4737<br>4738<br>4739<br>4740<br>4741<br>4742<br>4743<br>4744<br>4745<br>4746<br>4747<br>4748<br>4749<br>4750<br>4751<br>4752<br>4753<br>4754<br>4755<br>4756<br>4757<br>4758<br>4759<br>4760<br>4761<br>4762<br>4763<br>4764<br>4765<br>4766<br>4767<br>4768<br>4769<br>4770<br>4771<br>4772<br>4773<br>4774<br>4775<br>4776<br>4777<br>4778<br>4779<br>4780<br>4781<br>4782<br>4783<br>4784<br>4785<br>4786<br>4787<br>4788<br>4789<br>4790<br>4791<br>4792<br>4793<br>4794<br>4795<br>4796<br>4797<br>4798<br>4799<br>4800<br>4801<br>4802<br>4803<br>4804<br>4805<br>4806<br>4807<br>4808<br>4809<br>4810<br>4811<br>4812<br>4813<br>4814<br>4815<br>4816<br>4817<br>4818<br>4819<br>4820<br>4821<br>4822<br>4823<br>4824<br>4825<br>4826<br>4827<br>4828<br>4829<br>4830<br>4831<br>4832<br>4833<br>4834<br>4835<br>4836<br>4837<br>4838<br>4839<br>4840<br>4841<br>4842<br>4843<br>4844<br>4845<br>4846<br>4847<br>4848<br>4849<br>4850<br>4851<br>4852<br>4853<br>4854<br>4855<br>4856<br>4857<br>4858<br>4859<br>4860<br>4861<br>4862<br>4863<br>4864<br>4865<br>4866<br>4867<br>4868<br>4869<br>4870<br>4871<br>4872<br>4873<br>4874<br>4875<br>4876<br>4877<br>4878<br>4879<br>4880<br>4881<br>4882<br>4883<br>4884<br>4885<br>4886<br>4887<br>4888<br>4889<br>4890<br>4891<br>4892<br>4893<br>4894<br>4895<br>4896<br>4897<br>4898<br>4899<br>4900<br>4901<br>4902<br>4903<br>4904<br>4905<br>4906<br>4907<br>4908<br>4909<br>4910<br>4911<br>4912<br>4913<br>4914<br>4915<br>4916<br>4917<br>4918<br>4919<br>4920<br>4921<br>4922<br>4923<br>4924<br>4925<br>4926<br>4927<br>4928<br>4929<br>4930<br>4931<br>4932<br>4933<br>4934<br>4935<br>4936<br>4937<br>4938<br>4939<br>4940<br>4941<br>4942<br>4943<br>4944<br>4945<br>4946<br>4947<br>4948<br>4949<br>4950<br>4951<br>4952<br>4953<br>4954<br>4955<br>4956<br>4957<br>4958<br>4959<br>4960<br>4961<br>4962<br>4963<br>4964<br>4965<br>4966<br>4967<br>4968<br>4969<br>4970<br>4971<br>4972<br>4973<br>4974<br>4975<br>4976<br>4977<br>4978<br>4979<br>4980<br>4981<br>4982<br>4983<br>4984<br>4985<br>4986<br>4987<br>4988<br>4989<br>4990<br>4991<br>4992<br>4993<br>4994<br>4995<br>4996<br>4997<br>4998<br>4999<br>5000<br>5001<br>5002<br>5003<br>5004<br>5005<br>5006<br>5007<br>5008<br>5009<br>5010<br>5011<br>5012<br>5013<br>5014<br>5015<br>5016<br>5017<br>5018<br>5019<br>5020<br>5021<br>5022<br>5023<br>5024<br>5025<br>5026<br>5027<br>5028<br>5029<br>5030<br>5031<br>5032<br>5033<br>5034<br>5035<br>5036<br>5037<br>5038<br>5039<br>5040<br>5041<br>5042<br>5043<br>5044<br>5045<br>5046<br>5047<br>5048<br>5049<br>5050<br>5051<br>5052<br>5053<br>5054<br>5055<br>5056<br>5057<br>5058<br>5059<br>5060<br>5061<br>5062<br>5063<br>5064<br>5065<br>5066<br>5067<br>5068<br>5069<br>5070<br>5071<br>5072<br>5073<br>5074<br>5075<br>5076<br>5077<br>5078<br>5079<br>5080<br>5081<br>5082<br>5083<br>5084<br>5085<br>5086<br>5087<br>5088<br>5089<br>5090<br>5091<br>5092<br>5093<br>5094<br>5095<br>5096<br>5097<br>5098<br>5099<br>5100<br>5101<br>5102<br>5103<br>5104<br>5105<br>5106<br>5107<br>5108<br>5109<br>5110<br>5111<br>5112<br>5113<br>5114<br>5115<br>5116<br>5117<br>5118<br>5119<br>5120<br>5121<br>5122<br>5123<br>5124<br>5125<br>5126<br>5127<br>5128<br>5129<br>5130<br>5131<br>5132<br>5133<br>5134<br>5135<br>5136<br>5137<br>5138<br>5139<br>5140<br>5141<br>5142<br>5143<br>5144<br>5145<br>5146<br>5147<br>5148<br>5149<br>5150<br>5151<br>5152<br>5153<br>5154<br>5155<br>5156<br>5157<br>5158<br>5159<br>5160<br>5161<br>5162<br>5163<br>5164<br>5165<br>5166<br>5167<br>5168<br>5169<br>5170<br>5171<br>5172<br>5173<br>5174<br>5175<br>5176<br>5177<br>5178<br>5179<br>5180<br>5181<br>5182<br>5183<br>5184<br>5185<br>5186<br>5187<br>5188<br>5189<br>5190<br>5191<br>5192<br>5193<br>5194<br>5195<br>5196<br>5197<br>5198<br>5199<br>5200<br>5201<br>5202<br>5203<br>5204<br>5205<br>5206<br>5207<br>5208<br>5209<br>5210<br>5211<br>5212<br>5213<br>5214<br>5215<br>5216<br>5217<br>5218<br>5219<br>5220<br>5221<br>5222<br>5223<br>5224<br>5225<br>5226<br>5227<br>5228<br>5229<br>5230<br>5231<br>5232<br>5233<br>5234<br>5235<br>5236<br>5237<br>5238<br>5239<br>5240<br>5241<br>5242<br>5243<br>5244<br>5245<br>5246<br>5247<br>5248<br>5249<br>5250<br>5251<br>5252<br>5253<br>5254<br>5255<br>5256<br>5257<br>5258<br>5259<br>5260<br>5261<br>5262<br>5263<br>5264<br>5265<br>5266<br>5267<br>5268<br>5269<br>5270<br>5271<br>5272<br>5273<br>5274<br>5275<br>5276<br>5277<br>5278<br>5279<br>5280<br>5281<br>5282<br>5283<br>5284<br>5285<br>5286<br>5287<br>5288<br>5289<br>5290<br>5291<br>5292<br>5293<br>5294<br>5295<br>5296<br>5297<br>5298<br>5299<br>5300<br>5301<br>5302<br>5303<br>5304<br>5305<br>5306<br>5307<br>5308<br>5309<br>5310<br>5311<br>5312<br>5313<br>5314<br>5315<br>5316<br>5317<br>5318<br>5319<br>5320<br>5321<br>5322<br>5323<br>5324<br>5325<br>5326<br>5327<br>5328<br>5329<br>5330<br>5331<br>5332<br>5333<br>5334<br>5335<br>5336<br>5337<br>5338<br>5339<br>5340<br>5341<br>5342<br>5343<br>5344<br>5345<br>5346<br>5347<br>5348<br>5349<br>5350<br>5351<br>5352<br>5353<br>5354<br>5355<br>5356<br>5357<br>5358<br>5359<br>5360<br>5361<br>5362<br>5363<br>5364<br>5365<br>5366<br>5367<br>5368<br>5369<br>5370<br>5371<br>5372<br>5373<br>5374<br>5375<br>5376<br>5377<br>5378<br>5379<br>5380<br>5381<br>5382<br>5383<br>5384<br>5385<br>5386<br>5387<br>5388<br>5389<br>5390<br>5391<br>5392<br>5393<br>5394<br>5395<br>5396<br>5397<br>5398<br>5399<br>5400<br>5401<br>5402<br>5403<br>5404<br>5405<br>5406<br>5407<br>5408<br>5409<br>5410<br>5411<br>5412<br>5413<br>5414<br>5415<br>5416<br>5417<br>5418<br>5419<br>5420<br>5421<br>5422<br>5423<br>5424<br>5425<br>5426<br>5427<br>5428<br>5429<br>5430<br>5431<br>5432<br>5433<br>5434<br>5435<br>5436<br>5437<br>5438<br>5439<br>5440<br>5441<br>5442<br>5443<br>5444<br>5445<br>5446<br>5447<br>5448<br>5449<br>5450<br>5451<br>5452<br>5453<br>5454<br>5455<br>5456<br>5457<br>5458<br>5459<br>5460<br>5461<br>5462<br>5463<br>5464<br>5465<br>5466<br>5467<br>5468<br>5469<br>5470<br>5471<br>5472<br>5473<br>5474<br>5475<br>5476<br>5477<br>5478<br>5479<br>5480<br>5481<br>5482<br>5483<br>5484<br>5485<br>5486<br>5487<br>5488<br>5489<br>5490<br>5491<br>5492<br>5493<br>5494<br>5495<br>5496<br>5497<br>5498<br>5499<br>5500<br>5501<br>5502<br>5503<br>5504<br>5505<br>5506<br>5507<br>5508<br>5509<br>5510<br>5511<br>5512<br>5513<br>5514<br>5515<br>5516<br>5517<br>5518<br>5519<br>5520<br>5521<br>5522<br>5523<br>5524<br>5525<br>5526<br>5527<br>5528<br>5529<br>5530<br>5531<br>5532<br>5533<br>5534<br>5535<br>5536<br>5537<br>5538<br>5539<br>5540<br>5541<br>5542<br>5543<br>5544<br>5545<br>5546<br>5547<br>5548<br>5549<br>5550<br>5551<br>5552<br>5553<br>5554<br>5555<br>5556<br>5557<br>5558<br>5559<br>5560<br>5561<br>5562<br>5563<br>5564<br>5565<br>5566<br>5567<br>5568<br>5569<br>5570<br>5571<br>5572<br>5573<br>5574<br>5575<br>5576<br>5577<br>5578<br>5579<br>5580<br>5581<br>5582<br>5583<br>5584<br>5585<br>5586<br>5587<br>5588<br>5589<br>5590<br>5591<br>5592<br>5593<br>5594<br>5595<br>5596<br>5597<br>5598<br>5599<br>5600<br>5601<br>5602<br>5603<br>5604<br>5605<br>5606<br>5607<br>5608<br>5609<br>5610<br>5611<br>5612<br>5613<br>5614<br>5615<br>5616<br>5617<br>5618<br>5619<br>5620<br>5621<br>5622<br>5623<br>5624<br>5625<br>5626<br>5627<br>5628<br>5629<br>5630<br>5631<br>5632<br>5633<br>5634<br>5635<br>5636<br>5637<br>5638<br>5639<br>5640<br>5641<br>5642<br>5643<br>5644<br>5645<br>5646<br>5647<br>5648<br>5649<br>5650<br>5651<br>5652<br>5653<br>5654<br>5655<br>5656<br>5657<br>5658<br>5659<br>5660<br>5661<br>5662<br>5663<br>5664<br>5665<br>5666<br>5667<br>5668<br>5669<br>5670<br>5671<br>5672<br>5673<br>5674<br>5675<br>5676<br>5677<br>5678<br>5679<br>5680<br>5681<br>5682<br>5683<br>5684<br>5685<br>5686<br>5687<br>5688<br>5689<br>5690<br>5691<br>5692<br>5693<br>5694<br>5695<br>5696<br>5697<br>5698<br>5699<br>5700<br>5701<br>5702<br>5703<br>5704<br>5705<br>5706<br>5707<br>5708<br>5709<br>5710<br>5711<br>5712<br>5713<br>5714<br>5715<br>5716<br>5717<br>5718<br>5719<br>5720<br>5721<br>5722<br>5723<br>5724<br>5725<br>5726<br>5727<br>5728<br>5729<br>5730<br>5731<br>5732<br>5733<br>5734<br>5735<br>5736<br>5737<br>5738<br>5739<br>5740<br>5741<br>5742<br>5743<br>5744<br>5745<br>5746<br>5747<br>5748<br>5749<br>5750<br>5751<br>5752<br>5753<br>5754<br>5755<br>5756<br>5757<br>5758<br>5759<br>5760<br>5761<br>5762<br>5763<br>5764<br>5765<br>5766<br>5767<br>5768<br>5769<br>5770<br>5771<br>5772<br>5773<br>5774<br>5775<br>5776<br>5777<br>5778<br>5779<br>5780<br>5781<br>5782<br>5783<br>5784<br>5785<br>5786<br>5787<br>5788<br>5789<br>5790<br>5791<br>5792<br>5793<br>5794<br>5795<br>5796<br>5797<br>5798<br>5799<br>5800<br>5801<br>5802<br>5803<br>5804<br>5805<br>5806<br>5807<br>5808<br>5809<br>5810<br>5811<br>5812<br>5813<br>5814<br>5815<br>5816<br>5817<br>5818<br>5819<br>5820<br>5821<br>5822<br>5823<br>5824<br>5825<br>5826<br>5827<br>5828<br>5829<br>5830<br>5831<br>5832<br>5833<br>5834<br>5835<br>5836<br>5837<br>5838<br>5839<br>5840<br>5841<br>5842<br>5843<br>5844<br>5845<br>5846<br>5847<br>5848<br>5849<br>5850<br>5851<br>5852<br>5853<br>5854<br>5855<br>5856<br>5857<br>5858<br>5859<br>5860<br>5861<br>5862<br>5863<br>5864<br>5865<br>5866<br>5867<br>5868<br>5869<br>5870<br>5871<br>5872<br>5873<br>5874<br>5875<br>5876<br>5877<br>5878<br>5879<br>5880<br>5881<br>5882<br>5883<br>5884<br>5885<br>5886<br>5887<br>5888<br>5889<br>5890<br>5891<br>5892<br>5893<br>5894<br>5895<br>5896<br>5897<br>5898<br>5899<br>5900<br>5901<br>5902<br>5903<br>5904<br>5905<br>5906<br>5907<br>5908<br>5909<br>5910<br>5911<br>5912<br>5913<br>5914<br>5915<br>5916<br>5917<br>5918<br>5919<br>5920<br>5921<br>5922<br>5923<br>5924<br>5925<br>5926<br>5927<br>5928<br>5929<br>5930<br>5931<br>5932<br>5933<br>5934<br>5935<br>5936<br>5937<br>5938<br>5939<br>5940<br>5941<br>5942<br>5943<br>5944<br>5945<br>5946<br>5947<br>5948<br>5949<br>5950<br>5951<br>5952<br>5953<br>5954<br>5955<br>5956<br>5957<br>5958<br>5959<br>5960<br>5961<br>5962<br>5963<br>5964<br>5965<br>5966<br>5967<br>5968<br>5969<br>5970<br>5971<br>5972<br>5973<br>5974<br>5975<br>5976<br>5977<br>5978<br>5979<br>5980<br>5981<br>5982<br>5983<br>5984<br>5985<br>5986<br>5987<br>5988<br>5989<br>5990<br>5991<br>5992<br>5993<br>5994<br>5995<br>5996<br>5997<br>5998<br>5999<br>6000<br>6001<br>6002<br>6003<br>6004<br>6005<br>6006<br>6007<br>6008<br>6009<br>6010<br>6011<br>6012<br>6013<br>6014<br>6015<br>6016<br>6017<br>6018<br>6019<br>6020<br>6021<br>6022<br>6023<br>6024<br>6025<br>6026<br>6027<br>6028<br>6029<br>6030<br>6031<br>6032<br>6033<br>6034<br>6035<br>6036<br>6037<br>6038<br>6039<br>6040<br>6041<br>6042<br>6043<br>6044<br>6045<br>6046<br>6047<br>6048<br>6049<br>6050<br>6051<br>6052<br>6053<br>6054<br>6055<br>6056<br>6057<br>6058<br>6059<br>6060<br>6061<br>6062<br>6063<br>6064<br>6065<br>6066<br>6067<br>6068<br>6069<br>6070<br>6071<br>6072<br>6073<br>6074<br>6075<br>6076<br>6077<br>6078<br>6079<br>6080<br>6081<br>6082<br>6083<br>6084<br>6085<br>6086<br>6087<br>6088<br>6089<br>6090<br>6091<br>6092<br>6093<br>6094<br>6095<br>6096<br>6097<br>6098<br>6099<br>6100<br>6101<br>6102<br>6103<br>6104<br>6105<br>6106<br>6107<br>6108<br>6109<br>6110<br>6111<br>6112<br>6113<br>6114<br>6115<br>6116<br>6117<br>6118<br>6119<br>6120<br>6121<br>6122<br>6123<br>6124<br>6125<br>6126<br>6127<br>6128<br>6129<br>6130<br>6131<br>6132<br>6133<br>6134<br>6135<br>6136<br>6137<br>6138<br>6139<br>6140<br>6141<br>6142<br>6143<br>6144<br>6145<br>6146<br>6147<br>6148<br>6149<br>6150<br>6151<br>6152<br>6153<br>6154<br>6155<br>6156<br>6157<br>6158<br>6159<br>6160<br>6161<br>6162<br>6163<br>6164<br>6165<br>6166<br>6167<br>6168<br>6169<br>6170<br>6171<br>6172<br>6173<br>6174<br>6175<br>6176<br>6177<br>6178<br>6179<br>6180<br>6181<br>6182<br>6183<br>6184<br>6185<br>6186<br>6187<br>6188<br>6189<br>6190<br>6191<br>6192<br>6193<br>6194<br>6195<br>6196<br>6197<br>6198<br>6199<br>6200<br>6201<br>6202<br>6203<br>6204<br>6205<br>6206<br>6207<br>6208<br>6209<br>6210<br>6211<br>6212<br>6213<br>6214<br>6215<br>6216<br>6217<br>6218<br>6219<br>6220<br>6221<br>6222<br>6223<br>6224<br>6225<br>6226<br>6227<br>6228<br>6229<br>6230<br>6231<br>6232<br>6233<br>6234<br>6235<br>6236<br>6237<br>6238<br>6239<br>6240<br>6241<br>6242<br>6243<br>6244<br>6245<br>6246<br>6247<br>6248<br>6249<br>6250<br>6251<br>6252<br>6253<br>6254<br>6255<br>6256<br>6257<br>6258<br>6259<br>6260<br>6261<br>6262<br>6263<br>6264<br>6265<br>6266<br>6267<br>6268<br>6269<br>6270<br>6271<br>6272<br>6273<br>6274<br>6275<br>6276<br>6277<br>6278<br>6279<br>6280<br>6281<br>6282<br>6283<br>6284<br>6285<br>6286<br>6287<br>6288<br>6289<br>6290<br>6291<br>6292<br>6293<br>6294<br>6295<br>6296<br>6297<br>6298<br>6299<br>6300<br>6301<br>6302<br>6303<br>6304<br>6305<br>6306<br>6307<br>6308<br>6309<br>6310<br>6311<br>6312<br>6313<br>6314<br>6315<br>6316<br>6317<br>6318<br>6319<br>6320<br>6321<br>6322<br>6323<br>6324<br>6325<br>6326<br>6327<br>6328<br>6329<br>6330<br>6331<br>6332<br>6333<br>6334<br>6335<br>6336<br>6337<br>6338<br>6339<br>6340<br>6341<br>6342<br>6343<br>6344<br>6345<br>6346<br>6347<br>6348<br>6349<br>6350<br>6351<br>6352<br>6353<br>6354<br>6355<br>6356<br>6357<br>6358<br>6359<br>6360<br>6361<br>6362<br>6363<br>6364<br>6365<br>6366<br>6367<br>6368<br>6369<br>6370<br>6371<br>6372<br>6373<br>6374<br>6375<br>6376<br>6377<br>6378<br>6379<br>6380<br>6381<br>6382<br>6383<br>6384<br>6385<br>6386<br>6387<br>6388<br>6389<br>6390<br>6391<br>6392<br>6393<br>6394<br>6395<br>6396<br>6397<br>6398<br>6399<br>6400<br>6401<br>6402<br>6403<br>6404<br>6405<br>6406<br>6407<br>6408<br>6409<br>6410<br>6411<br>6412<br>6413<br>6414<br>6415<br>6416<br>6417<br>6418<br>6419<br>6420<br>6421<br>6422<br>6423<br>6424<br>6425<br>6426<br>6427<br>6428<br>6429<br>6430<br>6431<br>6432<br>6433<br>6434<br>6435<br>6436<br>6437<br>6438<br>6439<br>6440<br>6441<br>6442<br>6443<br>6444<br>6445<br>6446<br>6447<br>6448<br>6449<br>6450<br>6451<br>6452<br>6453<br>6454<br>6455<br>6456<br>6457<br>6458<br>6459<br>6460<br>6461<br>6462<br>6463<br>6464<br>6465<br>6466<br>6467<br>6468<br>6469<br>6470<br>6471<br>6472<br>6473<br>6474<br>6475<br>6476<br>6477<br>6478<br>6479<br>6480<br>6481<br>6482<br>6483<br>6484<br>6485<br>6486<br>6487<br>6488<br>6489<br>6490<br>6491<br>6492<br>6493<br>6494<br>6495<br>6496<br>6497<br>6498<br>6499<br>6500<br>6501<br>6502<br>6503<br>6504<br>6505<br>6506<br>6507<br>6508<br>6509<br>6510<br>6511<br>6512<br>6513<br>6514<br>6515<br>6516<br>6517<br>6518<br>6519<br>6520<br>6521<br>6522<br>6523<br>6524<br>6525<br>6526<br>6527<br>6528<br>6529<br>6530<br>6531<br>6532<br>6533<br>6534<br>6535<br>6536<br>6537<br>6538<br>6539<br>6540<br>6541<br>6542<br>6543<br>6544<br>6545<br>6546<br>6547<br>6548<br>6549<br>6550<br>6551<br>6552<br>6553<br>6554<br>6555<br>6556<br>6557<br>6558<br>6559<br>6560<br>6561<br>6562<br>6563<br>6564<br>6565<br>6566<br>6567<br>6568<br>6569<br>6570<br>6571<br>6572<br>6573<br>6574<br>6575<br>6576<br>6577<br>6578<br>6579<br>6580<br>6581<br>6582<br>6583<br>6584<br>6585<br>6586<br>6587<br>6588<br>6589<br>6590<br>6591<br>6592<br>6593<br>6594<br>6595<br>6596<br>6597<br>6598<br>6599<br>6600<br>6601<br>6602<br>6603<br>6604<br>6605<br>6606<br>6607<br>6608<br>6609<br>6610<br>6611<br>6612<br>6613<br>6614<br>6615<br>6616<br>6617<br>6618<br>6619<br>6620<br>6621<br>6622<br>6623<br>6624<br>6625<br>6626<br>6627<br>6628<br>6629<br>6630<br>6631<br>6632<br>6633<br>6634<br>6635<br>6636<br>6637<br>6638<br>6639<br>6640<br>6641<br>6642<br>6643<br>6644<br>6645<br>6646<br>6647<br>6648<br>6649<br>6650<br>6651<br>6652<br>6653<br>6654<br>6655<br>6656<br>6657<br>6658<br>6659<br>6660<br>6661<br>6662<br>6663<br>6664<br>6665<br>6666<br>6667<br>6668<br>6669<br>6670<br>6671<br>6672<br>6673<br>6674<br>6675<br>6676<br>6677<br>6678<br>6679<br>6680<br>6681<br>6682<br>6683<br>6684<br>6685<br>6686<br>6687<br>6688<br>6689<br>6690<br>6691<br>6692<br>6693<br>6694<br>6695<br>6696<br>6697<br>6698<br>6699<br>6700<br>6701<br>6702<br>6703<br>6704<br>6705<br>6706<br>6707<br>6708<br>6709<br>6710<br>6711<br>6712<br>6713<br>6714<br>6715<br>6716<br>6717<br>6718<br>6719<br>6720<br>6721<br>6722<br>6723<br>6724<br>6725<br>6726<br>6727<br>6728<br>6729<br>6730<br>6731<br>6732<br>6733<br>6734<br>6735<br>6736<br>6737<br>6738<br>6739<br>6740<br>6741<br>6742<br>6743<br>6744<br>6745<br>6746<br>6747<br>6748<br>6749<br>6750<br>6751<br>6752<br>6753<br>6754<br>6755<br>6756<br>6757<br>6758<br>6759<br>6760<br>6761<br>6762<br>6763<br>6764<br>6765<br>6766<br>6767<br>6768<br>6769<br>6770<br>6771<br>6772<br>6773<br>6774<br>6775<br>6776<br>6777<br>6778<br>6779<br>6780<br>6781<br>6782<br>6783<br>6784<br>6785<br>6786<br>6787<br>6788<br>6789<br>6790<br>6791<br>6792<br>6793<br>6794<br>6795<br>6796<br>6797<br>6798<br>6799<br>6800<br>6801<br>6802<br>6803<br>6804<br>6805<br>6806<br>6807<br>6808<br>6809<br>6810<br>6811<br>6812<br>6813<br>6814<br>6815<br>6816<br>6817<br>6818<br>6819<br>6820<br>6821<br>6822<br>6823<br>6824<br>6825<br>6826<br>6827<br>6828<br>6829<br>6830<br>6831<br>6832<br>6833<br>6834<br>6835<br>6836<br>6837<br>6838<br>6839<br>6840<br>6841<br>6842<br>6843<br>6844<br>6845<br>6846<br>6847<br>6848<br>6849<br>6850<br>6851<br>6852<br>6853<br>6854<br>6855<br>6856<br>6857<br>6858<br>6859<br>6860<br>6861<br>6862<br>6863<br>6864<br>6865<br>6866<br>6867<br>6868<br>6869<br>6870<br>6871<br>6872<br>6873<br>6874<br>6875<br>6876<br>6877<br>6878<br>6879<br>6880<br>6881<br>6882<br>6883<br>6884<br>6885<br>6886<br>6887<br>6888<br>6889<br>6890<br>6891<br>6892<br>6893<br>6894<br>6895<br>6896<br>6897<br>6898<br>6899<br>6900<br>6901<br>6902<br>6903<br>6904<br>6905<br>6906<br>6907<br>6908<br>6909<br>6910<br>6911<br>6912<br>6913<br>6914<br>6915<br>6916<br>6917<br>6918<br>6919<br>6920<br>6921<br>6922<br>6923<br>6924<br>6925<br>6926<br>6927<br>6928<br>6929<br>6930<br>6931<br>6932<br>6933<br>6934<br>6935<br>6936<br>6937<br>6938<br>6939<br>6940<br>6941<br>6942<br>6943<br>6944<br>6945<br>6946<br>6947<br>6948<br>6949<br>6950<br>6951<br>6952<br>6953<br>6954<br>6955<br>6956<br>6957<br>6958<br>6959<br>6960<br>6961<br>6962<br>6963<br>6964<br>6965<br>6966<br>6967<br>6968<br>6969<br>6970<br>6971<br>6972<br>6973<br>6974<br>6975<br>6976<br>6977<br>6978<br>6979<br>6980<br>6981<br>6982<br>6983<br>6984<br>6985<br>6986<br>6987<br>6988<br>6989<br>6990<br>6991<br>6992<br>6993<br>6994<br>6995<br>6996<br>6997<br>6998<br>6999<br>7000<br>7001<br>7002<br>7003<br>7004<br>7005<br>7006<br>7007<br>7008<br>7009<br>7010<br>7011<br>7012<br>7013<br>7014<br>7015<br>7016<br>7017<br>7018<br>7019<br>7020<br>7021<br>7022<br>7023<br>7024<br>7025<br>7026<br>7027<br>7028<br>7029<br>7030<br>7031<br>7032<br>7033<br>7034<br>7035<br>7036<br>7037<br>7038<br>7039<br>7040<br>7041<br>7042<br>7043<br>7044<br>7045<br>7046<br>7047<br>7048<br>7049<br>7050<br>7051<br>7052<br>7053<br>7054<br>7055<br>7056<br>7057<br>7058<br>7059<br>7060<br>7061<br>7062<br>7063<br>7064<br>7065<br>7066<br>7067<br>7068<br>7069<br>7070<br>7071<br>7072<br>7073<br>7074<br>7075<br>7076<br>7077<br>7078<br>7079<br>7080<br>7081<br>7082<br>7083<br>7084<br>7085<br>7086<br>7087<br>7088<br>7089<br>7090<br>7091<br>7092<br>7093<br>7094<br>7095<br>7096<br>7097<br>7098<br>7099<br>7100<br>7101<br>7102<br>7103<br>7104<br>7105<br>7106<br>7107<br>7108<br>7109<br>7110<br>7111<br>7112<br>7113<br>7114<br>7115<br>7116<br>7117<br>7118<br>7119<br>7120<br>7121<br>7122<br>7123<br>7124<br>7125<br>7126<br>7127<br>7128<br>7129<br>7130<br>7131<br>7132<br>7133<br>7134<br>7135<br>7136<br>7137<br>7138<br>7139<br>7140<br>7141<br>7142<br>7143<br>7144<br>7145<br>7146<br>7147<br>7148<br>7149<br>7150<br>7151<br>7152<br>7153<br>7154<br>7155<br>7156<br>7157<br>7158<br>7159<br>7160<br>7161<br>7162<br>7163<br>7164<br>7165<br>7166<br>7167<br>7168<br>7169<br>7170<br>7171<br>7172<br>7173<br>7174<br>7175<br>7176<br>7177<br>7178<br>7179<br>7180<br>7181<br>7182<br>7183<br>7184<br>7185<br>7186<br>7187<br>7188<br>7189<br>7190<br>7191<br>7192<br>7193<br>7194<br>7195<br>7196<br>7197<br>7198<br>7199<br>7200<br>7201<br>7202<br>7203<br>7204<br>7205<br>7206<br>7207<br>7208<br>7209<br>7210<br>7211<br>7212<br>7213<br>7214<br>7215<br>7216<br>7217<br>7218<br>7219<br>7220<br>7221<br>7222<br>7223<br>7224<br>7225<br>7226<br>7227<br>7228<br>7229<br>7230<br>7231<br>7232<br>7233<br>7234<br>7235<br>7236<br>7237<br>7238<br>7239<br>7240<br>7241<br>7242<br>7243<br>7244<br>7245<br>7246<br>7247<br>7248<br>7249<br>7250<br>7251<br>7252<br>7253<br>7254<br>7255<br>7256<br>7257<br>7258<br>7259<br>7260<br>7261<br>7262<br>7263<br>7264<br>7265<br>7266<br>7267<br>7268<br>7269<br>7270<br>7271<br>7272<br>7273<br>7274<br>7275<br>7276<br>7277<br>7278<br>7279<br>7280<br>7281<br>7282<br>7283<br>7284<br>7285<br>7286<br>7287<br>7288<br>7289<br>7290<br>7291<br>7292<br>7293<br>7294<br>7295<br>7296<br>7297<br>7298<br>7299<br>7300<br>7301<br>7302<br>7303<br>7304<br>7305<br>7306<br>7307<br>7308<br>7309<br>7310<br>7311<br>7312<br>7313<br>7314<br>7315<br>7316<br>7317<br>7318<br>7319<br>7320<br>7321<br>7322<br>7323<br>7324<br>7325<br>7326<br>7327<br>7328<br>7329<br>7330<br>7331<br>7332<br>7333<br>7334<br>7335<br>7336<br>7337<br>7338<br>7339<br>7340<br>7341<br>7342<br>7343<br>7344<br>7345<br>7346<br>7347<br>7348<br>7349<br>7350<br>7351<br>7352<br>7353<br>7354<br>7355<br>7356<br>7357<br>7358<br>7359<br>7360<br>7361<br>7362<br>7363<br>7364<br>7365<br>7366<br>7367<br>7368<br>7369<br>7370<br>7371<br>7372<br>7373<br>7374<br>7375<br>7376<br>7377<br>7378<br>7379<br>7380<br>7381<br>7382<br>7383<br>7384<br>7385<br>7386<br>7387<br>7388<br>7389<br>7390<br>7391<br>7392<br>7393<br>7394<br>7395<br>7396<br>7397<br>7398<br>7399<br>7400<br>7401<br>7402<br>7403<br>7404<br>7405<br>7406<br>7407<br>7408<br>7409<br>7410<br>7411<br>7412<br>7413<br>7414<br>7415<br>7416<br>7417<br>7418<br>7419<br>7420<br>7421<br>7422<br>7423<br>7424<br>7425<br>7426<br>7427<br>7428<br>7429<br>7430<br>7431<br>7432<br>7433<br>7434<br>7435<br>7436<br>7437<br>7438<br>7439<br>7440<br>7441<br>7442<br>7443<br>7444<br>7445<br>7446<br>7447<br>7448<br>7449<br>7450<br>7451<br>7452<br>7453<br>7454<br>7455<br>7456<br>7457<br>7458<br>7459<br>7460<br>7461<br>7462<br>7463<br>7464<br>7465<br>7466<br>7467<br>7468<br>7469<br>7470<br>7471<br>7472<br>7473<br>7474<br>7475<br>7476<br>7477<br>7478<br>7479<br>7480<br>7481<br>7482<br>7483<br>7484<br>7485<br>7486<br>7487<br>7488<br>7489<br>7490<br>7491<br>7492<br>7493<br>7494<br>7495<br>7496<br>7497<br>7498<br>7499<br>7500<br>7501<br>7502<br>7503<br>7504<br>7505<br>7506<br>7507<br>7508<br>7509<br>7510<br>7511<br>7512<br>7513<br>7514<br>7515<br>7516<br>7517<br>7518<br>7519<br>7520<br>7521<br>7522<br>7523<br>7524<br>7525<br>7526<br>7527<br>7528<br>7529<br>7530<br>7531<br>7532<br>7533<br>7534<br>7535<br>7536<br>7537<br>7538<br>7539<br>7540<br>7541<br>7542<br>7543<br>7544<br>7545<br>7546<br>7547<br>7548<br>7549<br>7550<br>7551<br>7552<br>7553<br>7554<br>7555<br>7556<br>7557<br>7558<br>7559<br>7560<br>7561<br>7562<br>7563<br>7564<br>7565<br>7566<br>7567<br>7568<br>7569<br>7570<br>7571<br>7572<br>7573<br>7574<br>7575<br>7576<br>7577<br>7578<br>7579<br>7580<br>7581<br>7582<br>7583<br>7584<br>7585<br>7586<br>7587<br>7588<br>7589<br>7590<br>7591<br>7592<br>7593<br>7594<br>7595<br>7596<br>7597<br>7598<br>7599<br>7600<br>7601<br>7602<br>7603<br>7604<br>7605<br>7606<br>7607<br>7608<br>7609<br>7610<br>7611<br>7612<br>7613<br>7614<br>7615<br>7616<br>7617<br>7618<br>7619<br>7620<br>7621<br>7622<br>7623<br>7624<br>7625<br>7626<br>7627<br>7628<br>7629<br>7630<br>7631<br>7632<br>7633<br>7634<br>7635<br>7636<br>7637<br>7638<br>7639<br>7640<br>7641<br>7642<br>7643<br>7644<br>7645<br>7646<br>7647<br>7648<br>7649<br>7650<br>7651<br>7652<br>7653<br>7654<br>7655<br>7656<br>7657<br>7658<br>7659<br>7660<br>7661<br>7662<br>7663<br>7664<br>7665<br>7666<br>7667<br>7668<br>7669<br>7670<br>7671<br>7672<br>7673<br>7674<br>7675<br>7676<br>7677<br>7678<br>7679<br>7680<br>7681<br>7682<br>7683<br>7684<br>7685<br>7686<br>7687<br>7688<br>7689<br>7690<br>7691<br>7692<br>7693<br>7694<br>7695<br>7696<br>7697<br>7698<br>7699<br>7700<br>7701<br>7702<br>7703<br>7704<br>7705<br>7706<br>7707<br>7708<br>7709<br>7710<br>7711<br>7712<br>7713<br>7714<br>7715<br>7716<br>7717<br>7718<br>7719<br>7720<br>7721<br>7722<br>7723<br>7724<br>7725<br>7726<br>7727<br>7728<br>7729<br>7730<br>7731<br>7732<br>7733<br>7734<br>7735<br>7736<br>7737<br>7738<br>7739<br>7740<br>7741<br>7742<br>7743<br>7744<br>7745<br>7746<br>7747<br>7748<br>7749<br>7750<br>7751<br>7752<br>7753<br>7754<br>7755<br>7756<br>7757<br>7758<br>7759<br>7760<br>7761<br>7762<br>7763<br>7764<br>7765<br>7766<br>7767<br>7768<br>7769<br>7770<br>7771<br>7772<br>7773<br>7774<br>7775<br>7776<br>7777<br>7778<br>7779<br>7780<br>7781<br>7782<br>7783<br>7784<br>7785<br>7786<br>7787<br>7788<br>7789<br>7790<br>7791<br>7792<br>7793<br>7794<br>7795<br>7796<br>7797<br>7798<br>7799<br>7800<br>7801<br>7802<br>7803<br>7804<br>7805<br>7806<br>7807<br>7808<br>7809<br>7810<br>7811<br>7812<br>7813<br>7814<br>7815<br>7816<br>7817<br>7818<br>7819<br>7820<br>7821<br>7822<br>7823<br>7824<br>7825<br>7826<br>7827<br>7828<br>7829<br>7830<br>7831<br>7832<br>7833<br>7834<br>7835<br>7836<br>7837<br>7838<br>7839<br>7840<br>7841<br>7842<br>7843<br>7844<br>7845<br>7846<br>7847<br>7848<br>7849<br>7850<br>7851<br>7852<br>7853<br>7854<br>7855<br>7856<br>7857<br>7858<br>7859<br>7860<br>7861<br>7862<br>7863<br>7864<br>7865<br>7866<br>7867<br>7868<br>7869<br>7870<br>7871<br>7872<br>7873<br>7874<br>7875<br>7876<br>7877<br>7878<br>7879<br>7880<br>7881<br>7882<br>7883<br>7884<br>7885<br>7886<br>7887<br>7888<br>7889<br>7890<br>7891<br>7892<br>7893<br>7894<br>7895<br>7896<br>7897<br>7898<br>7899<br>7900<br>7901<br>7902<br>7903<br>7904<br>7905<br>7906<br>7907<br>7908<br>7909<br>7910<br>7911<br>7912<br>7913<br>7914<br>7915<br>7916<br>7917<br>7918<br>7919<br>7920<br>7921<br>7922<br>7923<br>7924<br>7925<br>7926<br>7927<br>7928<br>7929<br>7930<br>7931<br>7932<br>7933<br>7934<br>7935<br>7936<br>7937<br>7938<br>7939<br>7940<br>7941<br>7942<br>7943<br>7944<br>7945<br>7946<br>7947<br>7948<br>7949<br>7950<br>7951<br>7952<br>7953<br>7954<br>7955<br>7956<br>7957<br>7958<br>7959<br>7960<br>7961<br>7962<br>7963<br>7964<br>7965<br>7966<br>7967<br>7968<br>7969<br>7970<br>7971<br>7972<br>7973<br>7974<br>7975<br>7976<br>7977<br>7978<br>7979<br>7980<br>7981<br>7982<br>7983<br>7984<br>7985<br>7986<br>7987<br>7988<br>7989<br>7990<br>7991<br>7992<br>7993<br>7994<br>7995<br>7996<br>7997<br>7998<br>7999<br>8000<br>8001<br>8002<br>8003<br>8004<br>8005<br>8006<br>8007<br>8008<br>8009<br>8010<br>8011<br>8012<br>8013<br>8014<br>8015<br>8016<br>8017<br>8018<br>8019<br>8020<br>8021<br>8022<br>8023<br>8024<br>8025<br>8026<br>8027<br>8028<br>8029<br>8030<br>8031<br>8032<br>8033<br>8034<br>8035<br>8036<br>8037<br>8038<br>8039<br>8040<br>8041<br>8042<br>8043<br>8044<br>8045<br>8046<br>8047<br>8048<br>8049<br>8050<br>8051<br>8052<br>8053<br>8054<br>8055<br>8056<br>8057<br>8058<br>8059<br>8060<br>8061<br>8062<br>8063<br>8064<br>8065<br>8066<br>8067<br>8068<br>8069<br>8070<br>8071<br>8072<br>8073<br>8074<br>8075<br>8076<br>8077<br>8078<br>8079<br>8080<br>8081<br>8082<br>8083<br>8084<br>8085<br>8086<br>8087<br>8088<br>8089<br>8090<br>8091<br>8092<br>8093<br>8094<br>8095<br>8096<br>8097<br>8098<br>8099<br>8100<br>8101<br>8102<br>8103<br>8104<br>8105<br>8106<br>8107<br>8108<br>8109<br>8110<br>8111<br>8112<br>8113<br>8114<br>8115<br>8116<br>8117<br>8118<br>8119<br>8120<br>8121<br>8122<br>8123<br>8124<br>8125<br>8126<br>8127<br>8128<br>8129<br>8130<br>8131<br>8132<br>8133<br>8134<br>8135<br>8136<br>8137<br>8138<br>8139<br>8140<br>8141<br>8142<br>8143<br>8144<br>8145<br>8146<br>8147<br>8148<br>8149<br>8150<br>8151<br>8152<br>8153<br>8154<br>8155<br>8156<br>8157<br>8158<br>8159<br>8160<br>8161<br>8162<br>8163<br>8164<br>8165<br>8166<br>8167<br>8168<br>8169<br>8170<br>8171<br>8172<br>8173<br>8174<br>8175<br>8176<br>8177<br>8178<br>8179<br>8180<br>8181<br>8182<br>8183<br>8184<br>8185<br>8186<br>8187<br>8188<br>8189<br>8190<br>8191<br>8192<br>8193<br>8194<br>8195<br>8196<br>8197<br>8198<br>8199<br>8200<br>8201<br>8202<br>8203<br>8204<br>8205<br>8206<br>8207<br>8208<br>8209<br>8210<br>8211<br>8212<br>8213<br>8214<br>8215<br>8216<br>8217<br>8218<br>8219<br>8220<br>8221<br>8222<br>8223<br>8224<br>8225<br>8226<br>8227<br>8228<br>8229<br>8230<br>8231<br>8232<br>8233<br>8234<br>8235<br>8236<br>8237<br>8238<br>8239<br>8240<br>8241<br>8242<br>8243<br>8244<br>8245<br>8246<br>8247<br>8248<br>8249<br>8250<br>8251<br>8252<br>8253<br>8254<br>8255<br>8256<br>8257<br>8258<br>8259<br>8260<br>8261<br>8262<br>8263<br>8264<br>8265<br>8266<br>8267<br>8268<br>8269<br>8270<br>8271<br>8272<br>8273<br>8274<br>8275<br>8276<br>8277<br>8278<br>8279<br>8280<br>8281<br>8282<br>8283<br>8284<br>8285<br>8286<br>8287<br>8288<br>8289<br>8290<br>8291<br>8292<br>8293<br>8294<br>8295<br>8296<br>8297<br>8298<br>8299<br>8300<br>8301<br>8302<br>8303<br>8304<br>8305<br>8306<br>8307<br>8308<br>8309<br>8310<br>8311<br>8312<br>8313<br>8314<br>8315<br>8316<br>8317<br>8318<br>8319<br>8320<br>8321<br>8322<br>8323<br>8324<br>8325<br>8326<br>8327<br>8328<br>8329<br>8330<br>8331<br>8332<br>8333<br>8334<br>8335<br>8336<br>8337<br>8338<br>8339<br>8340<br>8341<br>8342<br>8343<br>8344<br>8345<br>8346<br>8347<br>8348<br>8349<br>8350<br>8351<br>8352<br>8353<br>8354<br>8355<br>8356<br>8357<br>8358<br>8359<br>8360<br>8361<br>8362<br>8363<br>8364<br>8365<br>8366<br>8367<br>8368<br>8369<br>8370<br>8371<br>8372<br>8373<br>8374<br>8375<br>8376<br>8377<br>8378<br>8379<br>8380<br>8381<br>8382<br>8383<br>8384<br>8385<br>8386<br>8387<br>8388<br>8389<br>8390<br>8391<br>8392<br>8393<br>8394<br>8395<br>8396<br>8397<br>8398<br>8399<br>8400<br>8401<br>8402<br>8403<br>8404<br>8405<br>8406<br>8407<br>8408<br>8409<br>8410<br>8411<br>8412<br>8413<br>8414<br>8415<br>8416<br>8417<br>8418<br>8419<br>8420<br>8421<br>8422<br>8423<br>8424<br>8425<br>8426<br>8427<br>8428<br>8429<br>8430<br>8431<br>8432<br>8433<br>8434<br>8435<br>8436<br>8437<br>8438<br>8439<br>8440<br>8441<br>8442<br>8443<br>8444<br>8445<br>8446<br>8447<br>8448<br>8449<br>8450<br>8451<br>8452<br>8453<br>8454<br>8455<br>8456<br>8457<br>8458<br>8459<br>8460<br>8461<br>8462<br>8463<br>8464<br>8465<br>8466<br>8467<br>8468<br>8469<br>8470<br>8471<br>8472<br>8473<br>8474<br>8475<br>8476<br>8477<br>8478<br>8479<br>8480<br>8481<br>8482<br>8483<br>8484<br>8485<br>8486<br>8487<br>8488<br>8489<br>8490<br>8491<br>8492<br>8493<br>8494<br>8495<br>8496<br>8497<br>8498<br>8499<br>8500<br>8501<br>8502<br>8503<br>8504<br>8505<br>8506<br>8507<br>8508<br>8509<br>8510<br>8511<br>8512<br>8513<br>8514<br>8515<br>8516<br>8517<br>8518<br>8519<br>8520<br>8521<br>8522<br>8523<br>8524<br>8525<br>8526<br>8527<br>8528<br>8529<br>8530<br>8531<br>8532<br>8533<br>8534<br>8535<br>8536<br>8537<br>8538<br>8539<br>8540<br>8541<br>8542<br>8543<br>8544<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * Kernel-based Virtual Machine driver for Linux
 *
 * This module enables machines with Intel VT-x extensions to run virtual
 * machines without emulation or binary translation.
 *
 * Copyright (C) 2006 Qumranet, Inc.
 * Copyright 2010 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@qumranet.com&gt;
 *   Yaniv Kamay  &lt;yaniv@qumranet.com&gt;
 */

#include &lt;linux/highmem.h&gt;
#include &lt;linux/hrtimer.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/moduleparam.h&gt;
#include &lt;linux/mod_devicetable.h&gt;
#include &lt;linux/mm.h&gt;
#include &lt;linux/objtool.h&gt;
#include &lt;linux/sched.h&gt;
#include &lt;linux/sched/smt.h&gt;
#include &lt;linux/slab.h&gt;
#include &lt;linux/tboot.h&gt;
#include &lt;linux/trace_events.h&gt;
#include &lt;linux/entry-kvm.h&gt;

#include &lt;asm/apic.h&gt;
#include &lt;asm/asm.h&gt;
#include &lt;asm/cpu.h&gt;
#include &lt;asm/cpu_device_id.h&gt;
#include &lt;asm/debugreg.h&gt;
#include &lt;asm/desc.h&gt;
#include &lt;asm/fpu/api.h&gt;
#include &lt;asm/fpu/xstate.h&gt;
#include &lt;asm/idtentry.h&gt;
#include &lt;asm/io.h&gt;
#include &lt;asm/irq_remapping.h&gt;
#include &lt;asm/kexec.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &lt;asm/mmu_context.h&gt;
#include &lt;asm/mshyperv.h&gt;
#include &lt;asm/mwait.h&gt;
#include &lt;asm/spec-ctrl.h&gt;
#include &lt;asm/virtext.h&gt;
#include &lt;asm/vmx.h&gt;

#include &quot;capabilities.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;evmcs.h&quot;
#include &quot;hyperv.h&quot;
#include &quot;kvm_onhyperv.h&quot;
#include &quot;irq.h&quot;
#include &quot;kvm_cache_regs.h&quot;
#include &quot;lapic.h&quot;
#include &quot;mmu.h&quot;
#include &quot;nested.h&quot;
#include &quot;pmu.h&quot;
#include &quot;sgx.h&quot;
#include &quot;trace.h&quot;
#include &quot;vmcs.h&quot;
#include &quot;vmcs12.h&quot;
#include &quot;vmx.h&quot;
#include &quot;x86.h&quot;

MODULE_AUTHOR(&quot;Qumranet&quot;);
MODULE_LICENSE(&quot;GPL&quot;);

#ifdef MODULE
static const struct x86_cpu_id vmx_cpu_id[] = {
	X86_MATCH_FEATURE(X86_FEATURE_VMX, NULL),
	{}
};
MODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);
#endif

bool __read_mostly enable_vpid = 1;
module_param_named(vpid, enable_vpid, bool, 0444);

static bool __read_mostly enable_vnmi = 1;
module_param_named(vnmi, enable_vnmi, bool, S_IRUGO);

bool __read_mostly flexpriority_enabled = 1;
module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);

bool __read_mostly enable_ept = 1;
module_param_named(ept, enable_ept, bool, S_IRUGO);

bool __read_mostly enable_unrestricted_guest = 1;
module_param_named(unrestricted_guest,
			enable_unrestricted_guest, bool, S_IRUGO);

bool __read_mostly enable_ept_ad_bits = 1;
module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);

static bool __read_mostly emulate_invalid_guest_state = true;
module_param(emulate_invalid_guest_state, bool, S_IRUGO);

static bool __read_mostly fasteoi = 1;
module_param(fasteoi, bool, S_IRUGO);

module_param(enable_apicv, bool, S_IRUGO);

bool __read_mostly enable_ipiv = true;
module_param(enable_ipiv, bool, 0444);

/*
 * If nested=1, nested virtualization is supported, i.e., guests may use
 * VMX and be a hypervisor for its own guests. If nested=0, guests may not
 * use VMX instructions.
 */
static bool __read_mostly nested = 1;
module_param(nested, bool, S_IRUGO);

bool __read_mostly enable_pml = 1;
module_param_named(pml, enable_pml, bool, S_IRUGO);

static bool __read_mostly error_on_inconsistent_vmcs_config = true;
module_param(error_on_inconsistent_vmcs_config, bool, 0444);

static bool __read_mostly dump_invalid_vmcs = 0;
module_param(dump_invalid_vmcs, bool, 0644);

#define MSR_BITMAP_MODE_X2APIC		1
#define MSR_BITMAP_MODE_X2APIC_APICV	2

#define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL

/* Guest_tsc -&gt; host_tsc conversion requires 64-bit division.  */
static int __read_mostly cpu_preemption_timer_multi;
static bool __read_mostly enable_preemption_timer = 1;
#ifdef CONFIG_X86_64
module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
#endif

extern bool __read_mostly allow_smaller_maxphyaddr;
module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);

#define KVM_VM_CR0_ALWAYS_OFF (X86_CR0_NW | X86_CR0_CD)
#define KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR0_NE
#define KVM_VM_CR0_ALWAYS_ON				\
	(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | X86_CR0_PG | X86_CR0_PE)

#define KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR4_VMXE
#define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)
#define KVM_RMODE_VM_CR4_ALWAYS_ON (X86_CR4_VME | X86_CR4_PAE | X86_CR4_VMXE)

#define RMODE_GUEST_OWNED_EFLAGS_BITS (~(X86_EFLAGS_IOPL | X86_EFLAGS_VM))

#define MSR_IA32_RTIT_STATUS_MASK (~(RTIT_STATUS_FILTEREN | \
	RTIT_STATUS_CONTEXTEN | RTIT_STATUS_TRIGGEREN | \
	RTIT_STATUS_ERROR | RTIT_STATUS_STOPPED | \
	RTIT_STATUS_BYTECNT))

/*
 * List of MSRs that can be directly passed to the guest.
 * In addition to these x2apic and PT MSRs are handled specially.
 */
static u32 vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS] = {
	MSR_IA32_SPEC_CTRL,
	MSR_IA32_PRED_CMD,
	MSR_IA32_TSC,
#ifdef CONFIG_X86_64
	MSR_FS_BASE,
	MSR_GS_BASE,
	MSR_KERNEL_GS_BASE,
	MSR_IA32_XFD,
	MSR_IA32_XFD_ERR,
#endif
	MSR_IA32_SYSENTER_CS,
	MSR_IA32_SYSENTER_ESP,
	MSR_IA32_SYSENTER_EIP,
	MSR_CORE_C1_RES,
	MSR_CORE_C3_RESIDENCY,
	MSR_CORE_C6_RESIDENCY,
	MSR_CORE_C7_RESIDENCY,
};

/*
 * These 2 parameters are used to config the controls for Pause-Loop Exiting:
 * ple_gap:    upper bound on the amount of time between two successive
 *             executions of PAUSE in a loop. Also indicate if ple enabled.
 *             According to test, this time is usually smaller than 128 cycles.
 * ple_window: upper bound on the amount of time a guest is allowed to execute
 *             in a PAUSE loop. Tests indicate that most spinlocks are held for
 *             less than 2^12 cycles
 * Time is measured based on a counter that runs at the same rate as the TSC,
 * refer SDM volume 3b section 21.6.13 &amp; 22.1.3.
 */
static unsigned int ple_gap = KVM_DEFAULT_PLE_GAP;
module_param(ple_gap, uint, 0444);

static unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;
module_param(ple_window, uint, 0444);

/* Default doubles per-vcpu window every exit. */
static unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;
module_param(ple_window_grow, uint, 0444);

/* Default resets per-vcpu window every exit to ple_window. */
static unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;
module_param(ple_window_shrink, uint, 0444);

/* Default is to compute the maximum so we can never overflow. */
static unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
module_param(ple_window_max, uint, 0444);

/* Default is SYSTEM mode, 1 for host-guest mode */
int __read_mostly pt_mode = PT_MODE_SYSTEM;
module_param(pt_mode, int, S_IRUGO);

static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
static DEFINE_MUTEX(vmx_l1d_flush_mutex);

/* Storage for pre module init parameter parsing */
static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;

static const struct {
	const char *option;
	bool for_parse;
} vmentry_l1d_param[] = {
	[VMENTER_L1D_FLUSH_AUTO]	 = {&quot;auto&quot;, true},
	[VMENTER_L1D_FLUSH_NEVER]	 = {&quot;never&quot;, true},
	[VMENTER_L1D_FLUSH_COND]	 = {&quot;cond&quot;, true},
	[VMENTER_L1D_FLUSH_ALWAYS]	 = {&quot;always&quot;, true},
	[VMENTER_L1D_FLUSH_EPT_DISABLED] = {&quot;EPT disabled&quot;, false},
	[VMENTER_L1D_FLUSH_NOT_REQUIRED] = {&quot;not required&quot;, false},
};

#define L1D_CACHE_ORDER 4
static void *vmx_l1d_flush_pages;

/* Control for disabling CPU Fill buffer clear */
static bool __read_mostly vmx_fb_clear_ctrl_available;

static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
{
	struct page *page;
	unsigned int i;

<yellow>	if (!boot_cpu_has_bug(X86_BUG_L1TF)) {</yellow>
<yellow>		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;</yellow>
		return 0;
	}

<yellow>	if (!enable_ept) {</yellow>
<yellow>		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;</yellow>
		return 0;
	}

<yellow>	if (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES)) {</yellow>
		u64 msr;

<yellow>		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, msr);</yellow>
		if (msr &amp; ARCH_CAP_SKIP_VMENTRY_L1DFLUSH) {
<yellow>			l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;</yellow>
			return 0;
		}
	}

	/* If set to auto use the default l1tf mitigation method */
	if (l1tf == VMENTER_L1D_FLUSH_AUTO) {
<yellow>		switch (l1tf_mitigation) {</yellow>
		case L1TF_MITIGATION_OFF:
			l1tf = VMENTER_L1D_FLUSH_NEVER;
			break;
		case L1TF_MITIGATION_FLUSH_NOWARN:
		case L1TF_MITIGATION_FLUSH:
		case L1TF_MITIGATION_FLUSH_NOSMT:
			l1tf = VMENTER_L1D_FLUSH_COND;
			break;
		case L1TF_MITIGATION_FULL:
		case L1TF_MITIGATION_FULL_FORCE:
			l1tf = VMENTER_L1D_FLUSH_ALWAYS;
			break;
		}
<yellow>	} else if (l1tf_mitigation == L1TF_MITIGATION_FULL_FORCE) {</yellow>
		l1tf = VMENTER_L1D_FLUSH_ALWAYS;
	}

<yellow>	if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&</yellow>
<yellow>	    !boot_cpu_has(X86_FEATURE_FLUSH_L1D)) {</yellow>
		/*
		 * This allocation for vmx_l1d_flush_pages is not tied to a VM
		 * lifetime and so should not be charged to a memcg.
		 */
<yellow>		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);</yellow>
		if (!page)
			return -ENOMEM;
<yellow>		vmx_l1d_flush_pages = page_address(page);</yellow>

		/*
		 * Initialize each page with a different pattern in
		 * order to protect against KSM in the nested
		 * virtualization case.
		 */
		for (i = 0; i &lt; 1u &lt;&lt; L1D_CACHE_ORDER; ++i) {
<yellow>			memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1,</yellow>
			       PAGE_SIZE);
		}
	}

<yellow>	l1tf_vmx_mitigation = l1tf;</yellow>

	if (l1tf != VMENTER_L1D_FLUSH_NEVER)
		static_branch_enable(&amp;vmx_l1d_should_flush);
	else
		static_branch_disable(&amp;vmx_l1d_should_flush);

	if (l1tf == VMENTER_L1D_FLUSH_COND)
<yellow>		static_branch_enable(&vmx_l1d_flush_cond);</yellow>
	else
<yellow>		static_branch_disable(&vmx_l1d_flush_cond);</yellow>
	return 0;
<yellow>}</yellow>

static int vmentry_l1d_flush_parse(const char *s)
{
	unsigned int i;

<yellow>	if (s) {</yellow>
<yellow>		for (i = 0; i < ARRAY_SIZE(vmentry_l1d_param); i++) {</yellow>
<yellow>			if (vmentry_l1d_param[i].for_parse &&</yellow>
<yellow>			    sysfs_streq(s, vmentry_l1d_param[i].option))</yellow>
				return i;
		}
	}
	return -EINVAL;
}

static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
{
	int l1tf, ret;

<yellow>	l1tf = vmentry_l1d_flush_parse(s);</yellow>
	if (l1tf &lt; 0)
		return l1tf;

<yellow>	if (!boot_cpu_has(X86_BUG_L1TF))</yellow>
		return 0;

	/*
	 * Has vmx_init() run already? If not then this is the pre init
	 * parameter parsing. In that case just store the value and let
	 * vmx_init() do the proper setup after enable_ept has been
	 * established.
	 */
<yellow>	if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {</yellow>
<yellow>		vmentry_l1d_flush_param = l1tf;</yellow>
		return 0;
	}

<yellow>	mutex_lock(&vmx_l1d_flush_mutex);</yellow>
<yellow>	ret = vmx_setup_l1d_flush(l1tf);</yellow>
<yellow>	mutex_unlock(&vmx_l1d_flush_mutex);</yellow>
	return ret;
<yellow>}</yellow>

static int vmentry_l1d_flush_get(char *s, const struct kernel_param *kp)
{
<yellow>	if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))</yellow>
		return sprintf(s, &quot;???\n&quot;);

<yellow>	return sprintf(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);</yellow>
<yellow>}</yellow>

static void vmx_setup_fb_clear_ctrl(void)
{
	u64 msr;

<yellow>	if (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES) &&</yellow>
<yellow>	    !boot_cpu_has_bug(X86_BUG_MDS) &&</yellow>
<yellow>	    !boot_cpu_has_bug(X86_BUG_TAA)) {</yellow>
<yellow>		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, msr);</yellow>
		if (msr &amp; ARCH_CAP_FB_CLEAR_CTRL)
<yellow>			vmx_fb_clear_ctrl_available = true;</yellow>
	}
}

static __always_inline void vmx_disable_fb_clear(struct vcpu_vmx *vmx)
{
	u64 msr;

	if (!vmx-&gt;disable_fb_clear)
		return;

	msr = __rdmsr(MSR_IA32_MCU_OPT_CTRL);
	msr |= FB_CLEAR_DIS;
	native_wrmsrl(MSR_IA32_MCU_OPT_CTRL, msr);
	/* Cache the MSR value to avoid reading it later */
	vmx-&gt;msr_ia32_mcu_opt_ctrl = msr;
}

static __always_inline void vmx_enable_fb_clear(struct vcpu_vmx *vmx)
{
	if (!vmx-&gt;disable_fb_clear)
		return;

	vmx-&gt;msr_ia32_mcu_opt_ctrl &amp;= ~FB_CLEAR_DIS;
	native_wrmsrl(MSR_IA32_MCU_OPT_CTRL, vmx-&gt;msr_ia32_mcu_opt_ctrl);
}

static void vmx_update_fb_clear_dis(struct kvm_vcpu *vcpu, struct vcpu_vmx *vmx)
{
<blue>	vmx->disable_fb_clear = vmx_fb_clear_ctrl_available;</blue>

	/*
	 * If guest will not execute VERW, there is no need to set FB_CLEAR_DIS
	 * at VMEntry. Skip the MSR read/write when a guest has no use case to
	 * execute VERW.
	 */
	if ((vcpu-&gt;arch.arch_capabilities &amp; ARCH_CAP_FB_CLEAR) ||
	   ((vcpu-&gt;arch.arch_capabilities &amp; ARCH_CAP_MDS_NO) &amp;&amp;
	    (vcpu-&gt;arch.arch_capabilities &amp; ARCH_CAP_TAA_NO) &amp;&amp;
	    (vcpu-&gt;arch.arch_capabilities &amp; ARCH_CAP_PSDP_NO) &amp;&amp;
<blue>	    (vcpu->arch.arch_capabilities & ARCH_CAP_FBSDP_NO) &&</blue>
	    (vcpu-&gt;arch.arch_capabilities &amp; ARCH_CAP_SBDR_SSDP_NO)))
<blue>		vmx->disable_fb_clear = false;</blue>
}

static const struct kernel_param_ops vmentry_l1d_flush_ops = {
	.set = vmentry_l1d_flush_set,
	.get = vmentry_l1d_flush_get,
};
module_param_cb(vmentry_l1d_flush, &amp;vmentry_l1d_flush_ops, NULL, 0644);

static u32 vmx_segment_access_rights(struct kvm_segment *var);

void vmx_vmexit(void);

#define vmx_insn_failed(fmt...)		\
do {					\
	WARN_ONCE(1, fmt);		\
	pr_warn_ratelimited(fmt);	\
} while (0)

void vmread_error(unsigned long field, bool fault)
{
<yellow>	if (fault)</yellow>
<yellow>		kvm_spurious_fault();</yellow>
	else
<yellow>		vmx_insn_failed("kvm: vmread failed: field=%lx\n", field);</yellow>
<yellow>}</yellow>

noinline void vmwrite_error(unsigned long field, unsigned long value)
{
<yellow>	vmx_insn_failed("kvm: vmwrite failed: field=%lx val=%lx err=%u\n",</yellow>
			field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
<yellow>}</yellow>

noinline void vmclear_error(struct vmcs *vmcs, u64 phys_addr)
{
<yellow>	vmx_insn_failed("kvm: vmclear failed: %p/%llx err=%u\n",</yellow>
			vmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));
<yellow>}</yellow>

noinline void vmptrld_error(struct vmcs *vmcs, u64 phys_addr)
{
<yellow>	vmx_insn_failed("kvm: vmptrld failed: %p/%llx err=%u\n",</yellow>
			vmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));
<yellow>}</yellow>

noinline void invvpid_error(unsigned long ext, u16 vpid, gva_t gva)
{
<yellow>	vmx_insn_failed("kvm: invvpid failed: ext=0x%lx vpid=%u gva=0x%lx\n",</yellow>
			ext, vpid, gva);
<yellow>}</yellow>

noinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)
{
<yellow>	vmx_insn_failed("kvm: invept failed: ext=0x%lx eptp=%llx gpa=0x%llx\n",</yellow>
			ext, eptp, gpa);
<yellow>}</yellow>

static DEFINE_PER_CPU(struct vmcs *, vmxarea);
DEFINE_PER_CPU(struct vmcs *, current_vmcs);
/*
 * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed
 * when a CPU is brought down, and we need to VMCLEAR all VMCSs loaded on it.
 */
static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);

static DECLARE_BITMAP(vmx_vpid_bitmap, VMX_NR_VPIDS);
static DEFINE_SPINLOCK(vmx_vpid_lock);

struct vmcs_config vmcs_config;
struct vmx_capability vmx_capability;

#define VMX_SEGMENT_FIELD(seg)					\
	[VCPU_SREG_##seg] = {                                   \
		.selector = GUEST_##seg##_SELECTOR,		\
		.base = GUEST_##seg##_BASE,		   	\
		.limit = GUEST_##seg##_LIMIT,		   	\
		.ar_bytes = GUEST_##seg##_AR_BYTES,	   	\
	}

static const struct kvm_vmx_segment_field {
	unsigned selector;
	unsigned base;
	unsigned limit;
	unsigned ar_bytes;
} kvm_vmx_segment_fields[] = {
	VMX_SEGMENT_FIELD(CS),
	VMX_SEGMENT_FIELD(DS),
	VMX_SEGMENT_FIELD(ES),
	VMX_SEGMENT_FIELD(FS),
	VMX_SEGMENT_FIELD(GS),
	VMX_SEGMENT_FIELD(SS),
	VMX_SEGMENT_FIELD(TR),
	VMX_SEGMENT_FIELD(LDTR),
};

static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
{
<blue>	vmx->segment_cache.bitmask = 0;</blue>
}

static unsigned long host_idt_base;

#if IS_ENABLED(CONFIG_HYPERV)
static bool __read_mostly enlightened_vmcs = true;
module_param(enlightened_vmcs, bool, 0444);

static int hv_enable_direct_tlbflush(struct kvm_vcpu *vcpu)
{
	struct hv_enlightened_vmcs *evmcs;
	struct hv_partition_assist_pg **p_hv_pa_pg =
<yellow>			&to_kvm_hv(vcpu->kvm)->hv_pa_pg;</yellow>
	/*
	 * Synthetic VM-Exit is not enabled in current code and so All
	 * evmcs in singe VM shares same assist page.
	 */
	if (!*p_hv_pa_pg)
<yellow>		*p_hv_pa_pg = kzalloc(PAGE_SIZE, GFP_KERNEL_ACCOUNT);</yellow>

	if (!*p_hv_pa_pg)
		return -ENOMEM;

<yellow>	evmcs = (struct hv_enlightened_vmcs *)to_vmx(vcpu)->loaded_vmcs->vmcs;</yellow>

<yellow>	evmcs->partition_assist_page =</yellow>
<yellow>		__pa(*p_hv_pa_pg);</yellow>
	evmcs-&gt;hv_vm_id = (unsigned long)vcpu-&gt;kvm;
	evmcs-&gt;hv_enlightenments_control.nested_flush_hypercall = 1;

	return 0;
<yellow>}</yellow>

#endif /* IS_ENABLED(CONFIG_HYPERV) */

/*
 * Comment&#x27;s format: document - errata name - stepping - processor name.
 * Refer from
 * https://www.virtualbox.org/svn/vbox/trunk/src/VBox/VMM/VMMR0/HMR0.cpp
 */
static u32 vmx_preemption_cpu_tfms[] = {
/* 323344.pdf - BA86   - D0 - Xeon 7500 Series */
0x000206E6,
/* 323056.pdf - AAX65  - C2 - Xeon L3406 */
/* 322814.pdf - AAT59  - C2 - i7-600, i5-500, i5-400 and i3-300 Mobile */
/* 322911.pdf - AAU65  - C2 - i5-600, i3-500 Desktop and Pentium G6950 */
0x00020652,
/* 322911.pdf - AAU65  - K0 - i5-600, i3-500 Desktop and Pentium G6950 */
0x00020655,
/* 322373.pdf - AAO95  - B1 - Xeon 3400 Series */
/* 322166.pdf - AAN92  - B1 - i7-800 and i5-700 Desktop */
/*
 * 320767.pdf - AAP86  - B1 -
 * i7-900 Mobile Extreme, i7-800 and i7-700 Mobile
 */
0x000106E5,
/* 321333.pdf - AAM126 - C0 - Xeon 3500 */
0x000106A0,
/* 321333.pdf - AAM126 - C1 - Xeon 3500 */
0x000106A1,
/* 320836.pdf - AAJ124 - C0 - i7-900 Desktop Extreme and i7-900 Desktop */
0x000106A4,
 /* 321333.pdf - AAM126 - D0 - Xeon 3500 */
 /* 321324.pdf - AAK139 - D0 - Xeon 5500 */
 /* 320836.pdf - AAJ124 - D0 - i7-900 Extreme and i7-900 Desktop */
0x000106A5,
 /* Xeon E3-1220 V2 */
0x000306A8,
};

static inline bool cpu_has_broken_vmx_preemption_timer(void)
{
<yellow>	u32 eax = cpuid_eax(0x00000001), i;</yellow>

	/* Clear the reserved bits */
	eax &amp;= ~(0x3U &lt;&lt; 14 | 0xfU &lt;&lt; 28);
<yellow>	for (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)</yellow>
<yellow>		if (eax == vmx_preemption_cpu_tfms[i])</yellow>
			return true;

	return false;
}

static inline bool cpu_need_virtualize_apic_accesses(struct kvm_vcpu *vcpu)
{
<blue>	return flexpriority_enabled && lapic_in_kernel(vcpu);</blue>
}

static int possible_passthrough_msr_slot(u32 msr)
{
	u32 i;

<blue>	for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++)</blue>
<blue>		if (vmx_possible_passthrough_msrs[i] == msr)</blue>
			return i;

	return -ENOENT;
}

static bool is_valid_passthrough_msr(u32 msr)
{
	bool r;

<blue>	switch (msr) {</blue>
	case 0x800 ... 0x8ff:
		/* x2APIC MSRs. These are handled in vmx_update_msr_bitmap_x2apic() */
		return true;
	case MSR_IA32_RTIT_STATUS:
	case MSR_IA32_RTIT_OUTPUT_BASE:
	case MSR_IA32_RTIT_OUTPUT_MASK:
	case MSR_IA32_RTIT_CR3_MATCH:
	case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:
		/* PT MSRs. These are handled in pt_update_intercept_for_msr() */
	case MSR_LBR_SELECT:
	case MSR_LBR_TOS:
	case MSR_LBR_INFO_0 ... MSR_LBR_INFO_0 + 31:
	case MSR_LBR_NHM_FROM ... MSR_LBR_NHM_FROM + 31:
	case MSR_LBR_NHM_TO ... MSR_LBR_NHM_TO + 31:
	case MSR_LBR_CORE_FROM ... MSR_LBR_CORE_FROM + 8:
	case MSR_LBR_CORE_TO ... MSR_LBR_CORE_TO + 8:
		/* LBR MSRs. These are handled in vmx_update_intercept_for_lbr_msrs() */
		return true;
	}

<blue>	r = possible_passthrough_msr_slot(msr) != -ENOENT;</blue>

<yellow>	WARN(!r, "Invalid MSR %x, please adapt vmx_possible_passthrough_msrs[]", msr);</yellow>

	return r;
<blue>}</blue>

struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)
{
	int i;

<blue>	i = kvm_find_user_return_msr(msr);</blue>
	if (i &gt;= 0)
<blue>		return &vmx->guest_uret_msrs[i];</blue>
	return NULL;
<yellow>}</yellow>

static int vmx_set_guest_uret_msr(struct vcpu_vmx *vmx,
				  struct vmx_uret_msr *msr, u64 data)
{
	unsigned int slot = msr - vmx-&gt;guest_uret_msrs;
	int ret = 0;

<blue>	if (msr->load_into_hardware) {</blue>
<blue>		preempt_disable();</blue>
		ret = kvm_set_user_return_msr(slot, data, msr-&gt;mask);
<yellow>		preempt_enable();</yellow>
	}
<blue>	if (!ret)</blue>
<blue>		msr->data = data;</blue>
	return ret;
<blue>}</blue>

#ifdef CONFIG_KEXEC_CORE
static void crash_vmclear_local_loaded_vmcss(void)
<yellow>{</yellow>
<yellow>	int cpu = raw_smp_processor_id();</yellow>
	struct loaded_vmcs *v;

<yellow>	list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),</yellow>
			    loaded_vmcss_on_cpu_link)
<yellow>		vmcs_clear(v->vmcs);</yellow>
}
#endif /* CONFIG_KEXEC_CORE */

<blue>static void __loaded_vmcs_clear(void *arg)</blue>
<blue>{</blue>
	struct loaded_vmcs *loaded_vmcs = arg;
<blue>	int cpu = raw_smp_processor_id();</blue>

	if (loaded_vmcs-&gt;cpu != cpu)
		return; /* vcpu migration can race with cpu offline */
<blue>	if (per_cpu(current_vmcs, cpu) == loaded_vmcs->vmcs)</blue>
<blue>		per_cpu(current_vmcs, cpu) = NULL;</blue>

<blue>	vmcs_clear(loaded_vmcs->vmcs);</blue>
<blue>	if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)</blue>
<yellow>		vmcs_clear(loaded_vmcs->shadow_vmcs);</yellow>

<blue>	list_del(&loaded_vmcs->loaded_vmcss_on_cpu_link);</blue>

	/*
	 * Ensure all writes to loaded_vmcs, including deleting it from its
	 * current percpu list, complete before setting loaded_vmcs-&gt;cpu to
	 * -1, otherwise a different cpu can see loaded_vmcs-&gt;cpu == -1 first
	 * and add loaded_vmcs to its percpu list before it&#x27;s deleted from this
	 * cpu&#x27;s list. Pairs with the smp_rmb() in vmx_vcpu_load_vmcs().
	 */
	smp_wmb();

	loaded_vmcs-&gt;cpu = -1;
	loaded_vmcs-&gt;launched = 0;
}

void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs)
{
<blue>	int cpu = loaded_vmcs->cpu;</blue>

<blue>	if (cpu != -1)</blue>
<blue>		smp_call_function_single(cpu,</blue>
			 __loaded_vmcs_clear, loaded_vmcs, 1);
<yellow>}</yellow>

static bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,
				       unsigned field)
{
	bool ret;
<blue>	u32 mask = 1 << (seg * SEG_FIELD_NR + field);</blue>

	if (!kvm_register_is_available(&amp;vmx-&gt;vcpu, VCPU_EXREG_SEGMENTS)) {
<blue>		kvm_register_mark_available(&vmx->vcpu, VCPU_EXREG_SEGMENTS);</blue>
		vmx-&gt;segment_cache.bitmask = 0;
	}
<blue>	ret = vmx->segment_cache.bitmask & mask;</blue>
<blue>	vmx->segment_cache.bitmask |= mask;</blue>
	return ret;
}

static u16 vmx_read_guest_seg_selector(struct vcpu_vmx *vmx, unsigned seg)
{
<blue>	u16 *p = &vmx->segment_cache.seg[seg].selector;</blue>

	if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_SEL))
<blue>		*p = vmcs_read16(kvm_vmx_segment_fields[seg].selector);</blue>
<blue>	return *p;</blue>
<blue>}</blue>

static ulong vmx_read_guest_seg_base(struct vcpu_vmx *vmx, unsigned seg)
{
<blue>	ulong *p = &vmx->segment_cache.seg[seg].base;</blue>

	if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_BASE))
<blue>		*p = vmcs_readl(kvm_vmx_segment_fields[seg].base);</blue>
<blue>	return *p;</blue>
<blue>}</blue>

static u32 vmx_read_guest_seg_limit(struct vcpu_vmx *vmx, unsigned seg)
{
	u32 *p = &amp;vmx-&gt;segment_cache.seg[seg].limit;

	if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_LIMIT))
<blue>		*p = vmcs_read32(kvm_vmx_segment_fields[seg].limit);</blue>
<blue>	return *p;</blue>
}

static u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)
{
<blue>	u32 *p = &vmx->segment_cache.seg[seg].ar;</blue>

	if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_AR))
<blue>		*p = vmcs_read32(kvm_vmx_segment_fields[seg].ar_bytes);</blue>
<blue>	return *p;</blue>
<blue>}</blue>

void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
{
	u32 eb;

	eb = (1u &lt;&lt; PF_VECTOR) | (1u &lt;&lt; UD_VECTOR) | (1u &lt;&lt; MC_VECTOR) |
	     (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);
	/*
	 * Guest access to VMware backdoor ports could legitimately
	 * trigger #GP because of TSS I/O permission bitmap.
	 * We intercept those #GP and allow access to them anyway
	 * as VMware does.
	 */
<blue>	if (enable_vmware_backdoor)</blue>
		eb |= (1u &lt;&lt; GP_VECTOR);
<blue>	if ((vcpu->guest_debug &</blue>
	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==
	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))
<blue>		eb |= 1u << BP_VECTOR;</blue>
<blue>	if (to_vmx(vcpu)->rmode.vm86_active)</blue>
		eb = ~0;
<blue>	if (!vmx_need_pf_intercept(vcpu))</blue>
<blue>		eb &= ~(1u << PF_VECTOR);</blue>

	/* When we are running a nested L2 guest and L1 specified for it a
	 * certain exception bitmap, we must trap the same exceptions and pass
	 * them to L1. When running L2, we will only handle the exceptions
	 * specified above if L1 did not want them.
	 */
<blue>	if (is_guest_mode(vcpu))</blue>
<blue>		eb |= get_vmcs12(vcpu)->exception_bitmap;</blue>
        else {
		int mask = 0, match = 0;

<blue>		if (enable_ept && (eb & (1u << PF_VECTOR))) {</blue>
			/*
			 * If EPT is enabled, #PF is currently only intercepted
			 * if MAXPHYADDR is smaller on the guest than on the
			 * host.  In that case we only care about present,
			 * non-reserved faults.  For vmcs02, however, PFEC_MASK
			 * and PFEC_MATCH are set in prepare_vmcs02_rare.
			 */
			mask = PFERR_PRESENT_MASK | PFERR_RSVD_MASK;
			match = PFERR_PRESENT_MASK;
		}
<blue>		vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, mask);</blue>
<blue>		vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, match);</blue>
	}

	/*
	 * Disabling xfd interception indicates that dynamic xfeatures
	 * might be used in the guest. Always trap #NM in this case
	 * to save guest xfd_err timely.
	 */
<blue>	if (vcpu->arch.xfd_no_write_intercept)</blue>
<yellow>		eb |= (1u << NM_VECTOR);</yellow>

<blue>	vmcs_write32(EXCEPTION_BITMAP, eb);</blue>
<blue>}</blue>

/*
 * Check if MSR is intercepted for currently loaded MSR bitmap.
 */
static bool msr_write_intercepted(struct vcpu_vmx *vmx, u32 msr)
{
<blue>	if (!(exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS))</blue>
		return true;

<blue>	return vmx_test_msr_bitmap_write(vmx->loaded_vmcs->msr_bitmap, msr);</blue>
}

unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)
{
	unsigned int flags = 0;

<blue>	if (vmx->loaded_vmcs->launched)</blue>
		flags |= VMX_RUN_VMRESUME;

	/*
	 * If writes to the SPEC_CTRL MSR aren&#x27;t intercepted, the guest is free
	 * to change it directly without causing a vmexit.  In that case read
	 * it after vmexit and store it in vmx-&gt;spec_ctrl.
	 */
<blue>	if (unlikely(!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL)))</blue>
<yellow>		flags |= VMX_RUN_SAVE_SPEC_CTRL;</yellow>

	return flags;
<blue>}</blue>

static __always_inline void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,
		unsigned long entry, unsigned long exit)
{
<blue>	vm_entry_controls_clearbit(vmx, entry);</blue>
<blue>	vm_exit_controls_clearbit(vmx, exit);</blue>
}

int vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr)
{
	unsigned int i;

<blue>	for (i = 0; i < m->nr; ++i) {</blue>
<yellow>		if (m->val[i].index == msr)</yellow>
<yellow>			return i;</yellow>
	}
	return -ENOENT;
<blue>}</blue>

static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
{
	int i;
	struct msr_autoload *m = &amp;vmx-&gt;msr_autoload;

<blue>	switch (msr) {</blue>
	case MSR_EFER:
<blue>		if (cpu_has_load_ia32_efer()) {</blue>
<blue>			clear_atomic_switch_msr_special(vmx,</blue>
					VM_ENTRY_LOAD_IA32_EFER,
					VM_EXIT_LOAD_IA32_EFER);
			return;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		if (cpu_has_load_perf_global_ctrl()) {</blue>
<blue>			clear_atomic_switch_msr_special(vmx,</blue>
					VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,
					VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
			return;
		}
		break;
	}
<yellow>	i = vmx_find_loadstore_msr_slot(&m->guest, msr);</yellow>
<yellow>	if (i < 0)</yellow>
		goto skip_guest;
<yellow>	--m->guest.nr;</yellow>
	m-&gt;guest.val[i] = m-&gt;guest.val[m-&gt;guest.nr];
<yellow>	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);</yellow>

skip_guest:
<yellow>	i = vmx_find_loadstore_msr_slot(&m->host, msr);</yellow>
<yellow>	if (i < 0)</yellow>
		return;

	--m-&gt;host.nr;
<yellow>	m->host.val[i] = m->host.val[m->host.nr];</yellow>
<yellow>	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);</yellow>
<blue>}</blue>

static __always_inline void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
		unsigned long entry, unsigned long exit,
		unsigned long guest_val_vmcs, unsigned long host_val_vmcs,
		u64 guest_val, u64 host_val)
{
<blue>	vmcs_write64(guest_val_vmcs, guest_val);</blue>
	if (host_val_vmcs != HOST_IA32_EFER)
<yellow>		vmcs_write64(host_val_vmcs, host_val);</yellow>
<blue>	vm_entry_controls_setbit(vmx, entry);</blue>
<blue>	vm_exit_controls_setbit(vmx, exit);</blue>
}

static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
				  u64 guest_val, u64 host_val, bool entry_only)
{
	int i, j = 0;
	struct msr_autoload *m = &amp;vmx-&gt;msr_autoload;

<blue>	switch (msr) {</blue>
	case MSR_EFER:
<blue>		if (cpu_has_load_ia32_efer()) {</blue>
<blue>			add_atomic_switch_msr_special(vmx,</blue>
					VM_ENTRY_LOAD_IA32_EFER,
					VM_EXIT_LOAD_IA32_EFER,
					GUEST_IA32_EFER,
					HOST_IA32_EFER,
					guest_val, host_val);
			return;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_CTRL:
<yellow>		if (cpu_has_load_perf_global_ctrl()) {</yellow>
<yellow>			add_atomic_switch_msr_special(vmx,</yellow>
					VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,
					VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,
					GUEST_IA32_PERF_GLOBAL_CTRL,
					HOST_IA32_PERF_GLOBAL_CTRL,
					guest_val, host_val);
			return;
		}
		break;
	case MSR_IA32_PEBS_ENABLE:
		/* PEBS needs a quiescent period after being disabled (to write
		 * a record).  Disabling PEBS through VMX MSR swapping doesn&#x27;t
		 * provide that period, so a CPU could write host&#x27;s record into
		 * guest&#x27;s memory.
		 */
<yellow>		wrmsrl(MSR_IA32_PEBS_ENABLE, 0);</yellow>
	}

<yellow>	i = vmx_find_loadstore_msr_slot(&m->guest, msr);</yellow>
	if (!entry_only)
<yellow>		j = vmx_find_loadstore_msr_slot(&m->host, msr);</yellow>

<yellow>	if ((i < 0 && m->guest.nr == MAX_NR_LOADSTORE_MSRS) ||</yellow>
<yellow>	    (j < 0 &&  m->host.nr == MAX_NR_LOADSTORE_MSRS)) {</yellow>
<yellow>		printk_once(KERN_WARNING "Not enough msr switch entries. "</yellow>
				&quot;Can&#x27;t add msr %x\n&quot;, msr);
		return;
	}
<yellow>	if (i < 0) {</yellow>
<yellow>		i = m->guest.nr++;</yellow>
<yellow>		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);</yellow>
	}
<yellow>	m->guest.val[i].index = msr;</yellow>
	m-&gt;guest.val[i].value = guest_val;

	if (entry_only)
		return;

	if (j &lt; 0) {
<yellow>		j = m->host.nr++;</yellow>
<yellow>		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);</yellow>
	}
<yellow>	m->host.val[j].index = msr;</yellow>
	m-&gt;host.val[j].value = host_val;
<blue>}</blue>

static bool update_transition_efer(struct vcpu_vmx *vmx)
{
<blue>	u64 guest_efer = vmx->vcpu.arch.efer;</blue>
	u64 ignore_bits = 0;
	int i;

	/* Shadow paging assumes NX to be available.  */
<blue>	if (!enable_ept)</blue>
<yellow>		guest_efer |= EFER_NX;</yellow>

	/*
	 * LMA and LME handled by hardware; SCE meaningless outside long mode.
	 */
	ignore_bits |= EFER_SCE;
#ifdef CONFIG_X86_64
	ignore_bits |= EFER_LMA | EFER_LME;
	/* SCE is meaningful only in long mode on Intel */
<blue>	if (guest_efer & EFER_LMA)</blue>
		ignore_bits &amp;= ~(u64)EFER_SCE;
#endif

	/*
	 * On EPT, we can&#x27;t emulate NX, so we must switch EFER atomically.
	 * On CPUs that support &quot;load IA32_EFER&quot;, always switch EFER
	 * atomically, since it&#x27;s faster than switching it manually.
	 */
<blue>	if (cpu_has_load_ia32_efer() ||</blue>
<yellow>	    (enable_ept && ((vmx->vcpu.arch.efer ^ host_efer) & EFER_NX))) {</yellow>
<yellow>		if (!(guest_efer & EFER_LMA))</yellow>
<blue>			guest_efer &= ~EFER_LME;</blue>
<blue>		if (guest_efer != host_efer)</blue>
<blue>			add_atomic_switch_msr(vmx, MSR_EFER,</blue>
					      guest_efer, host_efer, false);
		else
<blue>			clear_atomic_switch_msr(vmx, MSR_EFER);</blue>
		return false;
	}

<yellow>	i = kvm_find_user_return_msr(MSR_EFER);</yellow>
	if (i &lt; 0)
		return false;

<yellow>	clear_atomic_switch_msr(vmx, MSR_EFER);</yellow>

	guest_efer &amp;= ~ignore_bits;
	guest_efer |= host_efer &amp; ignore_bits;

	vmx-&gt;guest_uret_msrs[i].data = guest_efer;
	vmx-&gt;guest_uret_msrs[i].mask = ~ignore_bits;

	return true;
}

#ifdef CONFIG_X86_32
/*
 * On 32-bit kernels, VM exits still load the FS and GS bases from the
 * VMCS rather than the segment table.  KVM uses this helper to figure
 * out the current bases to poke them into the VMCS before entry.
 */
static unsigned long segment_base(u16 selector)
{
	struct desc_struct *table;
	unsigned long v;

	if (!(selector &amp; ~SEGMENT_RPL_MASK))
		return 0;

	table = get_current_gdt_ro();

	if ((selector &amp; SEGMENT_TI_MASK) == SEGMENT_LDT) {
		u16 ldt_selector = kvm_read_ldt();

		if (!(ldt_selector &amp; ~SEGMENT_RPL_MASK))
			return 0;

		table = (struct desc_struct *)segment_base(ldt_selector);
	}
	v = get_desc_base(&amp;table[selector &gt;&gt; 3]);
	return v;
}
#endif

static inline bool pt_can_write_msr(struct vcpu_vmx *vmx)
{
<blue>	return vmx_pt_mode_is_host_guest() &&</blue>
<yellow>	       !(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN);</yellow>
}

static inline bool pt_output_base_valid(struct kvm_vcpu *vcpu, u64 base)
{
	/* The base must be 128-byte aligned and a legal physical address. */
<yellow>	return kvm_vcpu_is_legal_aligned_gpa(vcpu, base, 128);</yellow>
}

static inline void pt_load_msr(struct pt_ctx *ctx, u32 addr_range)
{
	u32 i;

<yellow>	wrmsrl(MSR_IA32_RTIT_STATUS, ctx->status);</yellow>
	wrmsrl(MSR_IA32_RTIT_OUTPUT_BASE, ctx-&gt;output_base);
	wrmsrl(MSR_IA32_RTIT_OUTPUT_MASK, ctx-&gt;output_mask);
	wrmsrl(MSR_IA32_RTIT_CR3_MATCH, ctx-&gt;cr3_match);
	for (i = 0; i &lt; addr_range; i++) {
<yellow>		wrmsrl(MSR_IA32_RTIT_ADDR0_A + i * 2, ctx->addr_a[i]);</yellow>
		wrmsrl(MSR_IA32_RTIT_ADDR0_B + i * 2, ctx-&gt;addr_b[i]);
	}
<yellow>}</yellow>

static inline void pt_save_msr(struct pt_ctx *ctx, u32 addr_range)
{
	u32 i;

	rdmsrl(MSR_IA32_RTIT_STATUS, ctx-&gt;status);
	rdmsrl(MSR_IA32_RTIT_OUTPUT_BASE, ctx-&gt;output_base);
	rdmsrl(MSR_IA32_RTIT_OUTPUT_MASK, ctx-&gt;output_mask);
	rdmsrl(MSR_IA32_RTIT_CR3_MATCH, ctx-&gt;cr3_match);
	for (i = 0; i &lt; addr_range; i++) {
<yellow>		rdmsrl(MSR_IA32_RTIT_ADDR0_A + i * 2, ctx->addr_a[i]);</yellow>
		rdmsrl(MSR_IA32_RTIT_ADDR0_B + i * 2, ctx-&gt;addr_b[i]);
	}
}

static void pt_guest_enter(struct vcpu_vmx *vmx)
{
	if (vmx_pt_mode_is_system())
		return;

	/*
	 * GUEST_IA32_RTIT_CTL is already set in the VMCS.
	 * Save host state before VM entry.
	 */
<yellow>	rdmsrl(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);</yellow>
	if (vmx-&gt;pt_desc.guest.ctl &amp; RTIT_CTL_TRACEEN) {
<yellow>		wrmsrl(MSR_IA32_RTIT_CTL, 0);</yellow>
<yellow>		pt_save_msr(&vmx->pt_desc.host, vmx->pt_desc.num_address_ranges);</yellow>
<yellow>		pt_load_msr(&vmx->pt_desc.guest, vmx->pt_desc.num_address_ranges);</yellow>
	}
}

static void pt_guest_exit(struct vcpu_vmx *vmx)
{
	if (vmx_pt_mode_is_system())
		return;

<yellow>	if (vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) {</yellow>
<yellow>		pt_save_msr(&vmx->pt_desc.guest, vmx->pt_desc.num_address_ranges);</yellow>
<yellow>		pt_load_msr(&vmx->pt_desc.host, vmx->pt_desc.num_address_ranges);</yellow>
	}

	/*
	 * KVM requires VM_EXIT_CLEAR_IA32_RTIT_CTL to expose PT to the guest,
	 * i.e. RTIT_CTL is always cleared on VM-Exit.  Restore it if necessary.
	 */
<yellow>	if (vmx->pt_desc.host.ctl)</yellow>
<yellow>		wrmsrl(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);</yellow>
}

void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
			unsigned long fs_base, unsigned long gs_base)
{
<blue>	if (unlikely(fs_sel != host->fs_sel)) {</blue>
<yellow>		if (!(fs_sel & 7))</yellow>
<yellow>			vmcs_write16(HOST_FS_SELECTOR, fs_sel);</yellow>
		else
<yellow>			vmcs_write16(HOST_FS_SELECTOR, 0);</yellow>
<yellow>		host->fs_sel = fs_sel;</yellow>
	}
<blue>	if (unlikely(gs_sel != host->gs_sel)) {</blue>
<yellow>		if (!(gs_sel & 7))</yellow>
<yellow>			vmcs_write16(HOST_GS_SELECTOR, gs_sel);</yellow>
		else
<yellow>			vmcs_write16(HOST_GS_SELECTOR, 0);</yellow>
<yellow>		host->gs_sel = gs_sel;</yellow>
	}
<blue>	if (unlikely(fs_base != host->fs_base)) {</blue>
<blue>		vmcs_writel(HOST_FS_BASE, fs_base);</blue>
<blue>		host->fs_base = fs_base;</blue>
	}
<blue>	if (unlikely(gs_base != host->gs_base)) {</blue>
<blue>		vmcs_writel(HOST_GS_BASE, gs_base);</blue>
<blue>		host->gs_base = gs_base;</blue>
	}
<blue>}</blue>

void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	struct vmcs_host_state *host_state;
#ifdef CONFIG_X86_64
<blue>	int cpu = raw_smp_processor_id();</blue>
#endif
	unsigned long fs_base, gs_base;
	u16 fs_sel, gs_sel;
	int i;

	vmx-&gt;req_immediate_exit = false;

	/*
	 * Note that guest MSRs to be saved/restored can also be changed
	 * when guest state is loaded. This happens when guest transitions
	 * to/from long-mode by setting MSR_EFER.LMA.
	 */
<blue>	if (!vmx->guest_uret_msrs_loaded) {</blue>
<blue>		vmx->guest_uret_msrs_loaded = true;</blue>
<blue>		for (i = 0; i < kvm_nr_uret_msrs; ++i) {</blue>
<blue>			if (!vmx->guest_uret_msrs[i].load_into_hardware)</blue>
				continue;

			kvm_set_user_return_msr(i,
						vmx-&gt;guest_uret_msrs[i].data,
<blue>						vmx->guest_uret_msrs[i].mask);</blue>
		}
	}

<blue>    	if (vmx->nested.need_vmcs12_to_shadow_sync)</blue>
<blue>		nested_sync_vmcs12_to_shadow(vcpu);</blue>

<blue>	if (vmx->guest_state_loaded)</blue>
		return;

<blue>	host_state = &vmx->loaded_vmcs->host_state;</blue>

	/*
	 * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not
	 * allow segment selectors with cpl &gt; 0 or ti == 1.
	 */
	host_state-&gt;ldt_sel = kvm_read_ldt();

#ifdef CONFIG_X86_64
	savesegment(ds, host_state-&gt;ds_sel);
	savesegment(es, host_state-&gt;es_sel);

	gs_base = cpu_kernelmode_gs_base(cpu);
	if (likely(is_64bit_mm(current-&gt;mm))) {
<blue>		current_save_fsgs();</blue>
		fs_sel = current-&gt;thread.fsindex;
		gs_sel = current-&gt;thread.gsindex;
		fs_base = current-&gt;thread.fsbase;
<blue>		vmx->msr_host_kernel_gs_base = current->thread.gsbase;</blue>
	} else {
		savesegment(fs, fs_sel);
		savesegment(gs, gs_sel);
<yellow>		fs_base = read_msr(MSR_FS_BASE);</yellow>
		vmx-&gt;msr_host_kernel_gs_base = read_msr(MSR_KERNEL_GS_BASE);
	}

	wrmsrl(MSR_KERNEL_GS_BASE, vmx-&gt;msr_guest_kernel_gs_base);
#else
	savesegment(fs, fs_sel);
	savesegment(gs, gs_sel);
	fs_base = segment_base(fs_sel);
	gs_base = segment_base(gs_sel);
#endif

	vmx_set_host_fs_gs(host_state, fs_sel, gs_sel, fs_base, gs_base);
	vmx-&gt;guest_state_loaded = true;
<blue>}</blue>

static void vmx_prepare_switch_to_host(struct vcpu_vmx *vmx)
{
	struct vmcs_host_state *host_state;

<blue>	if (!vmx->guest_state_loaded)</blue>
		return;

<blue>	host_state = &vmx->loaded_vmcs->host_state;</blue>

	++vmx-&gt;vcpu.stat.host_state_reload;

#ifdef CONFIG_X86_64
	rdmsrl(MSR_KERNEL_GS_BASE, vmx-&gt;msr_guest_kernel_gs_base);
#endif
<blue>	if (host_state->ldt_sel || (host_state->gs_sel & 7)) {</blue>
<yellow>		kvm_load_ldt(host_state->ldt_sel);</yellow>
#ifdef CONFIG_X86_64
		load_gs_index(host_state-&gt;gs_sel);
#else
		loadsegment(gs, host_state-&gt;gs_sel);
#endif
	}
<blue>	if (host_state->fs_sel & 7)</blue>
<yellow>		loadsegment(fs, host_state->fs_sel);</yellow>
#ifdef CONFIG_X86_64
<blue>	if (unlikely(host_state->ds_sel | host_state->es_sel)) {</blue>
<yellow>		loadsegment(ds, host_state->ds_sel);</yellow>
		loadsegment(es, host_state-&gt;es_sel);
	}
#endif
<blue>	invalidate_tss_limit();</blue>
#ifdef CONFIG_X86_64
<blue>	wrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);</blue>
#endif
	load_fixmap_gdt(raw_smp_processor_id());
	vmx-&gt;guest_state_loaded = false;
	vmx-&gt;guest_uret_msrs_loaded = false;
}

#ifdef CONFIG_X86_64
static u64 vmx_read_guest_kernel_gs_base(struct vcpu_vmx *vmx)
{
<blue>	preempt_disable();</blue>
<blue>	if (vmx->guest_state_loaded)</blue>
<yellow>		rdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);</yellow>
<blue>	preempt_enable();</blue>
<blue>	return vmx->msr_guest_kernel_gs_base;</blue>
}

static void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)
{
<blue>	preempt_disable();</blue>
<blue>	if (vmx->guest_state_loaded)</blue>
<yellow>		wrmsrl(MSR_KERNEL_GS_BASE, data);</yellow>
<blue>	preempt_enable();</blue>
<blue>	vmx->msr_guest_kernel_gs_base = data;</blue>
}
#endif

void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
			struct loaded_vmcs *buddy)
<blue>{</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<blue>	bool already_loaded = vmx->loaded_vmcs->cpu == cpu;</blue>
	struct vmcs *prev;

	if (!already_loaded) {
<blue>		loaded_vmcs_clear(vmx->loaded_vmcs);</blue>
<blue>		local_irq_disable();</blue>

		/*
		 * Ensure loaded_vmcs-&gt;cpu is read before adding loaded_vmcs to
		 * this cpu&#x27;s percpu list, otherwise it may not yet be deleted
		 * from its previous cpu&#x27;s percpu list.  Pairs with the
		 * smb_wmb() in __loaded_vmcs_clear().
		 */
		smp_rmb();

		list_add(&amp;vmx-&gt;loaded_vmcs-&gt;loaded_vmcss_on_cpu_link,
			 &amp;per_cpu(loaded_vmcss_on_cpu, cpu));
		local_irq_enable();
	}

<blue>	prev = per_cpu(current_vmcs, cpu);</blue>
	if (prev != vmx-&gt;loaded_vmcs-&gt;vmcs) {
<blue>		per_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;</blue>
<blue>		vmcs_load(vmx->loaded_vmcs->vmcs);</blue>

		/*
		 * No indirect branch prediction barrier needed when switching
		 * the active VMCS within a guest, e.g. on nested VM-Enter.
		 * The L1 VMM can protect itself with retpolines, IBPB or IBRS.
		 */
<blue>		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))</blue>
<blue>			indirect_branch_prediction_barrier();</blue>
	}

<blue>	if (!already_loaded) {</blue>
<blue>		void *gdt = get_current_gdt_ro();</blue>

		/*
		 * Flush all EPTP/VPID contexts, the new pCPU may have stale
		 * TLB entries from its previous association with the vCPU.
		 */
		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);

		/*
		 * Linux uses per-cpu TSS and GDT, so set these when switching
		 * processors.  See 22.2.4.
		 */
<blue>		vmcs_writel(HOST_TR_BASE,</blue>
			    (unsigned long)&amp;get_cpu_entry_area(cpu)-&gt;tss.x86_tss);
<blue>		vmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */</blue>

		if (IS_ENABLED(CONFIG_IA32_EMULATION) || IS_ENABLED(CONFIG_X86_32)) {
			/* 22.2.3 */
<blue>			vmcs_writel(HOST_IA32_SYSENTER_ESP,</blue>
<blue>				    (unsigned long)(cpu_entry_stack(cpu) + 1));</blue>
		}

<blue>		vmx->loaded_vmcs->cpu = cpu;</blue>
	}
}

/*
 * Switches to specified vcpu, until a matching vcpu_put(), but assumes
 * vcpu mutex is already taken.
 */
static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	vmx_vcpu_load_vmcs(vcpu, cpu, NULL);</blue>

	vmx_vcpu_pi_load(vcpu, cpu);

	vmx-&gt;host_debugctlmsr = get_debugctlmsr();
}

static void vmx_vcpu_put(struct kvm_vcpu *vcpu)
<blue>{</blue>
<blue>	vmx_vcpu_pi_put(vcpu);</blue>

<blue>	vmx_prepare_switch_to_host(to_vmx(vcpu));</blue>
}

bool vmx_emulation_required(struct kvm_vcpu *vcpu)
<blue>{</blue>
<blue>	return emulate_invalid_guest_state && !vmx_guest_state_valid(vcpu);</blue>
<blue>}</blue>

unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	unsigned long rflags, save_rflags;

<blue>	if (!kvm_register_is_available(vcpu, VCPU_EXREG_RFLAGS)) {</blue>
<blue>		kvm_register_mark_available(vcpu, VCPU_EXREG_RFLAGS);</blue>
<blue>		rflags = vmcs_readl(GUEST_RFLAGS);</blue>
<blue>		if (vmx->rmode.vm86_active) {</blue>
			rflags &amp;= RMODE_GUEST_OWNED_EFLAGS_BITS;
<yellow>			save_rflags = vmx->rmode.save_rflags;</yellow>
			rflags |= save_rflags &amp; ~RMODE_GUEST_OWNED_EFLAGS_BITS;
		}
<blue>		vmx->rflags = rflags;</blue>
	}
<blue>	return vmx->rflags;</blue>
<blue>}</blue>

void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	unsigned long old_rflags;

<blue>	if (is_unrestricted_guest(vcpu)) {</blue>
<blue>		kvm_register_mark_available(vcpu, VCPU_EXREG_RFLAGS);</blue>
		vmx-&gt;rflags = rflags;
<blue>		vmcs_writel(GUEST_RFLAGS, rflags);</blue>
		return;
	}

	old_rflags = vmx_get_rflags(vcpu);
<blue>	vmx->rflags = rflags;</blue>
	if (vmx-&gt;rmode.vm86_active) {
<yellow>		vmx->rmode.save_rflags = rflags;</yellow>
		rflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;
	}
<blue>	vmcs_writel(GUEST_RFLAGS, rflags);</blue>

<blue>	if ((old_rflags ^ vmx->rflags) & X86_EFLAGS_VM)</blue>
<blue>		vmx->emulation_required = vmx_emulation_required(vcpu);</blue>
<blue>}</blue>

static bool vmx_get_if_flag(struct kvm_vcpu *vcpu)
{
<blue>	return vmx_get_rflags(vcpu) & X86_EFLAGS_IF;</blue>
}

u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
{
<blue>	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);</blue>
	int ret = 0;

<blue>	if (interruptibility & GUEST_INTR_STATE_STI)</blue>
		ret |= KVM_X86_SHADOW_INT_STI;
<blue>	if (interruptibility & GUEST_INTR_STATE_MOV_SS)</blue>
<blue>		ret |= KVM_X86_SHADOW_INT_MOV_SS;</blue>

<blue>	return ret;</blue>
<blue>}</blue>

void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
{
<blue>	u32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);</blue>
	u32 interruptibility = interruptibility_old;

	interruptibility &amp;= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);

<blue>	if (mask & KVM_X86_SHADOW_INT_MOV_SS)</blue>
<blue>		interruptibility |= GUEST_INTR_STATE_MOV_SS;</blue>
<blue>	else if (mask & KVM_X86_SHADOW_INT_STI)</blue>
<blue>		interruptibility |= GUEST_INTR_STATE_STI;</blue>

<blue>	if ((interruptibility != interruptibility_old))</blue>
<blue>		vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);</blue>
<blue>}</blue>

static int vmx_rtit_ctl_check(struct kvm_vcpu *vcpu, u64 data)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	unsigned long value;

	/*
	 * Any MSR write that attempts to change bits marked reserved will
	 * case a #GP fault.
	 */
<yellow>	if (data & vmx->pt_desc.ctl_bitmask)</yellow>
		return 1;

	/*
	 * Any attempt to modify IA32_RTIT_CTL while TraceEn is set will
	 * result in a #GP unless the same write also clears TraceEn.
	 */
<yellow>	if ((vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) &&</yellow>
<yellow>		((vmx->pt_desc.guest.ctl ^ data) & ~RTIT_CTL_TRACEEN))</yellow>
		return 1;

	/*
	 * WRMSR to IA32_RTIT_CTL that sets TraceEn but clears this bit
	 * and FabricEn would cause #GP, if
	 * CPUID.(EAX=14H, ECX=0):ECX.SNGLRGNOUT[bit 2] = 0
	 */
<yellow>	if ((data & RTIT_CTL_TRACEEN) && !(data & RTIT_CTL_TOPA) &&</yellow>
		!(data &amp; RTIT_CTL_FABRIC_EN) &amp;&amp;
<yellow>		!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					PT_CAP_single_range_output))
		return 1;

	/*
	 * MTCFreq, CycThresh and PSBFreq encodings check, any MSR write that
	 * utilize encodings marked reserved will cause a #GP fault.
	 */
<yellow>	value = intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc_periods);</yellow>
	if (intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_mtc) &amp;&amp;
<yellow>			!test_bit((data & RTIT_CTL_MTC_RANGE) >></yellow>
			RTIT_CTL_MTC_RANGE_OFFSET, &amp;value))
		return 1;
<yellow>	value = intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
						PT_CAP_cycle_thresholds);
	if (intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_psb_cyc) &amp;&amp;
<yellow>			!test_bit((data & RTIT_CTL_CYC_THRESH) >></yellow>
			RTIT_CTL_CYC_THRESH_OFFSET, &amp;value))
		return 1;
<yellow>	value = intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_periods);</yellow>
	if (intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_psb_cyc) &amp;&amp;
<yellow>			!test_bit((data & RTIT_CTL_PSB_FREQ) >></yellow>
			RTIT_CTL_PSB_FREQ_OFFSET, &amp;value))
		return 1;

	/*
	 * If ADDRx_CFG is reserved or the encodings is &gt;2 will
	 * cause a #GP fault.
	 */
<yellow>	value = (data & RTIT_CTL_ADDR0) >> RTIT_CTL_ADDR0_OFFSET;</yellow>
<yellow>	if ((value && (vmx->pt_desc.num_address_ranges < 1)) || (value > 2))</yellow>
		return 1;
<yellow>	value = (data & RTIT_CTL_ADDR1) >> RTIT_CTL_ADDR1_OFFSET;</yellow>
<yellow>	if ((value && (vmx->pt_desc.num_address_ranges < 2)) || (value > 2))</yellow>
		return 1;
<yellow>	value = (data & RTIT_CTL_ADDR2) >> RTIT_CTL_ADDR2_OFFSET;</yellow>
<yellow>	if ((value && (vmx->pt_desc.num_address_ranges < 3)) || (value > 2))</yellow>
		return 1;
<yellow>	value = (data & RTIT_CTL_ADDR3) >> RTIT_CTL_ADDR3_OFFSET;</yellow>
<yellow>	if ((value && (vmx->pt_desc.num_address_ranges < 4)) || (value > 2))</yellow>
<yellow>		return 1;</yellow>

	return 0;
}

static bool vmx_can_emulate_instruction(struct kvm_vcpu *vcpu, int emul_type,
					void *insn, int insn_len)
{
	/*
	 * Emulation of instructions in SGX enclaves is impossible as RIP does
	 * not point at the failing instruction, and even if it did, the code
	 * stream is inaccessible.  Inject #UD instead of exiting to userspace
	 * so that guest userspace can&#x27;t DoS the guest simply by triggering
	 * emulation (enclaves are CPL3 only).
	 */
<blue>	if (to_vmx(vcpu)->exit_reason.enclave_mode) {</blue>
<yellow>		kvm_queue_exception(vcpu, UD_VECTOR);</yellow>
		return false;
	}
	return true;
<blue>}</blue>

static int skip_emulated_instruction(struct kvm_vcpu *vcpu)
<blue>{</blue>
<blue>	union vmx_exit_reason exit_reason = to_vmx(vcpu)->exit_reason;</blue>
	unsigned long rip, orig_rip;
	u32 instr_len;

	/*
	 * Using VMCS.VM_EXIT_INSTRUCTION_LEN on EPT misconfig depends on
	 * undefined behavior: Intel&#x27;s SDM doesn&#x27;t mandate the VMCS field be
	 * set when EPT misconfig occurs.  In practice, real hardware updates
	 * VM_EXIT_INSTRUCTION_LEN on EPT misconfig, but other hypervisors
	 * (namely Hyper-V) don&#x27;t set it due to it being undefined behavior,
	 * i.e. we end up advancing IP with some random value.
	 */
<blue>	if (!static_cpu_has(X86_FEATURE_HYPERVISOR) ||</blue>
	    exit_reason.basic != EXIT_REASON_EPT_MISCONFIG) {
<blue>		instr_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);</blue>

		/*
		 * Emulating an enclave&#x27;s instructions isn&#x27;t supported as KVM
		 * cannot access the enclave&#x27;s memory or its true RIP, e.g. the
		 * vmcs.GUEST_RIP points at the exit point of the enclave, not
		 * the RIP that actually triggered the VM-Exit.  But, because
		 * most instructions that cause VM-Exit will #UD in an enclave,
		 * most instruction-based VM-Exits simply do not occur.
		 *
		 * There are a few exceptions, notably the debug instructions
		 * INT1ICEBRK and INT3, as they are allowed in debug enclaves
		 * and generate #DB/#BP as expected, which KVM might intercept.
		 * But again, the CPU does the dirty work and saves an instr
		 * length of zero so VMMs don&#x27;t shoot themselves in the foot.
		 * WARN if KVM tries to skip a non-zero length instruction on
		 * a VM-Exit from an enclave.
		 */
<blue>		if (!instr_len)</blue>
			goto rip_updated;

<blue>		WARN(exit_reason.enclave_mode,</blue>
		     &quot;KVM: skipping instruction after SGX enclave VM-Exit&quot;);

<blue>		orig_rip = kvm_rip_read(vcpu);</blue>
		rip = orig_rip + instr_len;
#ifdef CONFIG_X86_64
		/*
		 * We need to mask out the high 32 bits of RIP if not in 64-bit
		 * mode, but just finding out that we are in 64-bit mode is
		 * quite expensive.  Only do it if there was a carry.
		 */
<yellow>		if (unlikely(((rip ^ orig_rip) >> 31) == 3) && !is_64_bit_mode(vcpu))</yellow>
<yellow>			rip = (u32)rip;</yellow>
#endif
<blue>		kvm_rip_write(vcpu, rip);</blue>
	} else {
<yellow>		if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))</yellow>
			return 0;
	}

rip_updated:
	/* skipping an emulated instruction also counts */
<blue>	vmx_set_interrupt_shadow(vcpu, 0);</blue>

	return 1;
}

/*
 * Recognizes a pending MTF VM-exit and records the nested state for later
 * delivery.
 */
static void vmx_update_emulated_instruction(struct kvm_vcpu *vcpu)
{
<blue>	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (!is_guest_mode(vcpu))</blue>
		return;

	/*
	 * Per the SDM, MTF takes priority over debug-trap exceptions besides
	 * TSS T-bit traps and ICEBP (INT1).  KVM doesn&#x27;t emulate T-bit traps
	 * or ICEBP (in the emulator proper), and skipping of ICEBP after an
	 * intercepted #DB deliberately avoids single-step #DB and MTF updates
	 * as ICEBP is higher priority than both.  As instruction emulation is
	 * completed at this point (i.e. KVM is at the instruction boundary),
	 * any #DB exception pending delivery must be a debug-trap of lower
	 * priority than MTF.  Record the pending MTF state to be delivered in
	 * vmx_check_nested_events().
	 */
<yellow>	if (nested_cpu_has_mtf(vmcs12) &&</yellow>
<yellow>	    (!vcpu->arch.exception.pending ||</yellow>
<yellow>	     vcpu->arch.exception.vector == DB_VECTOR) &&</yellow>
<yellow>	    (!vcpu->arch.exception_vmexit.pending ||</yellow>
<yellow>	     vcpu->arch.exception_vmexit.vector == DB_VECTOR)) {</yellow>
<yellow>		vmx->nested.mtf_pending = true;</yellow>
		kvm_make_request(KVM_REQ_EVENT, vcpu);
	} else {
<blue>		vmx->nested.mtf_pending = false;</blue>
	}
<blue>}</blue>

static int vmx_skip_emulated_instruction(struct kvm_vcpu *vcpu)
{
<blue>	vmx_update_emulated_instruction(vcpu);</blue>
	return skip_emulated_instruction(vcpu);
}

static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
{
	/*
	 * Ensure that we clear the HLT state in the VMCS.  We don&#x27;t need to
	 * explicitly skip the instruction because if the HLT state is set,
	 * then the instruction is already executing and RIP has already been
	 * advanced.
	 */
<blue>	if (kvm_hlt_in_guest(vcpu->kvm) &&</blue>
<blue>			vmcs_read32(GUEST_ACTIVITY_STATE) == GUEST_ACTIVITY_HLT)</blue>
<yellow>		vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);</yellow>
<blue>}</blue>

static void vmx_inject_exception(struct kvm_vcpu *vcpu)
{
	struct kvm_queued_exception *ex = &amp;vcpu-&gt;arch.exception;
<blue>	u32 intr_info = ex->vector | INTR_INFO_VALID_MASK;</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	kvm_deliver_exception_payload(vcpu, ex);

<blue>	if (ex->has_error_code) {</blue>
		/*
		 * Despite the error code being architecturally defined as 32
		 * bits, and the VMCS field being 32 bits, Intel CPUs and thus
		 * VMX don&#x27;t actually supporting setting bits 31:16.  Hardware
		 * will (should) never provide a bogus error code, but AMD CPUs
		 * do generate error codes with bits 31:16 set, and so KVM&#x27;s
		 * ABI lets userspace shove in arbitrary 32-bit values.  Drop
		 * the upper bits to avoid VM-Fail, losing information that
		 * does&#x27;t really exist is preferable to killing the VM.
		 */
<blue>		vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, (u16)ex->error_code);</blue>
<blue>		intr_info |= INTR_INFO_DELIVER_CODE_MASK;</blue>
	}

<blue>	if (vmx->rmode.vm86_active) {</blue>
		int inc_eip = 0;
<yellow>		if (kvm_exception_is_soft(ex->vector))</yellow>
<yellow>			inc_eip = vcpu->arch.event_exit_inst_len;</yellow>
<yellow>		kvm_inject_realmode_interrupt(vcpu, ex->vector, inc_eip);</yellow>
		return;
	}

<blue>	WARN_ON_ONCE(vmx->emulation_required);</blue>

<blue>	if (kvm_exception_is_soft(ex->vector)) {</blue>
<blue>		vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,</blue>
<blue>			     vmx->vcpu.arch.event_exit_inst_len);</blue>
<blue>		intr_info |= INTR_TYPE_SOFT_EXCEPTION;</blue>
	} else
<blue>		intr_info |= INTR_TYPE_HARD_EXCEPTION;</blue>

<blue>	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);</blue>

<blue>	vmx_clear_hlt(vcpu);</blue>
<blue>}</blue>

static void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr,
			       bool load_into_hardware)
{
	struct vmx_uret_msr *uret_msr;

<blue>	uret_msr = vmx_find_uret_msr(vmx, msr);</blue>
	if (!uret_msr)
		return;

<blue>	uret_msr->load_into_hardware = load_into_hardware;</blue>
}

/*
 * Configuring user return MSRs to automatically save, load, and restore MSRs
 * that need to be shoved into hardware when running the guest.  Note, omitting
 * an MSR here does _NOT_ mean it&#x27;s not emulated, only that it will not be
 * loaded into hardware when running the guest.
 */
static void vmx_setup_uret_msrs(struct vcpu_vmx *vmx)
{
#ifdef CONFIG_X86_64
	bool load_syscall_msrs;

	/*
	 * The SYSCALL MSRs are only needed on long mode guests, and only
	 * when EFER.SCE is set.
	 */
<blue>	load_syscall_msrs = is_long_mode(&vmx->vcpu) &&</blue>
<blue>			    (vmx->vcpu.arch.efer & EFER_SCE);</blue>

<blue>	vmx_setup_uret_msr(vmx, MSR_STAR, load_syscall_msrs);</blue>
<blue>	vmx_setup_uret_msr(vmx, MSR_LSTAR, load_syscall_msrs);</blue>
<blue>	vmx_setup_uret_msr(vmx, MSR_SYSCALL_MASK, load_syscall_msrs);</blue>
#endif
<blue>	vmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));</blue>

<blue>	vmx_setup_uret_msr(vmx, MSR_TSC_AUX,</blue>
<blue>			   guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP) ||</blue>
<blue>			   guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDPID));</blue>

	/*
	 * hle=0, rtm=0, tsx_ctrl=1 can be found with some combinations of new
	 * kernel and old userspace.  If those guests run on a tsx=off host, do
	 * allow guests to use TSX_CTRL, but don&#x27;t change the value in hardware
	 * so that TSX remains always disabled.
	 */
<blue>	vmx_setup_uret_msr(vmx, MSR_IA32_TSX_CTRL, boot_cpu_has(X86_FEATURE_RTM));</blue>

	/*
	 * The set of MSRs to load may have changed, reload MSRs before the
	 * next VM-Enter.
	 */
<blue>	vmx->guest_uret_msrs_loaded = false;</blue>
}

u64 vmx_get_l2_tsc_offset(struct kvm_vcpu *vcpu)
{
<blue>	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</blue>

	if (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING))
<yellow>		return vmcs12->tsc_offset;</yellow>

	return 0;
<blue>}</blue>

u64 vmx_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu)
{
<blue>	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</blue>

	if (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING) &amp;&amp;
<yellow>	    nested_cpu_has2(vmcs12, SECONDARY_EXEC_TSC_SCALING))</yellow>
<yellow>		return vmcs12->tsc_multiplier;</yellow>

<blue>	return kvm_caps.default_tsc_scaling_ratio;</blue>
<blue>}</blue>

static void vmx_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
{
<blue>	vmcs_write64(TSC_OFFSET, offset);</blue>
<blue>}</blue>

static void vmx_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 multiplier)
{
<blue>	vmcs_write64(TSC_MULTIPLIER, multiplier);</blue>
<blue>}</blue>

/*
 * nested_vmx_allowed() checks whether a guest should be allowed to use VMX
 * instructions and MSRs (i.e., nested VMX). Nested VMX is disabled for
 * all guests if the &quot;nested&quot; module option is off, and can also be disabled
 * for a single guest by disabling its VMX cpuid bit.
 */
bool nested_vmx_allowed(struct kvm_vcpu *vcpu)
<blue>{</blue>
<blue>	return nested && guest_cpuid_has(vcpu, X86_FEATURE_VMX);</blue>
<blue>}</blue>

static inline bool vmx_feature_control_msr_valid(struct kvm_vcpu *vcpu,
						 uint64_t val)
{
<blue>	uint64_t valid_bits = to_vmx(vcpu)->msr_ia32_feature_control_valid_bits;</blue>

	return !(val &amp; ~valid_bits);
}

<blue>static int vmx_get_msr_feature(struct kvm_msr_entry *msr)</blue>
<blue>{</blue>
<blue>	switch (msr->index) {</blue>
	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
<blue>		if (!nested)</blue>
			return 1;
<blue>		return vmx_get_vmx_msr(&vmcs_config.nested, msr->index, &msr->data);</blue>
	case MSR_IA32_PERF_CAPABILITIES:
<blue>		msr->data = vmx_get_perf_capabilities();</blue>
		return 0;
	default:
		return KVM_MSR_RET_INVALID;
	}
}

/*
 * Reads an msr value (of &#x27;msr_info-&gt;index&#x27;) into &#x27;msr_info-&gt;data&#x27;.
 * Returns 0 on success, non-0 otherwise.
 * Assumes vcpu_load() was already called.
 */
static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	struct vmx_uret_msr *msr;
	u32 index;

<blue>	switch (msr_info->index) {</blue>
#ifdef CONFIG_X86_64
	case MSR_FS_BASE:
<blue>		msr_info->data = vmcs_readl(GUEST_FS_BASE);</blue>
		break;
	case MSR_GS_BASE:
<blue>		msr_info->data = vmcs_readl(GUEST_GS_BASE);</blue>
		break;
	case MSR_KERNEL_GS_BASE:
<blue>		msr_info->data = vmx_read_guest_kernel_gs_base(vmx);</blue>
		break;
#endif
	case MSR_EFER:
		return kvm_get_msr_common(vcpu, msr_info);
	case MSR_IA32_TSX_CTRL:
<blue>		if (!msr_info->host_initiated &&</blue>
<yellow>		    !(vcpu->arch.arch_capabilities & ARCH_CAP_TSX_CTRL_MSR))</yellow>
			return 1;
		goto find_uret_msr;
	case MSR_IA32_UMWAIT_CONTROL:
<blue>		if (!msr_info->host_initiated && !vmx_has_waitpkg(vmx))</blue>
			return 1;

<blue>		msr_info->data = vmx->msr_ia32_umwait_control;</blue>
		break;
	case MSR_IA32_SPEC_CTRL:
<blue>		if (!msr_info->host_initiated &&</blue>
<blue>		    !guest_has_spec_ctrl_msr(vcpu))</blue>
			return 1;

<blue>		msr_info->data = to_vmx(vcpu)->spec_ctrl;</blue>
		break;
	case MSR_IA32_SYSENTER_CS:
<blue>		msr_info->data = vmcs_read32(GUEST_SYSENTER_CS);</blue>
		break;
	case MSR_IA32_SYSENTER_EIP:
<blue>		msr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);</blue>
		break;
	case MSR_IA32_SYSENTER_ESP:
<blue>		msr_info->data = vmcs_readl(GUEST_SYSENTER_ESP);</blue>
		break;
	case MSR_IA32_BNDCFGS:
<blue>		if (!kvm_mpx_supported() ||</blue>
<yellow>		    (!msr_info->host_initiated &&</yellow>
<yellow>		     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))</yellow>
			return 1;
<yellow>		msr_info->data = vmcs_read64(GUEST_BNDCFGS);</yellow>
		break;
	case MSR_IA32_MCG_EXT_CTL:
<blue>		if (!msr_info->host_initiated &&</blue>
<yellow>		    !(vmx->msr_ia32_feature_control &</yellow>
		      FEAT_CTL_LMCE_ENABLED))
			return 1;
<blue>		msr_info->data = vcpu->arch.mcg_ext_ctl;</blue>
		break;
	case MSR_IA32_FEAT_CTL:
<blue>		msr_info->data = vmx->msr_ia32_feature_control;</blue>
		break;
	case MSR_IA32_SGXLEPUBKEYHASH0 ... MSR_IA32_SGXLEPUBKEYHASH3:
<blue>		if (!msr_info->host_initiated &&</blue>
<blue>		    !guest_cpuid_has(vcpu, X86_FEATURE_SGX_LC))</blue>
			return 1;
		msr_info-&gt;data = to_vmx(vcpu)-&gt;msr_ia32_sgxlepubkeyhash
<blue>			[msr_info->index - MSR_IA32_SGXLEPUBKEYHASH0];</blue>
		break;
	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
<blue>		if (!nested_vmx_allowed(vcpu))</blue>
			return 1;
<blue>		if (vmx_get_vmx_msr(&vmx->nested.msrs, msr_info->index,</blue>
				    &amp;msr_info-&gt;data))
			return 1;
		/*
		 * Enlightened VMCS v1 doesn&#x27;t have certain VMCS fields but
		 * instead of just ignoring the features, different Hyper-V
		 * versions are either trying to use them and fail or do some
		 * sanity checking and refuse to boot. Filter all unsupported
		 * features out.
		 */
<blue>		if (!msr_info->host_initiated && guest_cpuid_has_evmcs(vcpu))</blue>
<yellow>			nested_evmcs_filter_control_msr(vcpu, msr_info->index,</yellow>
							&amp;msr_info-&gt;data);
		break;
	case MSR_IA32_RTIT_CTL:
<blue>		if (!vmx_pt_mode_is_host_guest())</blue>
			return 1;
<yellow>		msr_info->data = vmx->pt_desc.guest.ctl;</yellow>
		break;
	case MSR_IA32_RTIT_STATUS:
<blue>		if (!vmx_pt_mode_is_host_guest())</blue>
			return 1;
<yellow>		msr_info->data = vmx->pt_desc.guest.status;</yellow>
		break;
	case MSR_IA32_RTIT_CR3_MATCH:
<blue>		if (!vmx_pt_mode_is_host_guest() ||</blue>
<yellow>			!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
						PT_CAP_cr3_filtering))
			return 1;
<yellow>		msr_info->data = vmx->pt_desc.guest.cr3_match;</yellow>
		break;
	case MSR_IA32_RTIT_OUTPUT_BASE:
<blue>		if (!vmx_pt_mode_is_host_guest() ||</blue>
<yellow>			(!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					PT_CAP_topa_output) &amp;&amp;
<yellow>			 !intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					PT_CAP_single_range_output)))
			return 1;
<yellow>		msr_info->data = vmx->pt_desc.guest.output_base;</yellow>
		break;
	case MSR_IA32_RTIT_OUTPUT_MASK:
<blue>		if (!vmx_pt_mode_is_host_guest() ||</blue>
<yellow>			(!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					PT_CAP_topa_output) &amp;&amp;
<yellow>			 !intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					PT_CAP_single_range_output)))
			return 1;
<yellow>		msr_info->data = vmx->pt_desc.guest.output_mask;</yellow>
		break;
	case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:
		index = msr_info-&gt;index - MSR_IA32_RTIT_ADDR0_A;
<blue>		if (!vmx_pt_mode_is_host_guest() ||</blue>
<yellow>		    (index >= 2 * vmx->pt_desc.num_address_ranges))</yellow>
			return 1;
<yellow>		if (index % 2)</yellow>
<yellow>			msr_info->data = vmx->pt_desc.guest.addr_b[index / 2];</yellow>
		else
<yellow>			msr_info->data = vmx->pt_desc.guest.addr_a[index / 2];</yellow>
		break;
	case MSR_IA32_DEBUGCTLMSR:
<blue>		msr_info->data = vmcs_read64(GUEST_IA32_DEBUGCTL);</blue>
		break;
	default:
	find_uret_msr:
<blue>		msr = vmx_find_uret_msr(vmx, msr_info->index);</blue>
		if (msr) {
<blue>			msr_info->data = msr->data;</blue>
			break;
		}
<blue>		return kvm_get_msr_common(vcpu, msr_info);</blue>
	}

	return 0;
<blue>}</blue>

static u64 nested_vmx_truncate_sysenter_addr(struct kvm_vcpu *vcpu,
						    u64 data)
{
#ifdef CONFIG_X86_64
<blue>	if (!guest_cpuid_has(vcpu, X86_FEATURE_LM))</blue>
<yellow>		return (u32)data;</yellow>
#endif
	return (unsigned long)data;
}

static u64 vmx_get_supported_debugctl(struct kvm_vcpu *vcpu, bool host_initiated)
{
	u64 debugctl = 0;

<yellow>	if (boot_cpu_has(X86_FEATURE_BUS_LOCK_DETECT) &&</yellow>
<yellow>	    (host_initiated || guest_cpuid_has(vcpu, X86_FEATURE_BUS_LOCK_DETECT)))</yellow>
		debugctl |= DEBUGCTLMSR_BUS_LOCK_DETECT;

<blue>	if ((vmx_get_perf_capabilities() & PMU_CAP_LBR_FMT) &&</blue>
<yellow>	    (host_initiated || intel_pmu_lbr_is_enabled(vcpu)))</yellow>
<yellow>		debugctl |= DEBUGCTLMSR_LBR | DEBUGCTLMSR_FREEZE_LBRS_ON_PMI;</yellow>

	return debugctl;
}

/*
 * Writes msr value into the appropriate &quot;register&quot;.
 * Returns 0 on success, non-0 otherwise.
 * Assumes vcpu_load() was already called.
 */
static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	struct vmx_uret_msr *msr;
<yellow>	int ret = 0;</yellow>
<blue>	u32 msr_index = msr_info->index;</blue>
	u64 data = msr_info-&gt;data;
	u32 index;

	switch (msr_index) {
	case MSR_EFER:
		ret = kvm_set_msr_common(vcpu, msr_info);
		break;
#ifdef CONFIG_X86_64
	case MSR_FS_BASE:
<blue>		vmx_segment_cache_clear(vmx);</blue>
<blue>		vmcs_writel(GUEST_FS_BASE, data);</blue>
		break;
	case MSR_GS_BASE:
<blue>		vmx_segment_cache_clear(vmx);</blue>
<blue>		vmcs_writel(GUEST_GS_BASE, data);</blue>
		break;
	case MSR_KERNEL_GS_BASE:
<blue>		vmx_write_guest_kernel_gs_base(vmx, data);</blue>
		break;
	case MSR_IA32_XFD:
<blue>		ret = kvm_set_msr_common(vcpu, msr_info);</blue>
		/*
		 * Always intercepting WRMSR could incur non-negligible
		 * overhead given xfd might be changed frequently in
		 * guest context switch. Disable write interception
		 * upon the first write with a non-zero value (indicating
		 * potential usage on dynamic xfeatures). Also update
		 * exception bitmap to trap #NM for proper virtualization
		 * of guest xfd_err.
		 */
<blue>		if (!ret && data) {</blue>
<yellow>			vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD,</yellow>
						      MSR_TYPE_RW);
			vcpu-&gt;arch.xfd_no_write_intercept = true;
			vmx_update_exception_bitmap(vcpu);
		}
		break;
#endif
	case MSR_IA32_SYSENTER_CS:
<blue>		if (is_guest_mode(vcpu))</blue>
<blue>			get_vmcs12(vcpu)->guest_sysenter_cs = data;</blue>
<blue>		vmcs_write32(GUEST_SYSENTER_CS, data);</blue>
		break;
	case MSR_IA32_SYSENTER_EIP:
<blue>		if (is_guest_mode(vcpu)) {</blue>
<blue>			data = nested_vmx_truncate_sysenter_addr(vcpu, data);</blue>
<blue>			get_vmcs12(vcpu)->guest_sysenter_eip = data;</blue>
		}
<blue>		vmcs_writel(GUEST_SYSENTER_EIP, data);</blue>
		break;
	case MSR_IA32_SYSENTER_ESP:
<blue>		if (is_guest_mode(vcpu)) {</blue>
<blue>			data = nested_vmx_truncate_sysenter_addr(vcpu, data);</blue>
<blue>			get_vmcs12(vcpu)->guest_sysenter_esp = data;</blue>
		}
<blue>		vmcs_writel(GUEST_SYSENTER_ESP, data);</blue>
		break;
	case MSR_IA32_DEBUGCTLMSR: {
		u64 invalid;

<blue>		invalid = data & ~vmx_get_supported_debugctl(vcpu, msr_info->host_initiated);</blue>
		if (invalid &amp; (DEBUGCTLMSR_BTF|DEBUGCTLMSR_LBR)) {
<blue>			if (report_ignored_msrs)</blue>
<blue>				vcpu_unimpl(vcpu, "%s: BTF|LBR in IA32_DEBUGCTLMSR 0x%llx, nop\n",</blue>
					    __func__, data);
<blue>			data &= ~(DEBUGCTLMSR_BTF|DEBUGCTLMSR_LBR);</blue>
			invalid &amp;= ~(DEBUGCTLMSR_BTF|DEBUGCTLMSR_LBR);
		}

<blue>		if (invalid)</blue>
			return 1;

<blue>		if (is_guest_mode(vcpu) && get_vmcs12(vcpu)->vm_exit_controls &</blue>
						VM_EXIT_SAVE_DEBUG_CONTROLS)
<blue>			get_vmcs12(vcpu)->guest_ia32_debugctl = data;</blue>

<blue>		vmcs_write64(GUEST_IA32_DEBUGCTL, data);</blue>
<blue>		if (intel_pmu_lbr_is_enabled(vcpu) && !to_vmx(vcpu)->lbr_desc.event &&</blue>
<yellow>		    (data & DEBUGCTLMSR_LBR))</yellow>
<yellow>			intel_pmu_create_guest_lbr_event(vcpu);</yellow>
		return 0;
	}
	case MSR_IA32_BNDCFGS:
<blue>		if (!kvm_mpx_supported() ||</blue>
<yellow>		    (!msr_info->host_initiated &&</yellow>
<yellow>		     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))</yellow>
			return 1;
<yellow>		if (is_noncanonical_address(data & PAGE_MASK, vcpu) ||</yellow>
<yellow>		    (data & MSR_IA32_BNDCFGS_RSVD))</yellow>
			return 1;

<yellow>		if (is_guest_mode(vcpu) &&</yellow>
<yellow>		    ((vmx->nested.msrs.entry_ctls_high & VM_ENTRY_LOAD_BNDCFGS) ||</yellow>
<yellow>		     (vmx->nested.msrs.exit_ctls_high & VM_EXIT_CLEAR_BNDCFGS)))</yellow>
<yellow>			get_vmcs12(vcpu)->guest_bndcfgs = data;</yellow>

<yellow>		vmcs_write64(GUEST_BNDCFGS, data);</yellow>
		break;
	case MSR_IA32_UMWAIT_CONTROL:
<blue>		if (!msr_info->host_initiated && !vmx_has_waitpkg(vmx))</blue>
			return 1;

		/* The reserved bit 1 and non-32 bit [63:32] should be zero */
<blue>		if (data & (BIT_ULL(1) | GENMASK_ULL(63, 32)))</blue>
			return 1;

<blue>		vmx->msr_ia32_umwait_control = data;</blue>
		break;
	case MSR_IA32_SPEC_CTRL:
<blue>		if (!msr_info->host_initiated &&</blue>
<blue>		    !guest_has_spec_ctrl_msr(vcpu))</blue>
			return 1;

<blue>		if (kvm_spec_ctrl_test_value(data))</blue>
			return 1;

<blue>		vmx->spec_ctrl = data;</blue>
		if (!data)
			break;

		/*
		 * For non-nested:
		 * When it&#x27;s written (to non-zero) for the first time, pass
		 * it through.
		 *
		 * For nested:
		 * The handling of the MSR bitmap for L2 guests is done in
		 * nested_vmx_prepare_msr_bitmap. We should not touch the
		 * vmcs02.msr_bitmap here since it gets completely overwritten
		 * in the merging. We update the vmcs01 here for L1 as well
		 * since it will end up touching the MSR anyway now.
		 */
<blue>		vmx_disable_intercept_for_msr(vcpu,</blue>
					      MSR_IA32_SPEC_CTRL,
					      MSR_TYPE_RW);
		break;
	case MSR_IA32_TSX_CTRL:
<blue>		if (!msr_info->host_initiated &&</blue>
<yellow>		    !(vcpu->arch.arch_capabilities & ARCH_CAP_TSX_CTRL_MSR))</yellow>
			return 1;
<blue>		if (data & ~(TSX_CTRL_RTM_DISABLE | TSX_CTRL_CPUID_CLEAR))</blue>
			return 1;
		goto find_uret_msr;
	case MSR_IA32_PRED_CMD:
<blue>		if (!msr_info->host_initiated &&</blue>
<yellow>		    !guest_has_pred_cmd_msr(vcpu))</yellow>
			return 1;

<blue>		if (data & ~PRED_CMD_IBPB)</blue>
			return 1;
<blue>		if (!boot_cpu_has(X86_FEATURE_IBPB))</blue>
			return 1;
<blue>		if (!data)</blue>
			break;

<blue>		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);</blue>

		/*
		 * For non-nested:
		 * When it&#x27;s written (to non-zero) for the first time, pass
		 * it through.
		 *
		 * For nested:
		 * The handling of the MSR bitmap for L2 guests is done in
		 * nested_vmx_prepare_msr_bitmap. We should not touch the
		 * vmcs02.msr_bitmap here since it gets completely overwritten
		 * in the merging.
		 */
		vmx_disable_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W);
		break;
	case MSR_IA32_CR_PAT:
<blue>		if (!kvm_pat_valid(data))</blue>
			return 1;

<blue>		if (is_guest_mode(vcpu) &&</blue>
<blue>		    get_vmcs12(vcpu)->vm_exit_controls & VM_EXIT_SAVE_IA32_PAT)</blue>
<yellow>			get_vmcs12(vcpu)->guest_ia32_pat = data;</yellow>

<blue>		if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {</blue>
<blue>			vmcs_write64(GUEST_IA32_PAT, data);</blue>
<blue>			vcpu->arch.pat = data;</blue>
			break;
		}
		ret = kvm_set_msr_common(vcpu, msr_info);
		break;
	case MSR_IA32_MCG_EXT_CTL:
<blue>		if ((!msr_info->host_initiated &&</blue>
<yellow>		     !(to_vmx(vcpu)->msr_ia32_feature_control &</yellow>
		       FEAT_CTL_LMCE_ENABLED)) ||
<blue>		    (data & ~MCG_EXT_CTL_LMCE_EN))</blue>
			return 1;
<blue>		vcpu->arch.mcg_ext_ctl = data;</blue>
		break;
	case MSR_IA32_FEAT_CTL:
<blue>		if (!vmx_feature_control_msr_valid(vcpu, data) ||</blue>
<blue>		    (to_vmx(vcpu)->msr_ia32_feature_control &</blue>
<blue>		     FEAT_CTL_LOCKED && !msr_info->host_initiated))</blue>
			return 1;
<blue>		vmx->msr_ia32_feature_control = data;</blue>
<blue>		if (msr_info->host_initiated && data == 0)</blue>
<blue>			vmx_leave_nested(vcpu);</blue>

		/* SGX may be enabled/disabled by guest&#x27;s firmware */
<blue>		vmx_write_encls_bitmap(vcpu, NULL);</blue>
		break;
	case MSR_IA32_SGXLEPUBKEYHASH0 ... MSR_IA32_SGXLEPUBKEYHASH3:
		/*
		 * On real hardware, the LE hash MSRs are writable before
		 * the firmware sets bit 0 in MSR 0x7a (&quot;activating&quot; SGX),
		 * at which point SGX related bits in IA32_FEATURE_CONTROL
		 * become writable.
		 *
		 * KVM does not emulate SGX activation for simplicity, so
		 * allow writes to the LE hash MSRs if IA32_FEATURE_CONTROL
		 * is unlocked.  This is technically not architectural
		 * behavior, but it&#x27;s close enough.
		 */
<blue>		if (!msr_info->host_initiated &&</blue>
<yellow>		    (!guest_cpuid_has(vcpu, X86_FEATURE_SGX_LC) ||</yellow>
<yellow>		    ((vmx->msr_ia32_feature_control & FEAT_CTL_LOCKED) &&</yellow>
		    !(vmx-&gt;msr_ia32_feature_control &amp; FEAT_CTL_SGX_LC_ENABLED))))
			return 1;
		vmx-&gt;msr_ia32_sgxlepubkeyhash
<blue>			[msr_index - MSR_IA32_SGXLEPUBKEYHASH0] = data;</blue>
		break;
	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
<blue>		if (!msr_info->host_initiated)</blue>
			return 1; /* they are read-only */
<blue>		if (!nested_vmx_allowed(vcpu))</blue>
			return 1;
<blue>		return vmx_set_vmx_msr(vcpu, msr_index, data);</blue>
	case MSR_IA32_RTIT_CTL:
<blue>		if (!vmx_pt_mode_is_host_guest() ||</blue>
<yellow>			vmx_rtit_ctl_check(vcpu, data) ||</yellow>
<yellow>			vmx->nested.vmxon)</yellow>
			return 1;
<yellow>		vmcs_write64(GUEST_IA32_RTIT_CTL, data);</yellow>
<yellow>		vmx->pt_desc.guest.ctl = data;</yellow>
		pt_update_intercept_for_msr(vcpu);
		break;
	case MSR_IA32_RTIT_STATUS:
<blue>		if (!pt_can_write_msr(vmx))</blue>
			return 1;
		if (data &amp; MSR_IA32_RTIT_STATUS_MASK)
			return 1;
<yellow>		vmx->pt_desc.guest.status = data;</yellow>
		break;
	case MSR_IA32_RTIT_CR3_MATCH:
<blue>		if (!pt_can_write_msr(vmx))</blue>
			return 1;
<yellow>		if (!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					   PT_CAP_cr3_filtering))
			return 1;
<yellow>		vmx->pt_desc.guest.cr3_match = data;</yellow>
		break;
	case MSR_IA32_RTIT_OUTPUT_BASE:
<blue>		if (!pt_can_write_msr(vmx))</blue>
			return 1;
<yellow>		if (!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					   PT_CAP_topa_output) &amp;&amp;
<yellow>		    !intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					   PT_CAP_single_range_output))
			return 1;
<yellow>		if (!pt_output_base_valid(vcpu, data))</yellow>
			return 1;
<yellow>		vmx->pt_desc.guest.output_base = data;</yellow>
		break;
	case MSR_IA32_RTIT_OUTPUT_MASK:
<blue>		if (!pt_can_write_msr(vmx))</blue>
			return 1;
<yellow>		if (!intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
<blue>					   PT_CAP_topa_output) &&</blue>
<yellow>		    !intel_pt_validate_cap(vmx->pt_desc.caps,</yellow>
					   PT_CAP_single_range_output))
			return 1;
<yellow>		vmx->pt_desc.guest.output_mask = data;</yellow>
		break;
	case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:
<blue>		if (!pt_can_write_msr(vmx))</blue>
			return 1;
		index = msr_info-&gt;index - MSR_IA32_RTIT_ADDR0_A;
<yellow>		if (index >= 2 * vmx->pt_desc.num_address_ranges)</yellow>
			return 1;
<yellow>		if (is_noncanonical_address(data, vcpu))</yellow>
			return 1;
<yellow>		if (index % 2)</yellow>
<yellow>			vmx->pt_desc.guest.addr_b[index / 2] = data;</yellow>
		else
<yellow>			vmx->pt_desc.guest.addr_a[index / 2] = data;</yellow>
		break;
	case MSR_IA32_PERF_CAPABILITIES:
<blue>		if (data && !vcpu_to_pmu(vcpu)->version)</blue>
			return 1;
<blue>		if (data & PMU_CAP_LBR_FMT) {</blue>
<blue>			if ((data & PMU_CAP_LBR_FMT) !=</blue>
<blue>			    (vmx_get_perf_capabilities() & PMU_CAP_LBR_FMT))</blue>
				return 1;
<yellow>			if (!cpuid_model_is_consistent(vcpu))</yellow>
				return 1;
		}
<blue>		if (data & PERF_CAP_PEBS_FORMAT) {</blue>
<blue>			if ((data & PERF_CAP_PEBS_MASK) !=</blue>
<blue>			    (vmx_get_perf_capabilities() & PERF_CAP_PEBS_MASK))</blue>
				return 1;
<yellow>			if (!guest_cpuid_has(vcpu, X86_FEATURE_DS))</yellow>
				return 1;
<yellow>			if (!guest_cpuid_has(vcpu, X86_FEATURE_DTES64))</yellow>
				return 1;
<yellow>			if (!cpuid_model_is_consistent(vcpu))</yellow>
				return 1;
		}
<blue>		ret = kvm_set_msr_common(vcpu, msr_info);</blue>
		break;

	default:
	find_uret_msr:
<blue>		msr = vmx_find_uret_msr(vmx, msr_index);</blue>
		if (msr)
<blue>			ret = vmx_set_guest_uret_msr(vmx, msr, data);</blue>
		else
<blue>			ret = kvm_set_msr_common(vcpu, msr_info);</blue>
	}

	/* FB_CLEAR may have changed, also update the FB_CLEAR_DIS behavior */
<blue>	if (msr_index == MSR_IA32_ARCH_CAPABILITIES)</blue>
<blue>		vmx_update_fb_clear_dis(vcpu, vmx);</blue>

	return ret;
}

static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
{
	unsigned long guest_owned_bits;

<blue>	kvm_register_mark_available(vcpu, reg);</blue>

	switch (reg) {
	case VCPU_REGS_RSP:
<blue>		vcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);</blue>
		break;
	case VCPU_REGS_RIP:
<blue>		vcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);</blue>
		break;
	case VCPU_EXREG_PDPTR:
<yellow>		if (enable_ept)</yellow>
<yellow>			ept_save_pdptrs(vcpu);</yellow>
		break;
	case VCPU_EXREG_CR0:
<blue>		guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;</blue>

		vcpu-&gt;arch.cr0 &amp;= ~guest_owned_bits;
<blue>		vcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & guest_owned_bits;</blue>
		break;
	case VCPU_EXREG_CR3:
		/*
		 * When intercepting CR3 loads, e.g. for shadowing paging, KVM&#x27;s
		 * CR3 is loaded into hardware, not the guest&#x27;s CR3.
		 */
<blue>		if (!(exec_controls_get(to_vmx(vcpu)) & CPU_BASED_CR3_LOAD_EXITING))</blue>
<blue>			vcpu->arch.cr3 = vmcs_readl(GUEST_CR3);</blue>
		break;
	case VCPU_EXREG_CR4:
<blue>		guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;</blue>

		vcpu-&gt;arch.cr4 &amp;= ~guest_owned_bits;
<blue>		vcpu->arch.cr4 |= vmcs_readl(GUEST_CR4) & guest_owned_bits;</blue>
		break;
	default:
<yellow>		KVM_BUG_ON(1, vcpu->kvm);</yellow>
		break;
	}
<blue>}</blue>

static __init int cpu_has_kvm_support(void)
{
<yellow>	return cpu_has_vmx();</yellow>
}

static __init int vmx_disabled_by_bios(void)
{
<yellow>	return !boot_cpu_has(X86_FEATURE_MSR_IA32_FEAT_CTL) ||</yellow>
<yellow>	       !boot_cpu_has(X86_FEATURE_VMX);</yellow>
<yellow>}</yellow>

static int kvm_cpu_vmxon(u64 vmxon_pointer)
{
	u64 msr;

<yellow>	cr4_set_bits(X86_CR4_VMXE);</yellow>

<blue>	asm_volatile_goto("1: vmxon %[vmxon_pointer]\n\t"</blue>
			  _ASM_EXTABLE(1b, %l[fault])
			  : : [vmxon_pointer] &quot;m&quot;(vmxon_pointer)
			  : : fault);
	return 0;

fault:
<yellow>	WARN_ONCE(1, "VMXON faulted, MSR_IA32_FEAT_CTL (0x3a) = 0x%llx\n",</yellow>
		  rdmsrl_safe(MSR_IA32_FEAT_CTL, &amp;msr) ? 0xdeadbeef : msr);
	cr4_clear_bits(X86_CR4_VMXE);

<yellow>	return -EFAULT;</yellow>
}

static int vmx_hardware_enable(void)
<blue>{</blue>
<blue>	int cpu = raw_smp_processor_id();</blue>
<blue>	u64 phys_addr = __pa(per_cpu(vmxarea, cpu));</blue>
	int r;

	if (cr4_read_shadow() &amp; X86_CR4_VMXE)
		return -EBUSY;

	/*
	 * This can happen if we hot-added a CPU but failed to allocate
	 * VP assist page for it.
	 */
<blue>	if (static_branch_unlikely(&enable_evmcs) &&</blue>
<yellow>	    !hv_get_vp_assist_page(cpu))</yellow>
		return -EFAULT;

<blue>	intel_pt_handle_vmx(1);</blue>

<blue>	r = kvm_cpu_vmxon(phys_addr);</blue>
	if (r) {
		intel_pt_handle_vmx(0);
		return r;
	}

<blue>	if (enable_ept)</blue>
<blue>		ept_sync_global();</blue>

<blue>	return 0;</blue>
}

static void vmclear_local_loaded_vmcss(void)
{
<yellow>	int cpu = raw_smp_processor_id();</yellow>
	struct loaded_vmcs *v, *n;

	list_for_each_entry_safe(v, n, &amp;per_cpu(loaded_vmcss_on_cpu, cpu),
				 loaded_vmcss_on_cpu_link)
<yellow>		__loaded_vmcs_clear(v);</yellow>
}

static void vmx_hardware_disable(void)
{
<yellow>	vmclear_local_loaded_vmcss();</yellow>

<yellow>	if (cpu_vmxoff())</yellow>
		kvm_spurious_fault();

<yellow>	intel_pt_handle_vmx(0);</yellow>
}

/*
 * There is no X86_FEATURE for SGX yet, but anyway we need to query CPUID
 * directly instead of going through cpu_has(), to ensure KVM is trapping
 * ENCLS whenever it&#x27;s supported in hardware.  It does not matter whether
 * the host OS supports or has enabled SGX.
 */
static bool cpu_has_sgx(void)
{
<yellow>	return cpuid_eax(0) >= 0x12 && (cpuid_eax(0x12) & BIT(0));</yellow>
}

/*
 * Some cpus support VM_{ENTRY,EXIT}_IA32_PERF_GLOBAL_CTRL but they
 * can&#x27;t be used due to errata where VM Exit may incorrectly clear
 * IA32_PERF_GLOBAL_CTRL[34:32]. Work around the errata by using the
 * MSR load mechanism to switch IA32_PERF_GLOBAL_CTRL.
 */
<blue>static bool cpu_has_perf_global_ctrl_bug(void)</blue>
{
<blue>	if (boot_cpu_data.x86 == 0x6) {</blue>
<blue>		switch (boot_cpu_data.x86_model) {</blue>
		case INTEL_FAM6_NEHALEM_EP:	/* AAK155 */
		case INTEL_FAM6_NEHALEM:	/* AAP115 */
		case INTEL_FAM6_WESTMERE:	/* AAT100 */
		case INTEL_FAM6_WESTMERE_EP:	/* BC86,AAY89,BD102 */
		case INTEL_FAM6_NEHALEM_EX:	/* BA97 */
			return true;
		default:
			break;
		}
	}

	return false;
}

static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
				      u32 msr, u32 *result)
{
	u32 vmx_msr_low, vmx_msr_high;
	u32 ctl = ctl_min | ctl_opt;

<yellow>	rdmsr(msr, vmx_msr_low, vmx_msr_high);</yellow>

	ctl &amp;= vmx_msr_high; /* bit == 0 in high word ==&gt; must be zero */
	ctl |= vmx_msr_low;  /* bit == 1 in low word  ==&gt; must be one  */

	/* Ensure minimum (required) set of control bits are supported. */
	if (ctl_min &amp; ~ctl)
		return -EIO;

<yellow>	*result = ctl;</yellow>
	return 0;
<yellow>}</yellow>

static __init u64 adjust_vmx_controls64(u64 ctl_opt, u32 msr)
{
	u64 allowed;

<yellow>	rdmsrl(msr, allowed);</yellow>

	return  ctl_opt &amp; allowed;
}

static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf,
				    struct vmx_capability *vmx_cap)
<yellow>{</yellow>
	u32 vmx_msr_low, vmx_msr_high;
	u32 _pin_based_exec_control = 0;
	u32 _cpu_based_exec_control = 0;
	u32 _cpu_based_2nd_exec_control = 0;
	u64 _cpu_based_3rd_exec_control = 0;
	u32 _vmexit_control = 0;
	u32 _vmentry_control = 0;
	u64 misc_msr;
	int i;

	/*
	 * LOAD/SAVE_DEBUG_CONTROLS are absent because both are mandatory.
	 * SAVE_IA32_PAT and SAVE_IA32_EFER are absent because KVM always
	 * intercepts writes to PAT and EFER, i.e. never enables those controls.
	 */
	struct {
		u32 entry_control;
		u32 exit_control;
<yellow>	} const vmcs_entry_exit_pairs[] = {</yellow>
		{ VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,	VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL },
		{ VM_ENTRY_LOAD_IA32_PAT,		VM_EXIT_LOAD_IA32_PAT },
		{ VM_ENTRY_LOAD_IA32_EFER,		VM_EXIT_LOAD_IA32_EFER },
		{ VM_ENTRY_LOAD_BNDCFGS,		VM_EXIT_CLEAR_BNDCFGS },
		{ VM_ENTRY_LOAD_IA32_RTIT_CTL,		VM_EXIT_CLEAR_IA32_RTIT_CTL },
	};

	memset(vmcs_conf, 0, sizeof(*vmcs_conf));

	if (adjust_vmx_controls(KVM_REQUIRED_VMX_CPU_BASED_VM_EXEC_CONTROL,
				KVM_OPTIONAL_VMX_CPU_BASED_VM_EXEC_CONTROL,
				MSR_IA32_VMX_PROCBASED_CTLS,
				&amp;_cpu_based_exec_control))
		return -EIO;
<yellow>	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {</yellow>
<yellow>		if (adjust_vmx_controls(KVM_REQUIRED_VMX_SECONDARY_VM_EXEC_CONTROL,</yellow>
					KVM_OPTIONAL_VMX_SECONDARY_VM_EXEC_CONTROL,
					MSR_IA32_VMX_PROCBASED_CTLS2,
					&amp;_cpu_based_2nd_exec_control))
			return -EIO;
	}
#ifndef CONFIG_X86_64
	if (!(_cpu_based_2nd_exec_control &amp;
				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
		_cpu_based_exec_control &amp;= ~CPU_BASED_TPR_SHADOW;
#endif

<yellow>	if (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))</yellow>
<yellow>		_cpu_based_2nd_exec_control &= ~(</yellow>
				SECONDARY_EXEC_APIC_REGISTER_VIRT |
				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
				SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);

<yellow>	rdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,</yellow>
		&amp;vmx_cap-&gt;ept, &amp;vmx_cap-&gt;vpid);

<yellow>	if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&</yellow>
	    vmx_cap-&gt;ept) {
<yellow>		pr_warn_once("EPT CAP should not exist if not support "</yellow>
				&quot;1-setting enable EPT VM-execution control\n&quot;);

<yellow>		if (error_on_inconsistent_vmcs_config)</yellow>
			return -EIO;

<yellow>		vmx_cap->ept = 0;</yellow>
	}
<yellow>	if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_VPID) &&</yellow>
<yellow>	    vmx_cap->vpid) {</yellow>
<yellow>		pr_warn_once("VPID CAP should not exist if not support "</yellow>
				&quot;1-setting enable VPID VM-execution control\n&quot;);

<yellow>		if (error_on_inconsistent_vmcs_config)</yellow>
			return -EIO;

<yellow>		vmx_cap->vpid = 0;</yellow>
	}

<yellow>	if (!cpu_has_sgx())</yellow>
<yellow>		_cpu_based_2nd_exec_control &= ~SECONDARY_EXEC_ENCLS_EXITING;</yellow>

<yellow>	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_TERTIARY_CONTROLS)</yellow>
		_cpu_based_3rd_exec_control =
<yellow>			adjust_vmx_controls64(KVM_OPTIONAL_VMX_TERTIARY_VM_EXEC_CONTROL,</yellow>
					      MSR_IA32_VMX_PROCBASED_CTLS3);

<yellow>	if (adjust_vmx_controls(KVM_REQUIRED_VMX_VM_EXIT_CONTROLS,</yellow>
				KVM_OPTIONAL_VMX_VM_EXIT_CONTROLS,
				MSR_IA32_VMX_EXIT_CTLS,
				&amp;_vmexit_control))
		return -EIO;

<yellow>	if (adjust_vmx_controls(KVM_REQUIRED_VMX_PIN_BASED_VM_EXEC_CONTROL,</yellow>
				KVM_OPTIONAL_VMX_PIN_BASED_VM_EXEC_CONTROL,
				MSR_IA32_VMX_PINBASED_CTLS,
				&amp;_pin_based_exec_control))
		return -EIO;

<yellow>	if (cpu_has_broken_vmx_preemption_timer())</yellow>
<yellow>		_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;</yellow>
<yellow>	if (!(_cpu_based_2nd_exec_control &</yellow>
		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
<yellow>		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;</yellow>

<yellow>	if (adjust_vmx_controls(KVM_REQUIRED_VMX_VM_ENTRY_CONTROLS,</yellow>
				KVM_OPTIONAL_VMX_VM_ENTRY_CONTROLS,
				MSR_IA32_VMX_ENTRY_CTLS,
				&amp;_vmentry_control))
		return -EIO;

<yellow>	for (i = 0; i < ARRAY_SIZE(vmcs_entry_exit_pairs); i++) {</yellow>
<yellow>		u32 n_ctrl = vmcs_entry_exit_pairs[i].entry_control;</yellow>
		u32 x_ctrl = vmcs_entry_exit_pairs[i].exit_control;

		if (!(_vmentry_control &amp; n_ctrl) == !(_vmexit_control &amp; x_ctrl))
			continue;

<yellow>		pr_warn_once("Inconsistent VM-Entry/VM-Exit pair, entry = %x, exit = %x\n",</yellow>
			     _vmentry_control &amp; n_ctrl, _vmexit_control &amp; x_ctrl);

<yellow>		if (error_on_inconsistent_vmcs_config)</yellow>
			return -EIO;

<yellow>		_vmentry_control &= ~n_ctrl;</yellow>
		_vmexit_control &amp;= ~x_ctrl;
	}

<yellow>	rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);</yellow>

	/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */
	if ((vmx_msr_high &amp; 0x1fff) &gt; PAGE_SIZE)
		return -EIO;

#ifdef CONFIG_X86_64
	/* IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */
<yellow>	if (vmx_msr_high & (1u<<16))</yellow>
		return -EIO;
#endif

	/* Require Write-Back (WB) memory type for VMCS accesses. */
<yellow>	if (((vmx_msr_high >> 18) & 15) != 6)</yellow>
		return -EIO;

<yellow>	rdmsrl(MSR_IA32_VMX_MISC, misc_msr);</yellow>

	vmcs_conf-&gt;size = vmx_msr_high &amp; 0x1fff;
	vmcs_conf-&gt;basic_cap = vmx_msr_high &amp; ~0x1fff;

	vmcs_conf-&gt;revision_id = vmx_msr_low;

	vmcs_conf-&gt;pin_based_exec_ctrl = _pin_based_exec_control;
	vmcs_conf-&gt;cpu_based_exec_ctrl = _cpu_based_exec_control;
	vmcs_conf-&gt;cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
	vmcs_conf-&gt;cpu_based_3rd_exec_ctrl = _cpu_based_3rd_exec_control;
	vmcs_conf-&gt;vmexit_ctrl         = _vmexit_control;
	vmcs_conf-&gt;vmentry_ctrl        = _vmentry_control;
	vmcs_conf-&gt;misc	= misc_msr;

	return 0;
}

struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags)
<blue>{</blue>
<blue>	int node = cpu_to_node(cpu);</blue>
	struct page *pages;
	struct vmcs *vmcs;

<blue>	pages = __alloc_pages_node(node, flags, 0);</blue>
	if (!pages)
		return NULL;
<blue>	vmcs = page_address(pages);</blue>
	memset(vmcs, 0, vmcs_config.size);

	/* KVM supports Enlightened VMCS v1 only */
	if (static_branch_unlikely(&amp;enable_evmcs))
<blue>		vmcs->hdr.revision_id = KVM_EVMCS_VERSION;</blue>
	else
<blue>		vmcs->hdr.revision_id = vmcs_config.revision_id;</blue>

	if (shadow)
<blue>		vmcs->hdr.shadow_vmcs = 1;</blue>
	return vmcs;
}

void free_vmcs(struct vmcs *vmcs)
{
<blue>	free_page((unsigned long)vmcs);</blue>
}

/*
 * Free a VMCS, but before that VMCLEAR it on the CPU where it was last loaded
 */
void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
{
<blue>	if (!loaded_vmcs->vmcs)</blue>
		return;
<blue>	loaded_vmcs_clear(loaded_vmcs);</blue>
<blue>	free_vmcs(loaded_vmcs->vmcs);</blue>
	loaded_vmcs-&gt;vmcs = NULL;
	if (loaded_vmcs-&gt;msr_bitmap)
<blue>		free_page((unsigned long)loaded_vmcs->msr_bitmap);</blue>
<blue>	WARN_ON(loaded_vmcs->shadow_vmcs != NULL);</blue>
<blue>}</blue>

int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
<blue>{</blue>
<blue>	loaded_vmcs->vmcs = alloc_vmcs(false);</blue>
	if (!loaded_vmcs-&gt;vmcs)
		return -ENOMEM;

<blue>	vmcs_clear(loaded_vmcs->vmcs);</blue>

<blue>	loaded_vmcs->shadow_vmcs = NULL;</blue>
	loaded_vmcs-&gt;hv_timer_soft_disabled = false;
	loaded_vmcs-&gt;cpu = -1;
	loaded_vmcs-&gt;launched = 0;

	if (cpu_has_vmx_msr_bitmap()) {
		loaded_vmcs-&gt;msr_bitmap = (unsigned long *)
<blue>				__get_free_page(GFP_KERNEL_ACCOUNT);</blue>
		if (!loaded_vmcs-&gt;msr_bitmap)
			goto out_vmcs;
<blue>		memset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);</blue>
	}

<blue>	memset(&loaded_vmcs->host_state, 0, sizeof(struct vmcs_host_state));</blue>
	memset(&amp;loaded_vmcs-&gt;controls_shadow, 0,
		sizeof(struct vmcs_controls_shadow));

	return 0;

out_vmcs:
<yellow>	free_loaded_vmcs(loaded_vmcs);</yellow>
	return -ENOMEM;
}

static void free_kvm_area(void)
{
	int cpu;

<yellow>	for_each_possible_cpu(cpu) {</yellow>
<yellow>		free_vmcs(per_cpu(vmxarea, cpu));</yellow>
		per_cpu(vmxarea, cpu) = NULL;
	}
<yellow>}</yellow>

static __init int alloc_kvm_area(void)
{
	int cpu;

<yellow>	for_each_possible_cpu(cpu) {</yellow>
		struct vmcs *vmcs;

<yellow>		vmcs = alloc_vmcs_cpu(false, cpu, GFP_KERNEL);</yellow>
		if (!vmcs) {
<yellow>			free_kvm_area();</yellow>
			return -ENOMEM;
		}

		/*
		 * When eVMCS is enabled, alloc_vmcs_cpu() sets
		 * vmcs-&gt;revision_id to KVM_EVMCS_VERSION instead of
		 * revision_id reported by MSR_IA32_VMX_BASIC.
		 *
		 * However, even though not explicitly documented by
		 * TLFS, VMXArea passed as VMXON argument should
		 * still be marked with revision_id reported by
		 * physical CPU.
		 */
<yellow>		if (static_branch_unlikely(&enable_evmcs))</yellow>
<yellow>			vmcs->hdr.revision_id = vmcs_config.revision_id;</yellow>

<yellow>		per_cpu(vmxarea, cpu) = vmcs;</yellow>
	}
	return 0;
}

static void fix_pmode_seg(struct kvm_vcpu *vcpu, int seg,
		struct kvm_segment *save)
{
<yellow>	if (!emulate_invalid_guest_state) {</yellow>
		/*
		 * CS and SS RPL should be equal during guest entry according
		 * to VMX spec, but in reality it is not always so. Since vcpu
		 * is in the middle of the transition from real mode to
		 * protected mode it is safe to assume that RPL 0 is a good
		 * default value.
		 */
		if (seg == VCPU_SREG_CS || seg == VCPU_SREG_SS)
<yellow>			save->selector &= ~SEGMENT_RPL_MASK;</yellow>
<yellow>		save->dpl = save->selector & SEGMENT_RPL_MASK;</yellow>
		save-&gt;s = 1;
	}
<yellow>	__vmx_set_segment(vcpu, save, seg);</yellow>
}

static void enter_pmode(struct kvm_vcpu *vcpu)
{
	unsigned long flags;
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	/*
	 * Update real mode segment cache. It may be not up-to-date if segment
	 * register was written while vcpu was in a guest mode.
	 */
<yellow>	vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);</yellow>
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);

	vmx-&gt;rmode.vm86_active = 0;

	__vmx_set_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);

<yellow>	flags = vmcs_readl(GUEST_RFLAGS);</yellow>
<yellow>	flags &= RMODE_GUEST_OWNED_EFLAGS_BITS;</yellow>
<yellow>	flags |= vmx->rmode.save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;</yellow>
<yellow>	vmcs_writel(GUEST_RFLAGS, flags);</yellow>

<yellow>	vmcs_writel(GUEST_CR4, (vmcs_readl(GUEST_CR4) & ~X86_CR4_VME) |</yellow>
<yellow>			(vmcs_readl(CR4_READ_SHADOW) & X86_CR4_VME));</yellow>

<yellow>	vmx_update_exception_bitmap(vcpu);</yellow>

<yellow>	fix_pmode_seg(vcpu, VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);</yellow>
<yellow>	fix_pmode_seg(vcpu, VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);</yellow>
<yellow>	fix_pmode_seg(vcpu, VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);</yellow>
<yellow>	fix_pmode_seg(vcpu, VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);</yellow>
<yellow>	fix_pmode_seg(vcpu, VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);</yellow>
<yellow>	fix_pmode_seg(vcpu, VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);</yellow>
}

static void fix_rmode_seg(int seg, struct kvm_segment *save)
<yellow>{</yellow>
<yellow>	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];</yellow>
	struct kvm_segment var = *save;

	var.dpl = 0x3;
	if (seg == VCPU_SREG_CS)
<yellow>		var.type = 0x3;</yellow>

<yellow>	if (!emulate_invalid_guest_state) {</yellow>
<yellow>		var.selector = var.base >> 4;</yellow>
		var.base = var.base &amp; 0xffff0;
		var.limit = 0xffff;
		var.g = 0;
		var.db = 0;
		var.present = 1;
		var.s = 1;
		var.l = 0;
		var.unusable = 0;
		var.type = 0x3;
		var.avl = 0;
		if (save-&gt;base &amp; 0xf)
<yellow>			printk_once(KERN_WARNING "kvm: segment base is not "</yellow>
					&quot;paragraph aligned when entering &quot;
					&quot;protected mode (seg=%d)&quot;, seg);
	}

<yellow>	vmcs_write16(sf->selector, var.selector);</yellow>
<yellow>	vmcs_writel(sf->base, var.base);</yellow>
<yellow>	vmcs_write32(sf->limit, var.limit);</yellow>
<yellow>	vmcs_write32(sf->ar_bytes, vmx_segment_access_rights(&var));</yellow>
}

static void enter_rmode(struct kvm_vcpu *vcpu)
{
	unsigned long flags;
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<yellow>	struct kvm_vmx *kvm_vmx = to_kvm_vmx(vcpu->kvm);</yellow>

	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
	vmx_get_segment(vcpu, &amp;vmx-&gt;rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);

	vmx-&gt;rmode.vm86_active = 1;

	/*
	 * Very old userspace does not call KVM_SET_TSS_ADDR before entering
	 * vcpu. Warn the user that an update is overdue.
	 */
	if (!kvm_vmx-&gt;tss_addr)
<yellow>		printk_once(KERN_WARNING "kvm: KVM_SET_TSS_ADDR need to be "</yellow>
			     &quot;called before entering vcpu\n&quot;);

<yellow>	vmx_segment_cache_clear(vmx);</yellow>

<yellow>	vmcs_writel(GUEST_TR_BASE, kvm_vmx->tss_addr);</yellow>
<yellow>	vmcs_write32(GUEST_TR_LIMIT, RMODE_TSS_SIZE - 1);</yellow>
<yellow>	vmcs_write32(GUEST_TR_AR_BYTES, 0x008b);</yellow>

<yellow>	flags = vmcs_readl(GUEST_RFLAGS);</yellow>
<yellow>	vmx->rmode.save_rflags = flags;</yellow>

<yellow>	flags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;</yellow>

<yellow>	vmcs_writel(GUEST_RFLAGS, flags);</yellow>
<yellow>	vmcs_writel(GUEST_CR4, vmcs_readl(GUEST_CR4) | X86_CR4_VME);</yellow>
<yellow>	vmx_update_exception_bitmap(vcpu);</yellow>

	fix_rmode_seg(VCPU_SREG_SS, &amp;vmx-&gt;rmode.segs[VCPU_SREG_SS]);
	fix_rmode_seg(VCPU_SREG_CS, &amp;vmx-&gt;rmode.segs[VCPU_SREG_CS]);
	fix_rmode_seg(VCPU_SREG_ES, &amp;vmx-&gt;rmode.segs[VCPU_SREG_ES]);
	fix_rmode_seg(VCPU_SREG_DS, &amp;vmx-&gt;rmode.segs[VCPU_SREG_DS]);
	fix_rmode_seg(VCPU_SREG_GS, &amp;vmx-&gt;rmode.segs[VCPU_SREG_GS]);
	fix_rmode_seg(VCPU_SREG_FS, &amp;vmx-&gt;rmode.segs[VCPU_SREG_FS]);
}

int vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	/* Nothing to do if hardware doesn&#x27;t support EFER. */
<blue>	if (!vmx_find_uret_msr(vmx, MSR_EFER))</blue>
		return 0;

<blue>	vcpu->arch.efer = efer;</blue>
#ifdef CONFIG_X86_64
	if (efer &amp; EFER_LMA)
<blue>		vm_entry_controls_setbit(vmx, VM_ENTRY_IA32E_MODE);</blue>
	else
<blue>		vm_entry_controls_clearbit(vmx, VM_ENTRY_IA32E_MODE);</blue>
#else
	if (KVM_BUG_ON(efer &amp; EFER_LMA, vcpu-&gt;kvm))
		return 1;
#endif

<blue>	vmx_setup_uret_msrs(vmx);</blue>
	return 0;
<blue>}</blue>

#ifdef CONFIG_X86_64

static void enter_lmode(struct kvm_vcpu *vcpu)
{
	u32 guest_tr_ar;

<blue>	vmx_segment_cache_clear(to_vmx(vcpu));</blue>

<blue>	guest_tr_ar = vmcs_read32(GUEST_TR_AR_BYTES);</blue>
<blue>	if ((guest_tr_ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {</blue>
<blue>		pr_debug_ratelimited("%s: tss fixup for long mode. \n",</blue>
				     __func__);
<blue>		vmcs_write32(GUEST_TR_AR_BYTES,</blue>
<blue>			     (guest_tr_ar & ~VMX_AR_TYPE_MASK)</blue>
			     | VMX_AR_TYPE_BUSY_64_TSS);
	}
<blue>	vmx_set_efer(vcpu, vcpu->arch.efer | EFER_LMA);</blue>
}

static void exit_lmode(struct kvm_vcpu *vcpu)
{
<blue>	vmx_set_efer(vcpu, vcpu->arch.efer & ~EFER_LMA);</blue>
}

#endif

<blue>static void vmx_flush_tlb_all(struct kvm_vcpu *vcpu)</blue>
<blue>{</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	/*
	 * INVEPT must be issued when EPT is enabled, irrespective of VPID, as
	 * the CPU is not required to invalidate guest-physical mappings on
	 * VM-Entry, even if VPID is disabled.  Guest-physical mappings are
	 * associated with the root EPT structure and not any particular VPID
	 * (INVVPID also isn&#x27;t required to invalidate guest-physical mappings).
	 */
<blue>	if (enable_ept) {</blue>
<blue>		ept_sync_global();</blue>
<yellow>	} else if (enable_vpid) {</yellow>
<yellow>		if (cpu_has_vmx_invvpid_global()) {</yellow>
<yellow>			vpid_sync_vcpu_global();</yellow>
		} else {
<yellow>			vpid_sync_vcpu_single(vmx->vpid);</yellow>
<blue>			vpid_sync_vcpu_single(vmx->nested.vpid02);</blue>
		}
	}
}

<blue>static inline int vmx_get_current_vpid(struct kvm_vcpu *vcpu)</blue>
{
<blue>	if (is_guest_mode(vcpu))</blue>
<blue>		return nested_get_vpid02(vcpu);</blue>
<blue>	return to_vmx(vcpu)->vpid;</blue>
}

<blue>static void vmx_flush_tlb_current(struct kvm_vcpu *vcpu)</blue>
<blue>{</blue>
<blue>	struct kvm_mmu *mmu = vcpu->arch.mmu;</blue>
	u64 root_hpa = mmu-&gt;root.hpa;

	/* No flush required if the current context is invalid. */
	if (!VALID_PAGE(root_hpa))
		return;

<blue>	if (enable_ept)</blue>
<blue>		ept_sync_context(construct_eptp(vcpu, root_hpa,</blue>
<blue>						mmu->root_role.level));</blue>
	else
<blue>		vpid_sync_context(vmx_get_current_vpid(vcpu));</blue>
}

static void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)
<yellow>{</yellow>
	/*
	 * vpid_sync_vcpu_addr() is a nop if vpid==0, see the comment in
	 * vmx_flush_tlb_guest() for an explanation of why this is ok.
	 */
<yellow>	vpid_sync_vcpu_addr(vmx_get_current_vpid(vcpu), addr);</yellow>
<yellow>}</yellow>

static void vmx_flush_tlb_guest(struct kvm_vcpu *vcpu)
<blue>{</blue>
	/*
	 * vpid_sync_context() is a nop if vpid==0, e.g. if enable_vpid==0 or a
	 * vpid couldn&#x27;t be allocated for this vCPU.  VM-Enter and VM-Exit are
	 * required to flush GVA-&gt;{G,H}PA mappings from the TLB if vpid is
	 * disabled (VM-Enter with vpid enabled and vpid==0 is disallowed),
	 * i.e. no explicit INVVPID is necessary.
	 */
<blue>	vpid_sync_context(vmx_get_current_vpid(vcpu));</blue>
}

void vmx_ept_load_pdptrs(struct kvm_vcpu *vcpu)
{
<blue>	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;</blue>

	if (!kvm_register_is_dirty(vcpu, VCPU_EXREG_PDPTR))
		return;

<blue>	if (is_pae_paging(vcpu)) {</blue>
<yellow>		vmcs_write64(GUEST_PDPTR0, mmu->pdptrs[0]);</yellow>
<yellow>		vmcs_write64(GUEST_PDPTR1, mmu->pdptrs[1]);</yellow>
<yellow>		vmcs_write64(GUEST_PDPTR2, mmu->pdptrs[2]);</yellow>
<yellow>		vmcs_write64(GUEST_PDPTR3, mmu->pdptrs[3]);</yellow>
	}
<blue>}</blue>

void ept_save_pdptrs(struct kvm_vcpu *vcpu)
{
<yellow>	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;</yellow>

<yellow>	if (WARN_ON_ONCE(!is_pae_paging(vcpu)))</yellow>
		return;

<yellow>	mmu->pdptrs[0] = vmcs_read64(GUEST_PDPTR0);</yellow>
<yellow>	mmu->pdptrs[1] = vmcs_read64(GUEST_PDPTR1);</yellow>
<yellow>	mmu->pdptrs[2] = vmcs_read64(GUEST_PDPTR2);</yellow>
<yellow>	mmu->pdptrs[3] = vmcs_read64(GUEST_PDPTR3);</yellow>

<yellow>	kvm_register_mark_available(vcpu, VCPU_EXREG_PDPTR);</yellow>
<yellow>}</yellow>

#define CR3_EXITING_BITS (CPU_BASED_CR3_LOAD_EXITING | \
			  CPU_BASED_CR3_STORE_EXITING)

void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	unsigned long hw_cr0, old_cr0_pg;
	u32 tmp;

<blue>	old_cr0_pg = kvm_read_cr0_bits(vcpu, X86_CR0_PG);</blue>

	hw_cr0 = (cr0 &amp; ~KVM_VM_CR0_ALWAYS_OFF);
<blue>	if (is_unrestricted_guest(vcpu))</blue>
<blue>		hw_cr0 |= KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST;</blue>
	else {
<blue>		hw_cr0 |= KVM_VM_CR0_ALWAYS_ON;</blue>
<blue>		if (!enable_ept)</blue>
<yellow>			hw_cr0 |= X86_CR0_WP;</yellow>

<blue>		if (vmx->rmode.vm86_active && (cr0 & X86_CR0_PE))</blue>
<yellow>			enter_pmode(vcpu);</yellow>

<blue>		if (!vmx->rmode.vm86_active && !(cr0 & X86_CR0_PE))</blue>
<yellow>			enter_rmode(vcpu);</yellow>
	}

<blue>	vmcs_writel(CR0_READ_SHADOW, cr0);</blue>
<blue>	vmcs_writel(GUEST_CR0, hw_cr0);</blue>
<blue>	vcpu->arch.cr0 = cr0;</blue>
	kvm_register_mark_available(vcpu, VCPU_EXREG_CR0);

#ifdef CONFIG_X86_64
	if (vcpu-&gt;arch.efer &amp; EFER_LME) {
<blue>		if (!old_cr0_pg && (cr0 & X86_CR0_PG))</blue>
<blue>			enter_lmode(vcpu);</blue>
<blue>		else if (old_cr0_pg && !(cr0 & X86_CR0_PG))</blue>
<blue>			exit_lmode(vcpu);</blue>
	}
#endif

<blue>	if (enable_ept && !is_unrestricted_guest(vcpu)) {</blue>
		/*
		 * Ensure KVM has an up-to-date snapshot of the guest&#x27;s CR3.  If
		 * the below code _enables_ CR3 exiting, vmx_cache_reg() will
		 * (correctly) stop reading vmcs.GUEST_CR3 because it thinks
		 * KVM&#x27;s CR3 is installed.
		 */
<blue>		if (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))</blue>
<blue>			vmx_cache_reg(vcpu, VCPU_EXREG_CR3);</blue>

		/*
		 * When running with EPT but not unrestricted guest, KVM must
		 * intercept CR3 accesses when paging is _disabled_.  This is
		 * necessary because restricted guests can&#x27;t actually run with
		 * paging disabled, and so KVM stuffs its own CR3 in order to
		 * run the guest when identity mapped page tables.
		 *
		 * Do _NOT_ check the old CR0.PG, e.g. to optimize away the
		 * update, it may be stale with respect to CR3 interception,
		 * e.g. after nested VM-Enter.
		 *
		 * Lastly, honor L1&#x27;s desires, i.e. intercept CR3 loads and/or
		 * stores to forward them to L1, even if KVM does not need to
		 * intercept them to preserve its identity mapped page tables.
		 */
<blue>		if (!(cr0 & X86_CR0_PG)) {</blue>
<yellow>			exec_controls_setbit(vmx, CR3_EXITING_BITS);</yellow>
<blue>		} else if (!is_guest_mode(vcpu)) {</blue>
<yellow>			exec_controls_clearbit(vmx, CR3_EXITING_BITS);</yellow>
		} else {
			tmp = exec_controls_get(vmx);
			tmp &amp;= ~CR3_EXITING_BITS;
<blue>			tmp |= get_vmcs12(vcpu)->cpu_based_vm_exec_control & CR3_EXITING_BITS;</blue>
<yellow>			exec_controls_set(vmx, tmp);</yellow>
		}

		/* Note, vmx_set_cr4() consumes the new vcpu-&gt;arch.cr0. */
<blue>		if ((old_cr0_pg ^ cr0) & X86_CR0_PG)</blue>
<blue>			vmx_set_cr4(vcpu, kvm_read_cr4(vcpu));</blue>

		/*
		 * When !CR0_PG -&gt; CR0_PG, vcpu-&gt;arch.cr3 becomes active, but
		 * GUEST_CR3 is still vmx-&gt;ept_identity_map_addr if EPT + !URG.
		 */
<blue>		if (!(old_cr0_pg & X86_CR0_PG) && (cr0 & X86_CR0_PG))</blue>
<blue>			kvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);</blue>
	}

	/* depends on vcpu-&gt;arch.cr0 to be set to a new value */
<blue>	vmx->emulation_required = vmx_emulation_required(vcpu);</blue>
}

static int vmx_get_max_tdp_level(void)
{
<yellow>	if (cpu_has_vmx_ept_5levels())</yellow>
		return 5;
	return 4;
}

u64 construct_eptp(struct kvm_vcpu *vcpu, hpa_t root_hpa, int root_level)
{
	u64 eptp = VMX_EPTP_MT_WB;

<blue>	eptp |= (root_level == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;</blue>

<blue>	if (enable_ept_ad_bits &&</blue>
<blue>	    (!is_guest_mode(vcpu) || nested_ept_ad_enabled(vcpu)))</blue>
<blue>		eptp |= VMX_EPTP_AD_ENABLE_BIT;</blue>
<blue>	eptp |= root_hpa;</blue>

	return eptp;
}

static void vmx_load_mmu_pgd(struct kvm_vcpu *vcpu, hpa_t root_hpa,
			     int root_level)
{
<blue>	struct kvm *kvm = vcpu->kvm;</blue>
	bool update_guest_cr3 = true;
	unsigned long guest_cr3;
	u64 eptp;

<blue>	if (enable_ept) {</blue>
<blue>		eptp = construct_eptp(vcpu, root_hpa, root_level);</blue>
<blue>		vmcs_write64(EPT_POINTER, eptp);</blue>

<blue>		hv_track_root_tdp(vcpu, root_hpa);</blue>

<blue>		if (!enable_unrestricted_guest && !is_paging(vcpu))</blue>
<yellow>			guest_cr3 = to_kvm_vmx(kvm)->ept_identity_map_addr;</yellow>
<blue>		else if (kvm_register_is_dirty(vcpu, VCPU_EXREG_CR3))</blue>
<blue>			guest_cr3 = vcpu->arch.cr3;</blue>
		else /* vmcs.GUEST_CR3 is already up-to-date. */
			update_guest_cr3 = false;
<blue>		vmx_ept_load_pdptrs(vcpu);</blue>
	} else {
<yellow>		guest_cr3 = root_hpa | kvm_get_active_pcid(vcpu);</yellow>
	}

	if (update_guest_cr3)
<blue>		vmcs_writel(GUEST_CR3, guest_cr3);</blue>
<blue>}</blue>


<blue>static bool vmx_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)</blue>
{
	/*
	 * We operate under the default treatment of SMM, so VMX cannot be
	 * enabled under SMM.  Note, whether or not VMXE is allowed at all,
	 * i.e. is a reserved bit, is handled by common x86 code.
	 */
<blue>	if ((cr4 & X86_CR4_VMXE) && is_smm(vcpu))</blue>
		return false;

<blue>	if (to_vmx(vcpu)->nested.vmxon && !nested_cr4_valid(vcpu, cr4))</blue>
		return false;

	return true;
<blue>}</blue>

void vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
{
<blue>	unsigned long old_cr4 = vcpu->arch.cr4;</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	/*
	 * Pass through host&#x27;s Machine Check Enable value to hw_cr4, which
	 * is in force while we are in guest mode.  Do not let guests control
	 * this bit, even if host CR4.MCE == 0.
	 */
	unsigned long hw_cr4;

	hw_cr4 = (cr4_read_shadow() &amp; X86_CR4_MCE) | (cr4 &amp; ~X86_CR4_MCE);
<blue>	if (is_unrestricted_guest(vcpu))</blue>
<blue>		hw_cr4 |= KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST;</blue>
<blue>	else if (vmx->rmode.vm86_active)</blue>
<yellow>		hw_cr4 |= KVM_RMODE_VM_CR4_ALWAYS_ON;</yellow>
	else
<blue>		hw_cr4 |= KVM_PMODE_VM_CR4_ALWAYS_ON;</blue>

<blue>	if (!boot_cpu_has(X86_FEATURE_UMIP) && vmx_umip_emulated()) {</blue>
<yellow>		if (cr4 & X86_CR4_UMIP) {</yellow>
<yellow>			secondary_exec_controls_setbit(vmx, SECONDARY_EXEC_DESC);</yellow>
<yellow>			hw_cr4 &= ~X86_CR4_UMIP;</yellow>
<yellow>		} else if (!is_guest_mode(vcpu) ||</yellow>
<yellow>			!nested_cpu_has2(get_vmcs12(vcpu), SECONDARY_EXEC_DESC)) {</yellow>
<yellow>			secondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_DESC);</yellow>
		}
	}

<blue>	vcpu->arch.cr4 = cr4;</blue>
	kvm_register_mark_available(vcpu, VCPU_EXREG_CR4);

<blue>	if (!is_unrestricted_guest(vcpu)) {</blue>
<blue>		if (enable_ept) {</blue>
<blue>			if (!is_paging(vcpu)) {</blue>
<yellow>				hw_cr4 &= ~X86_CR4_PAE;</yellow>
				hw_cr4 |= X86_CR4_PSE;
<blue>			} else if (!(cr4 & X86_CR4_PAE)) {</blue>
<blue>				hw_cr4 &= ~X86_CR4_PAE;</blue>
			}
		}

		/*
		 * SMEP/SMAP/PKU is disabled if CPU is in non-paging mode in
		 * hardware.  To emulate this behavior, SMEP/SMAP/PKU needs
		 * to be manually disabled when guest switches to non-paging
		 * mode.
		 *
		 * If !enable_unrestricted_guest, the CPU is always running
		 * with CR0.PG=1 and CR4 needs to be modified.
		 * If enable_unrestricted_guest, the CPU automatically
		 * disables SMEP/SMAP/PKU when the guest sets CR0.PG=0.
		 */
<yellow>		if (!is_paging(vcpu))</yellow>
<yellow>			hw_cr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);</yellow>
	}

<blue>	vmcs_writel(CR4_READ_SHADOW, cr4);</blue>
<blue>	vmcs_writel(GUEST_CR4, hw_cr4);</blue>

<blue>	if ((cr4 ^ old_cr4) & (X86_CR4_OSXSAVE | X86_CR4_PKE))</blue>
<blue>		kvm_update_cpuid_runtime(vcpu);</blue>
<blue>}</blue>

void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u32 ar;

<blue>	if (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {</blue>
<yellow>		*var = vmx->rmode.segs[seg];</yellow>
		if (seg == VCPU_SREG_TR
<yellow>		    || var->selector == vmx_read_guest_seg_selector(vmx, seg))</yellow>
			return;
<yellow>		var->base = vmx_read_guest_seg_base(vmx, seg);</yellow>
		var-&gt;selector = vmx_read_guest_seg_selector(vmx, seg);
		return;
	}
<blue>	var->base = vmx_read_guest_seg_base(vmx, seg);</blue>
<blue>	var->limit = vmx_read_guest_seg_limit(vmx, seg);</blue>
	var-&gt;selector = vmx_read_guest_seg_selector(vmx, seg);
	ar = vmx_read_guest_seg_ar(vmx, seg);
	var-&gt;unusable = (ar &gt;&gt; 16) &amp; 1;
	var-&gt;type = ar &amp; 15;
	var-&gt;s = (ar &gt;&gt; 4) &amp; 1;
	var-&gt;dpl = (ar &gt;&gt; 5) &amp; 3;
	/*
	 * Some userspaces do not preserve unusable property. Since usable
	 * segment has to be present according to VMX spec we can use present
	 * property to amend userspace bug by making unusable segment always
	 * nonpresent. vmx_segment_access_rights() already marks nonpresent
	 * segment as unusable.
	 */
	var-&gt;present = !var-&gt;unusable;
	var-&gt;avl = (ar &gt;&gt; 12) &amp; 1;
	var-&gt;l = (ar &gt;&gt; 13) &amp; 1;
	var-&gt;db = (ar &gt;&gt; 14) &amp; 1;
	var-&gt;g = (ar &gt;&gt; 15) &amp; 1;
<blue>}</blue>

<yellow>static u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)</yellow>
<blue>{</blue>
	struct kvm_segment s;

<blue>	if (to_vmx(vcpu)->rmode.vm86_active) {</blue>
<yellow>		vmx_get_segment(vcpu, &s, seg);</yellow>
		return s.base;
	}
<blue>	return vmx_read_guest_seg_base(to_vmx(vcpu), seg);</blue>
}

int vmx_get_cpl(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (unlikely(vmx->rmode.vm86_active))</blue>
<yellow>		return 0;</yellow>
	else {
<blue>		int ar = vmx_read_guest_seg_ar(vmx, VCPU_SREG_SS);</blue>
<blue>		return VMX_AR_DPL(ar);</blue>
	}
}

<blue>static u32 vmx_segment_access_rights(struct kvm_segment *var)</blue>
{
	u32 ar;

<blue>	if (var->unusable || !var->present)</blue>
		ar = 1 &lt;&lt; 16;
	else {
<blue>		ar = var->type & 15;</blue>
<blue>		ar |= (var->s & 1) << 4;</blue>
		ar |= (var-&gt;dpl &amp; 3) &lt;&lt; 5;
		ar |= (var-&gt;present &amp; 1) &lt;&lt; 7;
		ar |= (var-&gt;avl &amp; 1) &lt;&lt; 12;
		ar |= (var-&gt;l &amp; 1) &lt;&lt; 13;
		ar |= (var-&gt;db &amp; 1) &lt;&lt; 14;
		ar |= (var-&gt;g &amp; 1) &lt;&lt; 15;
	}

	return ar;
<blue>}</blue>

void __vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<blue>	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];</blue>

	vmx_segment_cache_clear(vmx);

<yellow>	if (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {</yellow>
<yellow>		vmx->rmode.segs[seg] = *var;</yellow>
		if (seg == VCPU_SREG_TR)
<yellow>			vmcs_write16(sf->selector, var->selector);</yellow>
<yellow>		else if (var->s)</yellow>
<yellow>			fix_rmode_seg(seg, &vmx->rmode.segs[seg]);</yellow>
		return;
	}

<blue>	vmcs_writel(sf->base, var->base);</blue>
<blue>	vmcs_write32(sf->limit, var->limit);</blue>
<blue>	vmcs_write16(sf->selector, var->selector);</blue>

	/*
	 *   Fix the &quot;Accessed&quot; bit in AR field of segment registers for older
	 * qemu binaries.
	 *   IA32 arch specifies that at the time of processor reset the
	 * &quot;Accessed&quot; bit in the AR field of segment registers is 1. And qemu
	 * is setting it to 0 in the userland code. This causes invalid guest
	 * state vmexit when &quot;unrestricted guest&quot; mode is turned on.
	 *    Fix for this setup issue in cpu_reset is being pushed in the qemu
	 * tree. Newer qemu binaries with that qemu fix would not need this
	 * kvm hack.
	 */
<blue>	if (is_unrestricted_guest(vcpu) && (seg != VCPU_SREG_LDTR))</blue>
<blue>		var->type |= 0x1; /* Accessed */</blue>

<blue>	vmcs_write32(sf->ar_bytes, vmx_segment_access_rights(var));</blue>
<blue>}</blue>

static void vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)
{
<blue>	__vmx_set_segment(vcpu, var, seg);</blue>

<blue>	to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);</blue>
}

static void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)
{
<blue>	u32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);</blue>

	*db = (ar &gt;&gt; 14) &amp; 1;
	*l = (ar &gt;&gt; 13) &amp; 1;
}

static void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
{
<blue>	dt->size = vmcs_read32(GUEST_IDTR_LIMIT);</blue>
<blue>	dt->address = vmcs_readl(GUEST_IDTR_BASE);</blue>
}

static void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
{
<blue>	vmcs_write32(GUEST_IDTR_LIMIT, dt->size);</blue>
<blue>	vmcs_writel(GUEST_IDTR_BASE, dt->address);</blue>
<blue>}</blue>

static void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
{
<blue>	dt->size = vmcs_read32(GUEST_GDTR_LIMIT);</blue>
<blue>	dt->address = vmcs_readl(GUEST_GDTR_BASE);</blue>
}

static void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
{
<blue>	vmcs_write32(GUEST_GDTR_LIMIT, dt->size);</blue>
<blue>	vmcs_writel(GUEST_GDTR_BASE, dt->address);</blue>
<blue>}</blue>

static bool rmode_segment_valid(struct kvm_vcpu *vcpu, int seg)
<blue>{</blue>
	struct kvm_segment var;
	u32 ar;

<blue>	vmx_get_segment(vcpu, &var, seg);</blue>
	var.dpl = 0x3;
	if (seg == VCPU_SREG_CS)
<blue>		var.type = 0x3;</blue>
<blue>	ar = vmx_segment_access_rights(&var);</blue>

<blue>	if (var.base != (var.selector << 4))</blue>
		return false;
<yellow>	if (var.limit != 0xffff)</yellow>
		return false;
<yellow>	if (ar != 0xf3)</yellow>
		return false;

	return true;
}

static bool code_segment_valid(struct kvm_vcpu *vcpu)
{
	struct kvm_segment cs;
	unsigned int cs_rpl;

<blue>	vmx_get_segment(vcpu, &cs, VCPU_SREG_CS);</blue>
<blue>	cs_rpl = cs.selector & SEGMENT_RPL_MASK;</blue>

	if (cs.unusable)
		return false;
<blue>	if (~cs.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_ACCESSES_MASK))</blue>
		return false;
<blue>	if (!cs.s)</blue>
		return false;
	if (cs.type &amp; VMX_AR_TYPE_WRITEABLE_MASK) {
<yellow>		if (cs.dpl > cs_rpl)</yellow>
			return false;
	} else {
<blue>		if (cs.dpl != cs_rpl)</blue>
			return false;
	}
<blue>	if (!cs.present)</blue>
		return false;

	/* TODO: Add Reserved field check, this&#x27;ll require a new member in the kvm_segment_field structure */
	return true;
}

static bool stack_segment_valid(struct kvm_vcpu *vcpu)
{
	struct kvm_segment ss;
	unsigned int ss_rpl;

<blue>	vmx_get_segment(vcpu, &ss, VCPU_SREG_SS);</blue>
	ss_rpl = ss.selector &amp; SEGMENT_RPL_MASK;

	if (ss.unusable)
		return true;
<blue>	if (ss.type != 3 && ss.type != 7)</blue>
		return false;
<blue>	if (!ss.s)</blue>
		return false;
<blue>	if (ss.dpl != ss_rpl) /* DPL != RPL */</blue>
		return false;
<blue>	if (!ss.present)</blue>
		return false;

	return true;
}

static bool data_segment_valid(struct kvm_vcpu *vcpu, int seg)
<blue>{</blue>
	struct kvm_segment var;
	unsigned int rpl;

<blue>	vmx_get_segment(vcpu, &var, seg);</blue>
	rpl = var.selector &amp; SEGMENT_RPL_MASK;

	if (var.unusable)
		return true;
<blue>	if (!var.s)</blue>
		return false;
<blue>	if (!var.present)</blue>
		return false;
<blue>	if (~var.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_WRITEABLE_MASK)) {</blue>
<blue>		if (var.dpl < rpl) /* DPL < RPL */</blue>
			return false;
	}

	/* TODO: Add other members to kvm_segment_field to allow checking for other access
	 * rights flags
	 */
	return true;
}

static bool tr_valid(struct kvm_vcpu *vcpu)
{
	struct kvm_segment tr;

<blue>	vmx_get_segment(vcpu, &tr, VCPU_SREG_TR);</blue>

	if (tr.unusable)
		return false;
<blue>	if (tr.selector & SEGMENT_TI_MASK)	/* TI = 1 */</blue>
		return false;
<blue>	if (tr.type != 3 && tr.type != 11) /* TODO: Check if guest is in IA32e mode */</blue>
		return false;
<blue>	if (!tr.present)</blue>
<yellow>		return false;</yellow>

	return true;
}

static bool ldtr_valid(struct kvm_vcpu *vcpu)
{
	struct kvm_segment ldtr;

<blue>	vmx_get_segment(vcpu, &ldtr, VCPU_SREG_LDTR);</blue>

	if (ldtr.unusable)
		return true;
<blue>	if (ldtr.selector & SEGMENT_TI_MASK)	/* TI = 1 */</blue>
		return false;
<blue>	if (ldtr.type != 2)</blue>
		return false;
<blue>	if (!ldtr.present)</blue>
		return false;

	return true;
}

static bool cs_ss_rpl_check(struct kvm_vcpu *vcpu)
{
	struct kvm_segment cs, ss;

	vmx_get_segment(vcpu, &amp;cs, VCPU_SREG_CS);
<blue>	vmx_get_segment(vcpu, &ss, VCPU_SREG_SS);</blue>

	return ((cs.selector &amp; SEGMENT_RPL_MASK) ==
		 (ss.selector &amp; SEGMENT_RPL_MASK));
}

/*
 * Check if guest state is valid. Returns true if valid, false if
 * not.
 * We assume that registers are always usable
 */
bool __vmx_guest_state_valid(struct kvm_vcpu *vcpu)
<blue>{</blue>
	/* real mode guest state checks */
<blue>	if (!is_protmode(vcpu) || (vmx_get_rflags(vcpu) & X86_EFLAGS_VM)) {</blue>
<blue>		if (!rmode_segment_valid(vcpu, VCPU_SREG_CS))</blue>
			return false;
<yellow>		if (!rmode_segment_valid(vcpu, VCPU_SREG_SS))</yellow>
			return false;
<yellow>		if (!rmode_segment_valid(vcpu, VCPU_SREG_DS))</yellow>
			return false;
<yellow>		if (!rmode_segment_valid(vcpu, VCPU_SREG_ES))</yellow>
			return false;
<yellow>		if (!rmode_segment_valid(vcpu, VCPU_SREG_FS))</yellow>
			return false;
<yellow>		if (!rmode_segment_valid(vcpu, VCPU_SREG_GS))</yellow>
			return false;
	} else {
	/* protected mode guest state checks */
<blue>		if (!cs_ss_rpl_check(vcpu))</blue>
			return false;
<blue>		if (!code_segment_valid(vcpu))</blue>
			return false;
<blue>		if (!stack_segment_valid(vcpu))</blue>
			return false;
<blue>		if (!data_segment_valid(vcpu, VCPU_SREG_DS))</blue>
			return false;
<blue>		if (!data_segment_valid(vcpu, VCPU_SREG_ES))</blue>
			return false;
<blue>		if (!data_segment_valid(vcpu, VCPU_SREG_FS))</blue>
			return false;
<blue>		if (!data_segment_valid(vcpu, VCPU_SREG_GS))</blue>
			return false;
<blue>		if (!tr_valid(vcpu))</blue>
			return false;
<blue>		if (!ldtr_valid(vcpu))</blue>
			return false;
	}
	/* TODO:
	 * - Add checks on RIP
	 * - Add checks on RFLAGS
	 */

	return true;
}

static int init_rmode_tss(struct kvm *kvm, void __user *ua)
{
<yellow>	const void *zero_page = (const void *) __va(page_to_phys(ZERO_PAGE(0)));</yellow>
	u16 data;
	int i;

<yellow>	for (i = 0; i < 3; i++) {</yellow>
<yellow>		if (__copy_to_user(ua + PAGE_SIZE * i, zero_page, PAGE_SIZE))</yellow>
			return -EFAULT;
	}

<yellow>	data = TSS_BASE_SIZE + TSS_REDIRECTION_SIZE;</yellow>
	if (__copy_to_user(ua + TSS_IOPB_BASE_OFFSET, &amp;data, sizeof(u16)))
		return -EFAULT;

	data = ~0;
<yellow>	if (__copy_to_user(ua + RMODE_TSS_SIZE - 1, &data, sizeof(u8)))</yellow>
		return -EFAULT;

	return 0;
}

static int init_rmode_identity_map(struct kvm *kvm)
{
	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);
	int i, r = 0;
	void __user *uaddr;
	u32 tmp;

	/* Protect kvm_vmx-&gt;ept_identity_pagetable_done. */
	mutex_lock(&amp;kvm-&gt;slots_lock);

<yellow>	if (likely(kvm_vmx->ept_identity_pagetable_done))</yellow>
		goto out;

<yellow>	if (!kvm_vmx->ept_identity_map_addr)</yellow>
<yellow>		kvm_vmx->ept_identity_map_addr = VMX_EPT_IDENTITY_PAGETABLE_ADDR;</yellow>

<yellow>	uaddr = __x86_set_memory_region(kvm,</yellow>
					IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
					kvm_vmx-&gt;ept_identity_map_addr,
					PAGE_SIZE);
	if (IS_ERR(uaddr)) {
		r = PTR_ERR(uaddr);
		goto out;
	}

	/* Set up identity-mapping pagetable for EPT in real mode */
<yellow>	for (i = 0; i < (PAGE_SIZE / sizeof(tmp)); i++) {</yellow>
<yellow>		tmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |</yellow>
			_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);
		if (__copy_to_user(uaddr + i * sizeof(tmp), &amp;tmp, sizeof(tmp))) {
			r = -EFAULT;
			goto out;
		}
	}
<yellow>	kvm_vmx->ept_identity_pagetable_done = true;</yellow>

out:
<yellow>	mutex_unlock(&kvm->slots_lock);</yellow>
	return r;
}

static void seg_setup(int seg)
{
<blue>	const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];</blue>
	unsigned int ar;

<blue>	vmcs_write16(sf->selector, 0);</blue>
<blue>	vmcs_writel(sf->base, 0);</blue>
<blue>	vmcs_write32(sf->limit, 0xffff);</blue>
	ar = 0x93;
<blue>	if (seg == VCPU_SREG_CS)</blue>
		ar |= 0x08; /* code segment */

<blue>	vmcs_write32(sf->ar_bytes, ar);</blue>
<blue>}</blue>

static int alloc_apic_access_page(struct kvm *kvm)
{
	struct page *page;
	void __user *hva;
	int ret = 0;

	mutex_lock(&amp;kvm-&gt;slots_lock);
<blue>	if (kvm->arch.apic_access_memslot_enabled)</blue>
		goto out;
<blue>	hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,</blue>
				      APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);
	if (IS_ERR(hva)) {
		ret = PTR_ERR(hva);
		goto out;
	}

<blue>	page = gfn_to_page(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);</blue>
	if (is_error_page(page)) {
		ret = -EFAULT;
		goto out;
	}

	/*
	 * Do not pin the page in memory, so that memory hot-unplug
	 * is able to migrate it.
	 */
<blue>	put_page(page);</blue>
<blue>	kvm->arch.apic_access_memslot_enabled = true;</blue>
out:
<blue>	mutex_unlock(&kvm->slots_lock);</blue>
	return ret;
}

<blue>int allocate_vpid(void)</blue>
{
	int vpid;

<blue>	if (!enable_vpid)</blue>
		return 0;
<blue>	spin_lock(&vmx_vpid_lock);</blue>
	vpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);
	if (vpid &lt; VMX_NR_VPIDS)
<blue>		__set_bit(vpid, vmx_vpid_bitmap);</blue>
	else
		vpid = 0;
<blue>	spin_unlock(&vmx_vpid_lock);</blue>
	return vpid;
<blue>}</blue>

<blue>void free_vpid(int vpid)</blue>
{
<blue>	if (!enable_vpid || vpid == 0)</blue>
		return;
<blue>	spin_lock(&vmx_vpid_lock);</blue>
	__clear_bit(vpid, vmx_vpid_bitmap);
	spin_unlock(&amp;vmx_vpid_lock);
<blue>}</blue>

static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
{
	/*
	 * When KVM is a nested hypervisor on top of Hyper-V and uses
	 * &#x27;Enlightened MSR Bitmap&#x27; feature L0 needs to know that MSR
	 * bitmap has changed.
	 */
<blue>	if (static_branch_unlikely(&enable_evmcs))</blue>
<yellow>		evmcs_touch_msr_bitmap();</yellow>

<blue>	vmx->nested.force_msr_bitmap_recalc = true;</blue>
}

void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<blue>	unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;</blue>

	if (!cpu_has_vmx_msr_bitmap())
		return;

<blue>	vmx_msr_bitmap_l01_changed(vmx);</blue>

	/*
	 * Mark the desired intercept state in shadow bitmap, this is needed
	 * for resync when the MSR filters change.
	*/
	if (is_valid_passthrough_msr(msr)) {
<blue>		int idx = possible_passthrough_msr_slot(msr);</blue>

		if (idx != -ENOENT) {
<blue>			if (type & MSR_TYPE_R)</blue>
<blue>				clear_bit(idx, vmx->shadow_msr_intercept.read);</blue>
<blue>			if (type & MSR_TYPE_W)</blue>
<blue>				clear_bit(idx, vmx->shadow_msr_intercept.write);</blue>
		}
	}

<blue>	if ((type & MSR_TYPE_R) &&</blue>
<blue>	    !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {</blue>
<blue>		vmx_set_msr_bitmap_read(msr_bitmap, msr);</blue>
		type &amp;= ~MSR_TYPE_R;
	}

<blue>	if ((type & MSR_TYPE_W) &&</blue>
<blue>	    !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {</blue>
<yellow>		vmx_set_msr_bitmap_write(msr_bitmap, msr);</yellow>
<yellow>		type &= ~MSR_TYPE_W;</yellow>
	}

<blue>	if (type & MSR_TYPE_R)</blue>
<blue>		vmx_clear_msr_bitmap_read(msr_bitmap, msr);</blue>

<blue>	if (type & MSR_TYPE_W)</blue>
<blue>		vmx_clear_msr_bitmap_write(msr_bitmap, msr);</blue>
<blue>}</blue>

void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<yellow>	unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;</yellow>

	if (!cpu_has_vmx_msr_bitmap())
		return;

<yellow>	vmx_msr_bitmap_l01_changed(vmx);</yellow>

	/*
	 * Mark the desired intercept state in shadow bitmap, this is needed
	 * for resync when the MSR filter changes.
	*/
	if (is_valid_passthrough_msr(msr)) {
<yellow>		int idx = possible_passthrough_msr_slot(msr);</yellow>

		if (idx != -ENOENT) {
<yellow>			if (type & MSR_TYPE_R)</yellow>
<yellow>				set_bit(idx, vmx->shadow_msr_intercept.read);</yellow>
<yellow>			if (type & MSR_TYPE_W)</yellow>
<yellow>				set_bit(idx, vmx->shadow_msr_intercept.write);</yellow>
		}
	}

<yellow>	if (type & MSR_TYPE_R)</yellow>
<yellow>		vmx_set_msr_bitmap_read(msr_bitmap, msr);</yellow>

<yellow>	if (type & MSR_TYPE_W)</yellow>
<yellow>		vmx_set_msr_bitmap_write(msr_bitmap, msr);</yellow>
<yellow>}</yellow>

static void vmx_reset_x2apic_msrs(struct kvm_vcpu *vcpu, u8 mode)
{
<yellow>	unsigned long *msr_bitmap = to_vmx(vcpu)->vmcs01.msr_bitmap;</yellow>
	unsigned long read_intercept;
	int msr;

	read_intercept = (mode &amp; MSR_BITMAP_MODE_X2APIC_APICV) ? 0 : ~0;

	for (msr = 0x800; msr &lt;= 0x8ff; msr += BITS_PER_LONG) {
<yellow>		unsigned int read_idx = msr / BITS_PER_LONG;</yellow>
		unsigned int write_idx = read_idx + (0x800 / sizeof(long));

		msr_bitmap[read_idx] = read_intercept;
		msr_bitmap[write_idx] = ~0ul;
	}
}

static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u8 mode;

<blue>	if (!cpu_has_vmx_msr_bitmap())</blue>
		return;

<blue>	if (cpu_has_secondary_exec_ctrls() &&</blue>
<blue>	    (secondary_exec_controls_get(vmx) &</blue>
	     SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {
		mode = MSR_BITMAP_MODE_X2APIC;
<yellow>		if (enable_apicv && kvm_vcpu_apicv_active(vcpu))</yellow>
			mode |= MSR_BITMAP_MODE_X2APIC_APICV;
	} else {
		mode = 0;
	}

<blue>	if (mode == vmx->x2apic_msr_bitmap_mode)</blue>
		return;

	vmx-&gt;x2apic_msr_bitmap_mode = mode;

<yellow>	vmx_reset_x2apic_msrs(vcpu, mode);</yellow>

	/*
	 * TPR reads and writes can be virtualized even if virtual interrupt
	 * delivery is not in use.
	 */
<yellow>	vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,</yellow>
				  !(mode &amp; MSR_BITMAP_MODE_X2APIC));

<yellow>	if (mode & MSR_BITMAP_MODE_X2APIC_APICV) {</yellow>
<yellow>		vmx_enable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_RW);</yellow>
		vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
		vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
<yellow>		if (enable_ipiv)</yellow>
<yellow>			vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_ICR), MSR_TYPE_RW);</yellow>
	}
<blue>}</blue>

void pt_update_intercept_for_msr(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<yellow>	bool flag = !(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN);</yellow>
	u32 i;

<yellow>	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);</yellow>
	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
<yellow>	for (i = 0; i < vmx->pt_desc.num_address_ranges; i++) {</yellow>
<yellow>		vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);</yellow>
		vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
	}
<yellow>}</yellow>

static bool vmx_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	void *vapic_page;
	u32 vppr;
	int rvi;

<yellow>	if (WARN_ON_ONCE(!is_guest_mode(vcpu)) ||</yellow>
<yellow>		!nested_cpu_has_vid(get_vmcs12(vcpu)) ||</yellow>
<yellow>		WARN_ON_ONCE(!vmx->nested.virtual_apic_map.gfn))</yellow>
		return false;

<yellow>	rvi = vmx_get_rvi();</yellow>

<yellow>	vapic_page = vmx->nested.virtual_apic_map.hva;</yellow>
	vppr = *((u32 *)(vapic_page + APIC_PROCPRI));

<yellow>	return ((rvi & 0xf0) > (vppr & 0xf0));</yellow>
<yellow>}</yellow>

static void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u32 i;

	/*
	 * Redo intercept permissions for MSRs that KVM is passing through to
	 * the guest.  Disabling interception will check the new MSR filter and
	 * ensure that KVM enables interception if usersepace wants to filter
	 * the MSR.  MSRs that KVM is already intercepting don&#x27;t need to be
	 * refreshed since KVM is going to intercept them regardless of what
	 * userspace wants.
	 */
<yellow>	for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {</yellow>
<yellow>		u32 msr = vmx_possible_passthrough_msrs[i];</yellow>

		if (!test_bit(i, vmx-&gt;shadow_msr_intercept.read))
<yellow>			vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_R);</yellow>

<yellow>		if (!test_bit(i, vmx->shadow_msr_intercept.write))</yellow>
<yellow>			vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_W);</yellow>
	}

	/* PT MSRs can be passed through iff PT is exposed to the guest. */
<yellow>	if (vmx_pt_mode_is_host_guest())</yellow>
<yellow>		pt_update_intercept_for_msr(vcpu);</yellow>
<yellow>}</yellow>

<blue>static inline void kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,</blue>
						     int pi_vec)
{
#ifdef CONFIG_SMP
<blue>	if (vcpu->mode == IN_GUEST_MODE) {</blue>
		/*
		 * The vector of the virtual has already been set in the PIR.
		 * Send a notification event to deliver the virtual interrupt
		 * unless the vCPU is the currently running vCPU, i.e. the
		 * event is being sent from a fastpath VM-Exit handler, in
		 * which case the PIR will be synced to the vIRR before
		 * re-entering the guest.
		 *
		 * When the target is not the running vCPU, the following
		 * possibilities emerge:
		 *
		 * Case 1: vCPU stays in non-root mode. Sending a notification
		 * event posts the interrupt to the vCPU.
		 *
		 * Case 2: vCPU exits to root mode and is still runnable. The
		 * PIR will be synced to the vIRR before re-entering the guest.
		 * Sending a notification event is ok as the host IRQ handler
		 * will ignore the spurious event.
		 *
		 * Case 3: vCPU exits to root mode and is blocked. vcpu_block()
		 * has already synced PIR to vIRR and never blocks the vCPU if
		 * the vIRR is not empty. Therefore, a blocked vCPU here does
		 * not wait for any requested interrupts in PIR, and sending a
		 * notification event also results in a benign, spurious event.
		 */

<blue>		if (vcpu != kvm_get_running_vcpu())</blue>
<yellow>			apic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);</yellow>
		return;
	}
#endif
	/*
	 * The vCPU isn&#x27;t in the guest; wake the vCPU in case it is blocking,
	 * otherwise do nothing as KVM will grab the highest priority pending
	 * IRQ via -&gt;sync_pir_to_irr() in vcpu_enter_guest().
	 */
<blue>	kvm_vcpu_wake_up(vcpu);</blue>
}

static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
						int vector)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	if (is_guest_mode(vcpu) &amp;&amp;
<yellow>	    vector == vmx->nested.posted_intr_nv) {</yellow>
		/*
		 * If a posted intr is not recognized by hardware,
		 * we will accomplish it in the next vmentry.
		 */
<yellow>		vmx->nested.pi_pending = true;</yellow>
		kvm_make_request(KVM_REQ_EVENT, vcpu);

		/*
		 * This pairs with the smp_mb_*() after setting vcpu-&gt;mode in
		 * vcpu_enter_guest() to guarantee the vCPU sees the event
		 * request if triggering a posted interrupt &quot;fails&quot; because
		 * vcpu-&gt;mode != IN_GUEST_MODE.  The extra barrier is needed as
		 * the smb_wmb() in kvm_make_request() only ensures everything
		 * done before making the request is visible when the request
		 * is visible, it doesn&#x27;t ensure ordering between the store to
		 * vcpu-&gt;requests and the load from vcpu-&gt;mode.
		 */
		smp_mb__after_atomic();

		/* the PIR and ON have been set by L1. */
<yellow>		kvm_vcpu_trigger_posted_interrupt(vcpu, POSTED_INTR_NESTED_VECTOR);</yellow>
		return 0;
	}
	return -1;
}
/*
 * Send interrupt to vcpu via posted interrupt way.
 * 1. If target vcpu is running(non-root mode), send posted interrupt
 * notification to vcpu and hardware will sync PIR to vIRR atomically.
 * 2. If target vcpu isn&#x27;t running(root mode), kick it to pick up the
 * interrupt from PIR in next vmentry.
 */
static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	int r;

<yellow>	r = vmx_deliver_nested_posted_interrupt(vcpu, vector);</yellow>
	if (!r)
		return 0;

	/* Note, this is called iff the local APIC is in-kernel. */
<blue>	if (!vcpu->arch.apic->apicv_active)</blue>
		return -1;

<blue>	if (pi_test_and_set_pir(vector, &vmx->pi_desc))</blue>
		return 0;

	/* If a previous notification has sent the IPI, nothing to do.  */
<blue>	if (pi_test_and_set_on(&vmx->pi_desc))</blue>
		return 0;

	/*
	 * The implied barrier in pi_test_and_set_on() pairs with the smp_mb_*()
	 * after setting vcpu-&gt;mode in vcpu_enter_guest(), thus the vCPU is
	 * guaranteed to see PID.ON=1 and sync the PIR to IRR if triggering a
	 * posted interrupt &quot;fails&quot; because vcpu-&gt;mode != IN_GUEST_MODE.
	 */
<blue>	kvm_vcpu_trigger_posted_interrupt(vcpu, POSTED_INTR_VECTOR);</blue>
	return 0;
}

static void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
				  int trig_mode, int vector)
{
<blue>	struct kvm_vcpu *vcpu = apic->vcpu;</blue>

<blue>	if (vmx_deliver_posted_interrupt(vcpu, vector)) {</blue>
<yellow>		kvm_lapic_set_irr(vector, apic);</yellow>
		kvm_make_request(KVM_REQ_EVENT, vcpu);
		kvm_vcpu_kick(vcpu);
	} else {
<blue>		trace_kvm_apicv_accept_irq(vcpu->vcpu_id, delivery_mode,</blue>
					   trig_mode, vector);
	}
<blue>}</blue>

/*
 * Set up the vmcs&#x27;s constant host-state fields, i.e., host-state fields that
 * will not change in the lifetime of the guest.
 * Note that host-state that does change is set elsewhere. E.g., host-state
 * that is set differently for each CPU is set in vmx_vcpu_load(), not here.
 */
void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
{
	u32 low32, high32;
	unsigned long tmpl;
	unsigned long cr0, cr3, cr4;

<blue>	cr0 = read_cr0();</blue>
<yellow>	WARN_ON(cr0 & X86_CR0_TS);</yellow>
<blue>	vmcs_writel(HOST_CR0, cr0);  /* 22.2.3 */</blue>

	/*
	 * Save the most likely value for this task&#x27;s CR3 in the VMCS.
	 * We can&#x27;t use __get_current_cr3_fast() because we&#x27;re not atomic.
	 */
<blue>	cr3 = __read_cr3();</blue>
<blue>	vmcs_writel(HOST_CR3, cr3);		/* 22.2.3  FIXME: shadow tables */</blue>
<blue>	vmx->loaded_vmcs->host_state.cr3 = cr3;</blue>

	/* Save the most likely value for this task&#x27;s CR4 in the VMCS. */
	cr4 = cr4_read_shadow();
<blue>	vmcs_writel(HOST_CR4, cr4);			/* 22.2.3, 22.2.5 */</blue>
<blue>	vmx->loaded_vmcs->host_state.cr4 = cr4;</blue>

<blue>	vmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */</blue>
#ifdef CONFIG_X86_64
	/*
	 * Load null selectors, so we can avoid reloading them in
	 * vmx_prepare_switch_to_host(), in case userspace uses
	 * the null selectors too (the expected case).
	 */
<blue>	vmcs_write16(HOST_DS_SELECTOR, 0);</blue>
<blue>	vmcs_write16(HOST_ES_SELECTOR, 0);</blue>
#else
	vmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
	vmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
#endif
<blue>	vmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */</blue>
<blue>	vmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */</blue>

<blue>	vmcs_writel(HOST_IDTR_BASE, host_idt_base);   /* 22.2.4 */</blue>

<blue>	vmcs_writel(HOST_RIP, (unsigned long)vmx_vmexit); /* 22.2.5 */</blue>

<blue>	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);</blue>
<blue>	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);</blue>

	/*
	 * SYSENTER is used for 32-bit system calls on either 32-bit or
	 * 64-bit kernels.  It is always zero If neither is allowed, otherwise
	 * vmx_vcpu_load_vmcs loads it with the per-CPU entry stack (and may
	 * have already done so!).
	 */
	if (!IS_ENABLED(CONFIG_IA32_EMULATION) &amp;&amp; !IS_ENABLED(CONFIG_X86_32))
		vmcs_writel(HOST_IA32_SYSENTER_ESP, 0);

<blue>	rdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);</blue>
<blue>	vmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */</blue>

<blue>	if (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {</blue>
<blue>		rdmsr(MSR_IA32_CR_PAT, low32, high32);</blue>
<blue>		vmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));</blue>
	}

<blue>	if (cpu_has_load_ia32_efer())</blue>
<blue>		vmcs_write64(HOST_IA32_EFER, host_efer);</blue>
<blue>}</blue>

void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
{
	struct kvm_vcpu *vcpu = &amp;vmx-&gt;vcpu;

	vcpu-&gt;arch.cr4_guest_owned_bits = KVM_POSSIBLE_CR4_GUEST_BITS &amp;
<blue>					  ~vcpu->arch.cr4_guest_rsvd_bits;</blue>
<blue>	if (!enable_ept) {</blue>
		vcpu-&gt;arch.cr4_guest_owned_bits &amp;= ~X86_CR4_TLBFLUSH_BITS;
<yellow>		vcpu->arch.cr4_guest_owned_bits &= ~X86_CR4_PDPTR_BITS;</yellow>
	}
<blue>	if (is_guest_mode(&vmx->vcpu))</blue>
		vcpu-&gt;arch.cr4_guest_owned_bits &amp;=
<blue>			~get_vmcs12(vcpu)->cr4_guest_host_mask;</blue>
<blue>	vmcs_writel(CR4_GUEST_HOST_MASK, ~vcpu->arch.cr4_guest_owned_bits);</blue>
<blue>}</blue>

static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
{
<blue>	u32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;</blue>

<blue>	if (!kvm_vcpu_apicv_active(&vmx->vcpu))</blue>
<blue>		pin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;</blue>

<blue>	if (!enable_vnmi)</blue>
<yellow>		pin_based_exec_ctrl &= ~PIN_BASED_VIRTUAL_NMIS;</yellow>

<blue>	if (!enable_preemption_timer)</blue>
<yellow>		pin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;</yellow>

	return pin_based_exec_ctrl;
<blue>}</blue>

static u32 vmx_vmentry_ctrl(void)
{
	u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;

<blue>	if (vmx_pt_mode_is_system())</blue>
<blue>		vmentry_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP |</blue>
				  VM_ENTRY_LOAD_IA32_RTIT_CTL);
	/*
	 * IA32e mode, and loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically.
	 */
	vmentry_ctrl &amp;= ~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |
			  VM_ENTRY_LOAD_IA32_EFER |
			  VM_ENTRY_IA32E_MODE);

	if (cpu_has_perf_global_ctrl_bug())
		vmentry_ctrl &amp;= ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;

	return vmentry_ctrl;
}

static u32 vmx_vmexit_ctrl(void)
{
	u32 vmexit_ctrl = vmcs_config.vmexit_ctrl;

	/*
	 * Not used by KVM and never set in vmcs01 or vmcs02, but emulated for
	 * nested virtualization and thus allowed to be set in vmcs12.
	 */
<yellow>	vmexit_ctrl &= ~(VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER |</yellow>
			 VM_EXIT_SAVE_VMX_PREEMPTION_TIMER);

<blue>	if (vmx_pt_mode_is_system())</blue>
<blue>		vmexit_ctrl &= ~(VM_EXIT_PT_CONCEAL_PIP |</blue>
				 VM_EXIT_CLEAR_IA32_RTIT_CTL);

<blue>	if (cpu_has_perf_global_ctrl_bug())</blue>
<yellow>		vmexit_ctrl &= ~VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;</yellow>

	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
	return vmexit_ctrl &amp;
		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
}

<blue>static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (is_guest_mode(vcpu)) {</blue>
<yellow>		vmx->nested.update_vmcs01_apicv_status = true;</yellow>
		return;
	}

<blue>	pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));</blue>

<blue>	if (kvm_vcpu_apicv_active(vcpu)) {</blue>
<blue>		secondary_exec_controls_setbit(vmx,</blue>
					       SECONDARY_EXEC_APIC_REGISTER_VIRT |
					       SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
<yellow>		if (enable_ipiv)</yellow>
<yellow>			tertiary_exec_controls_setbit(vmx, TERTIARY_EXEC_IPI_VIRT);</yellow>
	} else {
<blue>		secondary_exec_controls_clearbit(vmx,</blue>
						 SECONDARY_EXEC_APIC_REGISTER_VIRT |
						 SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
<blue>		if (enable_ipiv)</blue>
<yellow>			tertiary_exec_controls_clearbit(vmx, TERTIARY_EXEC_IPI_VIRT);</yellow>
	}

<blue>	vmx_update_msr_bitmap_x2apic(vcpu);</blue>
<blue>}</blue>

static u32 vmx_exec_control(struct vcpu_vmx *vmx)
{
	u32 exec_control = vmcs_config.cpu_based_exec_ctrl;

	/*
	 * Not used by KVM, but fully supported for nesting, i.e. are allowed in
	 * vmcs12 and propagated to vmcs02 when set in vmcs12.
	 */
	exec_control &amp;= ~(CPU_BASED_RDTSC_EXITING |
			  CPU_BASED_USE_IO_BITMAPS |
			  CPU_BASED_MONITOR_TRAP_FLAG |
			  CPU_BASED_PAUSE_EXITING);

	/* INTR_WINDOW_EXITING and NMI_WINDOW_EXITING are toggled dynamically */
<blue>	exec_control &= ~(CPU_BASED_INTR_WINDOW_EXITING |</blue>
			  CPU_BASED_NMI_WINDOW_EXITING);

<blue>	if (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)</blue>
<yellow>		exec_control &= ~CPU_BASED_MOV_DR_EXITING;</yellow>

<blue>	if (!cpu_need_tpr_shadow(&vmx->vcpu))</blue>
<blue>		exec_control &= ~CPU_BASED_TPR_SHADOW;</blue>

#ifdef CONFIG_X86_64
<blue>	if (exec_control & CPU_BASED_TPR_SHADOW)</blue>
<blue>		exec_control &= ~(CPU_BASED_CR8_LOAD_EXITING |</blue>
				  CPU_BASED_CR8_STORE_EXITING);
	else
<blue>		exec_control |= CPU_BASED_CR8_STORE_EXITING |</blue>
				CPU_BASED_CR8_LOAD_EXITING;
#endif
	/* No need to intercept CR3 access or INVPLG when using EPT. */
<blue>	if (enable_ept)</blue>
<blue>		exec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |</blue>
				  CPU_BASED_CR3_STORE_EXITING |
				  CPU_BASED_INVLPG_EXITING);
<blue>	if (kvm_mwait_in_guest(vmx->vcpu.kvm))</blue>
<yellow>		exec_control &= ~(CPU_BASED_MWAIT_EXITING |</yellow>
				CPU_BASED_MONITOR_EXITING);
<blue>	if (kvm_hlt_in_guest(vmx->vcpu.kvm))</blue>
<blue>		exec_control &= ~CPU_BASED_HLT_EXITING;</blue>
	return exec_control;
}

static u64 vmx_tertiary_exec_control(struct vcpu_vmx *vmx)
{
	u64 exec_control = vmcs_config.cpu_based_3rd_exec_ctrl;

	/*
	 * IPI virtualization relies on APICv. Disable IPI virtualization if
	 * APICv is inhibited.
	 */
<yellow>	if (!enable_ipiv || !kvm_vcpu_apicv_active(&vmx->vcpu))</yellow>
<yellow>		exec_control &= ~TERTIARY_EXEC_IPI_VIRT;</yellow>

	return exec_control;
}

/*
 * Adjust a single secondary execution control bit to intercept/allow an
 * instruction in the guest.  This is usually done based on whether or not a
 * feature has been exposed to the guest in order to correctly emulate faults.
 */
static inline void
<blue>vmx_adjust_secondary_exec_control(struct vcpu_vmx *vmx, u32 *exec_control,</blue>
				  u32 control, bool enabled, bool exiting)
{
	/*
	 * If the control is for an opt-in feature, clear the control if the
	 * feature is not exposed to the guest, i.e. not enabled.  If the
	 * control is opt-out, i.e. an exiting control, clear the control if
	 * the feature _is_ exposed to the guest, i.e. exiting/interception is
	 * disabled for the associated instruction.  Note, the caller is
	 * responsible presetting exec_control to set all supported bits.
	 */
	if (enabled == exiting)
<blue>		*exec_control &= ~control;</blue>

	/*
	 * Update the nested MSR settings so that a nested VMM can/can&#x27;t set
	 * controls for features that are/aren&#x27;t exposed to the guest.
	 */
<blue>	if (nested) {</blue>
		if (enabled)
<blue>			vmx->nested.msrs.secondary_ctls_high |= control;</blue>
		else
<blue>			vmx->nested.msrs.secondary_ctls_high &= ~control;</blue>
	}
}

/*
 * Wrapper macro for the common case of adjusting a secondary execution control
 * based on a single guest CPUID bit, with a dedicated feature bit.  This also
 * verifies that the control is actually supported by KVM and hardware.
 */
#define vmx_adjust_sec_exec_control(vmx, exec_control, name, feat_name, ctrl_name, exiting) \
({									 \
	bool __enabled;							 \
									 \
	if (cpu_has_vmx_##name()) {					 \
		__enabled = guest_cpuid_has(&amp;(vmx)-&gt;vcpu,		 \
					    X86_FEATURE_##feat_name);	 \
		vmx_adjust_secondary_exec_control(vmx, exec_control,	 \
			SECONDARY_EXEC_##ctrl_name, __enabled, exiting); \
	}								 \
})

/* More macro magic for ENABLE_/opt-in versus _EXITING/opt-out controls. */
#define vmx_adjust_sec_exec_feature(vmx, exec_control, lname, uname) \
	vmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, ENABLE_##uname, false)

#define vmx_adjust_sec_exec_exiting(vmx, exec_control, lname, uname) \
	vmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, uname##_EXITING, true)

static u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)
{
	struct kvm_vcpu *vcpu = &amp;vmx-&gt;vcpu;

	u32 exec_control = vmcs_config.cpu_based_2nd_exec_ctrl;

<blue>	if (vmx_pt_mode_is_system())</blue>
<blue>		exec_control &= ~(SECONDARY_EXEC_PT_USE_GPA | SECONDARY_EXEC_PT_CONCEAL_VMX);</blue>
<blue>	if (!cpu_need_virtualize_apic_accesses(vcpu))</blue>
<blue>		exec_control &= ~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;</blue>
<blue>	if (vmx->vpid == 0)</blue>
<yellow>		exec_control &= ~SECONDARY_EXEC_ENABLE_VPID;</yellow>
<blue>	if (!enable_ept) {</blue>
		exec_control &amp;= ~SECONDARY_EXEC_ENABLE_EPT;
<yellow>		enable_unrestricted_guest = 0;</yellow>
	}
<blue>	if (!enable_unrestricted_guest)</blue>
<yellow>		exec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;</yellow>
<blue>	if (kvm_pause_in_guest(vmx->vcpu.kvm))</blue>
<blue>		exec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;</blue>
<blue>	if (!kvm_vcpu_apicv_active(vcpu))</blue>
<blue>		exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |</blue>
				  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
	exec_control &amp;= ~SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;

	/* SECONDARY_EXEC_DESC is enabled/disabled on writes to CR4.UMIP,
	 * in vmx_set_cr4.  */
	exec_control &amp;= ~SECONDARY_EXEC_DESC;

	/* SECONDARY_EXEC_SHADOW_VMCS is enabled when L1 executes VMPTRLD
	   (handle_vmptrld).
	   We can NOT enable shadow_vmcs here because we don&#x27;t have yet
	   a current VMCS12
	*/
<blue>	exec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;</blue>

	/*
	 * PML is enabled/disabled when dirty logging of memsmlots changes, but
	 * it needs to be set here when dirty logging is already active, e.g.
	 * if this vCPU was created after dirty logging was enabled.
	 */
<blue>	if (!vcpu->kvm->arch.cpu_dirty_logging_count)</blue>
<blue>		exec_control &= ~SECONDARY_EXEC_ENABLE_PML;</blue>

<blue>	if (cpu_has_vmx_xsaves()) {</blue>
		/* Exposing XSAVES only when XSAVE is exposed */
		bool xsaves_enabled =
<blue>			boot_cpu_has(X86_FEATURE_XSAVE) &&</blue>
<blue>			guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&</blue>
<blue>			guest_cpuid_has(vcpu, X86_FEATURE_XSAVES);</blue>

<blue>		vcpu->arch.xsaves_enabled = xsaves_enabled;</blue>

<blue>		vmx_adjust_secondary_exec_control(vmx, &exec_control,</blue>
						  SECONDARY_EXEC_XSAVES,
						  xsaves_enabled, false);
	}

	/*
	 * RDPID is also gated by ENABLE_RDTSCP, turn on the control if either
	 * feature is exposed to the guest.  This creates a virtualization hole
	 * if both are supported in hardware but only one is exposed to the
	 * guest, but letting the guest execute RDTSCP or RDPID when either one
	 * is advertised is preferable to emulating the advertised instruction
	 * in KVM on #UD, and obviously better than incorrectly injecting #UD.
	 */
<blue>	if (cpu_has_vmx_rdtscp()) {</blue>
		bool rdpid_or_rdtscp_enabled =
<blue>			guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) ||</blue>
<blue>			guest_cpuid_has(vcpu, X86_FEATURE_RDPID);</blue>

<blue>		vmx_adjust_secondary_exec_control(vmx, &exec_control,</blue>
						  SECONDARY_EXEC_ENABLE_RDTSCP,
						  rdpid_or_rdtscp_enabled, false);
	}
<blue>	vmx_adjust_sec_exec_feature(vmx, &exec_control, invpcid, INVPCID);</blue>

<blue>	vmx_adjust_sec_exec_exiting(vmx, &exec_control, rdrand, RDRAND);</blue>
<blue>	vmx_adjust_sec_exec_exiting(vmx, &exec_control, rdseed, RDSEED);</blue>

<blue>	vmx_adjust_sec_exec_control(vmx, &exec_control, waitpkg, WAITPKG,</blue>
				    ENABLE_USR_WAIT_PAUSE, false);

<blue>	if (!vcpu->kvm->arch.bus_lock_detection_enabled)</blue>
<blue>		exec_control &= ~SECONDARY_EXEC_BUS_LOCK_DETECTION;</blue>

<blue>	if (!kvm_notify_vmexit_enabled(vcpu->kvm))</blue>
<blue>		exec_control &= ~SECONDARY_EXEC_NOTIFY_VM_EXITING;</blue>

	return exec_control;
<blue>}</blue>

static inline int vmx_get_pid_table_order(struct kvm *kvm)
{
<yellow>	return get_order(kvm->arch.max_vcpu_ids * sizeof(*to_kvm_vmx(kvm)->pid_table));</yellow>
}

static int vmx_alloc_ipiv_pid_table(struct kvm *kvm)
{
	struct page *pages;
	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);

<blue>	if (!irqchip_in_kernel(kvm) || !enable_ipiv)</blue>
		return 0;

<yellow>	if (kvm_vmx->pid_table)</yellow>
		return 0;

<yellow>	pages = alloc_pages(GFP_KERNEL | __GFP_ZERO, vmx_get_pid_table_order(kvm));</yellow>
	if (!pages)
		return -ENOMEM;

<yellow>	kvm_vmx->pid_table = (void *)page_address(pages);</yellow>
	return 0;
}

static int vmx_vcpu_precreate(struct kvm *kvm)
{
<blue>	return vmx_alloc_ipiv_pid_table(kvm);</blue>
<blue>}</blue>

#define VMX_XSS_EXIT_BITMAP 0

static void init_vmcs(struct vcpu_vmx *vmx)
{
<blue>	struct kvm *kvm = vmx->vcpu.kvm;</blue>
	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);

<blue>	if (nested)</blue>
<blue>		nested_vmx_set_vmcs_shadowing_bitmap();</blue>

<blue>	if (cpu_has_vmx_msr_bitmap())</blue>
<blue>		vmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));</blue>

<blue>	vmcs_write64(VMCS_LINK_POINTER, INVALID_GPA); /* 22.3.1.5 */</blue>

	/* Control */
<blue>	pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));</blue>

<blue>	exec_controls_set(vmx, vmx_exec_control(vmx));</blue>

<blue>	if (cpu_has_secondary_exec_ctrls())</blue>
<blue>		secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));</blue>

<blue>	if (cpu_has_tertiary_exec_ctrls())</blue>
<yellow>		tertiary_exec_controls_set(vmx, vmx_tertiary_exec_control(vmx));</yellow>

<blue>	if (enable_apicv && lapic_in_kernel(&vmx->vcpu)) {</blue>
<blue>		vmcs_write64(EOI_EXIT_BITMAP0, 0);</blue>
<blue>		vmcs_write64(EOI_EXIT_BITMAP1, 0);</blue>
<blue>		vmcs_write64(EOI_EXIT_BITMAP2, 0);</blue>
<blue>		vmcs_write64(EOI_EXIT_BITMAP3, 0);</blue>

<blue>		vmcs_write16(GUEST_INTR_STATUS, 0);</blue>

<blue>		vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);</blue>
<blue>		vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));</blue>
	}

<blue>	if (vmx_can_use_ipiv(&vmx->vcpu)) {</blue>
<yellow>		vmcs_write64(PID_POINTER_TABLE, __pa(kvm_vmx->pid_table));</yellow>
<yellow>		vmcs_write16(LAST_PID_POINTER_INDEX, kvm->arch.max_vcpu_ids - 1);</yellow>
	}

<blue>	if (!kvm_pause_in_guest(kvm)) {</blue>
<yellow>		vmcs_write32(PLE_GAP, ple_gap);</yellow>
<yellow>		vmx->ple_window = ple_window;</yellow>
		vmx-&gt;ple_window_dirty = true;
	}

<blue>	if (kvm_notify_vmexit_enabled(kvm))</blue>
<yellow>		vmcs_write32(NOTIFY_WINDOW, kvm->arch.notify_window);</yellow>

<blue>	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);</blue>
<blue>	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);</blue>
<blue>	vmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */</blue>

<blue>	vmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */</blue>
<blue>	vmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */</blue>
<blue>	vmx_set_constant_host_state(vmx);</blue>
<blue>	vmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */</blue>
<blue>	vmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */</blue>

<blue>	if (cpu_has_vmx_vmfunc())</blue>
<blue>		vmcs_write64(VM_FUNCTION_CONTROL, 0);</blue>

<blue>	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);</blue>
<blue>	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);</blue>
<blue>	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));</blue>
<blue>	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);</blue>
<blue>	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));</blue>

<blue>	if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)</blue>
<blue>		vmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);</blue>

<blue>	vm_exit_controls_set(vmx, vmx_vmexit_ctrl());</blue>

	/* 22.2.1, 20.8.1 */
<blue>	vm_entry_controls_set(vmx, vmx_vmentry_ctrl());</blue>

<blue>	vmx->vcpu.arch.cr0_guest_owned_bits = KVM_POSSIBLE_CR0_GUEST_BITS;</blue>
<blue>	vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);</blue>

<blue>	set_cr4_guest_host_mask(vmx);</blue>

	if (vmx-&gt;vpid != 0)
<blue>		vmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);</blue>

<blue>	if (cpu_has_vmx_xsaves())</blue>
<blue>		vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);</blue>

<blue>	if (enable_pml) {</blue>
<blue>		vmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));</blue>
<blue>		vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);</blue>
	}

<blue>	vmx_write_encls_bitmap(&vmx->vcpu, NULL);</blue>

	if (vmx_pt_mode_is_host_guest()) {
<yellow>		memset(&vmx->pt_desc, 0, sizeof(vmx->pt_desc));</yellow>
		/* Bit[6~0] are forced to 1, writes are ignored. */
		vmx-&gt;pt_desc.guest.output_mask = 0x7F;
<yellow>		vmcs_write64(GUEST_IA32_RTIT_CTL, 0);</yellow>
	}

<blue>	vmcs_write32(GUEST_SYSENTER_CS, 0);</blue>
<blue>	vmcs_writel(GUEST_SYSENTER_ESP, 0);</blue>
<blue>	vmcs_writel(GUEST_SYSENTER_EIP, 0);</blue>
<blue>	vmcs_write64(GUEST_IA32_DEBUGCTL, 0);</blue>

<blue>	if (cpu_has_vmx_tpr_shadow()) {</blue>
<blue>		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);</blue>
<blue>		if (cpu_need_tpr_shadow(&vmx->vcpu))</blue>
<blue>			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,</blue>
<blue>				     __pa(vmx->vcpu.arch.apic->regs));</blue>
<blue>		vmcs_write32(TPR_THRESHOLD, 0);</blue>
	}

<blue>	vmx_setup_uret_msrs(vmx);</blue>
}

static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	init_vmcs(vmx);</blue>

<blue>	if (nested)</blue>
<blue>		memcpy(&vmx->nested.msrs, &vmcs_config.nested, sizeof(vmx->nested.msrs));</blue>

<blue>	vcpu_setup_sgx_lepubkeyhash(vcpu);</blue>

	vmx-&gt;nested.posted_intr_nv = -1;
	vmx-&gt;nested.vmxon_ptr = INVALID_GPA;
	vmx-&gt;nested.current_vmptr = INVALID_GPA;
	vmx-&gt;nested.hv_evmcs_vmptr = EVMPTR_INVALID;

	vcpu-&gt;arch.microcode_version = 0x100000000ULL;
	vmx-&gt;msr_ia32_feature_control_valid_bits = FEAT_CTL_LOCKED;

	/*
	 * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR
	 * or POSTED_INTR_WAKEUP_VECTOR.
	 */
	vmx-&gt;pi_desc.nv = POSTED_INTR_VECTOR;
	vmx-&gt;pi_desc.sn = 1;
}

static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
<blue>{</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (!init_event)</blue>
<blue>		__vmx_vcpu_reset(vcpu);</blue>

<blue>	vmx->rmode.vm86_active = 0;</blue>
	vmx-&gt;spec_ctrl = 0;

	vmx-&gt;msr_ia32_umwait_control = 0;

	vmx-&gt;hv_deadline_tsc = -1;
	kvm_set_cr8(vcpu, 0);

	vmx_segment_cache_clear(vmx);
	kvm_register_mark_available(vcpu, VCPU_EXREG_SEGMENTS);

	seg_setup(VCPU_SREG_CS);
<blue>	vmcs_write16(GUEST_CS_SELECTOR, 0xf000);</blue>
<blue>	vmcs_writel(GUEST_CS_BASE, 0xffff0000ul);</blue>

<blue>	seg_setup(VCPU_SREG_DS);</blue>
	seg_setup(VCPU_SREG_ES);
	seg_setup(VCPU_SREG_FS);
	seg_setup(VCPU_SREG_GS);
	seg_setup(VCPU_SREG_SS);

<blue>	vmcs_write16(GUEST_TR_SELECTOR, 0);</blue>
<blue>	vmcs_writel(GUEST_TR_BASE, 0);</blue>
<blue>	vmcs_write32(GUEST_TR_LIMIT, 0xffff);</blue>
<blue>	vmcs_write32(GUEST_TR_AR_BYTES, 0x008b);</blue>

<blue>	vmcs_write16(GUEST_LDTR_SELECTOR, 0);</blue>
<blue>	vmcs_writel(GUEST_LDTR_BASE, 0);</blue>
<blue>	vmcs_write32(GUEST_LDTR_LIMIT, 0xffff);</blue>
<blue>	vmcs_write32(GUEST_LDTR_AR_BYTES, 0x00082);</blue>

<blue>	vmcs_writel(GUEST_GDTR_BASE, 0);</blue>
<blue>	vmcs_write32(GUEST_GDTR_LIMIT, 0xffff);</blue>

<blue>	vmcs_writel(GUEST_IDTR_BASE, 0);</blue>
<blue>	vmcs_write32(GUEST_IDTR_LIMIT, 0xffff);</blue>

<blue>	vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);</blue>
<blue>	vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, 0);</blue>
<blue>	vmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, 0);</blue>
<blue>	if (kvm_mpx_supported())</blue>
<yellow>		vmcs_write64(GUEST_BNDCFGS, 0);</yellow>

<blue>	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */</blue>

<blue>	kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);</blue>

<blue>	vpid_sync_context(vmx->vpid);</blue>

<blue>	vmx_update_fb_clear_dis(vcpu, vmx);</blue>
}

static void vmx_enable_irq_window(struct kvm_vcpu *vcpu)
{
<blue>	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);</blue>
<blue>}</blue>

static void vmx_enable_nmi_window(struct kvm_vcpu *vcpu)
{
<blue>	if (!enable_vnmi ||</blue>
<blue>	    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {</blue>
<blue>		vmx_enable_irq_window(vcpu);</blue>
		return;
	}

<blue>	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);</blue>
<blue>}</blue>

static void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	uint32_t intr;
<blue>	int irq = vcpu->arch.interrupt.nr;</blue>

<blue>	trace_kvm_inj_virq(irq, vcpu->arch.interrupt.soft, reinjected);</blue>

<blue>	++vcpu->stat.irq_injections;</blue>
	if (vmx-&gt;rmode.vm86_active) {
		int inc_eip = 0;
<yellow>		if (vcpu->arch.interrupt.soft)</yellow>
<yellow>			inc_eip = vcpu->arch.event_exit_inst_len;</yellow>
<yellow>		kvm_inject_realmode_interrupt(vcpu, irq, inc_eip);</yellow>
		return;
	}
<blue>	intr = irq | INTR_INFO_VALID_MASK;</blue>
<blue>	if (vcpu->arch.interrupt.soft) {</blue>
		intr |= INTR_TYPE_SOFT_INTR;
<blue>		vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,</blue>
<blue>			     vmx->vcpu.arch.event_exit_inst_len);</blue>
	} else
		intr |= INTR_TYPE_EXT_INTR;
<blue>	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);</blue>

<blue>	vmx_clear_hlt(vcpu);</blue>
<blue>}</blue>

static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (!enable_vnmi) {</blue>
		/*
		 * Tracking the NMI-blocked state in software is built upon
		 * finding the next open IRQ window. This, in turn, depends on
		 * well-behaving guests: They have to keep IRQs disabled at
		 * least as long as the NMI handler runs. Otherwise we may
		 * cause NMI nesting, maybe breaking the guest. But as this is
		 * highly unlikely, we can live with the residual risk.
		 */
<yellow>		vmx->loaded_vmcs->soft_vnmi_blocked = 1;</yellow>
		vmx-&gt;loaded_vmcs-&gt;vnmi_blocked_time = 0;
	}

<blue>	++vcpu->stat.nmi_injections;</blue>
	vmx-&gt;loaded_vmcs-&gt;nmi_known_unmasked = false;

	if (vmx-&gt;rmode.vm86_active) {
<yellow>		kvm_inject_realmode_interrupt(vcpu, NMI_VECTOR, 0);</yellow>
		return;
	}

<blue>	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,</blue>
			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);

<blue>	vmx_clear_hlt(vcpu);</blue>
<blue>}</blue>

<blue>bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	bool masked;

<blue>	if (!enable_vnmi)</blue>
<blue>		return vmx->loaded_vmcs->soft_vnmi_blocked;</blue>
<blue>	if (vmx->loaded_vmcs->nmi_known_unmasked)</blue>
		return false;
<blue>	masked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;</blue>
<blue>	vmx->loaded_vmcs->nmi_known_unmasked = !masked;</blue>
	return masked;
<blue>}</blue>

void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (!enable_vnmi) {</blue>
<yellow>		if (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {</yellow>
<yellow>			vmx->loaded_vmcs->soft_vnmi_blocked = masked;</yellow>
			vmx-&gt;loaded_vmcs-&gt;vnmi_blocked_time = 0;
		}
	} else {
<blue>		vmx->loaded_vmcs->nmi_known_unmasked = !masked;</blue>
		if (masked)
<blue>			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,</blue>
				      GUEST_INTR_STATE_NMI);
		else
<blue>			vmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO,</blue>
					GUEST_INTR_STATE_NMI);
	}
<blue>}</blue>

<blue>bool vmx_nmi_blocked(struct kvm_vcpu *vcpu)</blue>
{
<blue>	if (is_guest_mode(vcpu) && nested_exit_on_nmi(vcpu))</blue>
		return false;

<blue>	if (!enable_vnmi && to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)</blue>
		return true;

<blue>	return (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &</blue>
		(GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI |
		 GUEST_INTR_STATE_NMI));
<blue>}</blue>

static int vmx_nmi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
{
<blue>	if (to_vmx(vcpu)->nested.nested_run_pending)</blue>
		return -EBUSY;

	/* An NMI must not be injected into L2 if it&#x27;s supposed to VM-Exit.  */
<blue>	if (for_injection && is_guest_mode(vcpu) && nested_exit_on_nmi(vcpu))</blue>
		return -EBUSY;

<blue>	return !vmx_nmi_blocked(vcpu);</blue>
<blue>}</blue>

bool vmx_interrupt_blocked(struct kvm_vcpu *vcpu)
{
<blue>	if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))</blue>
		return false;

<blue>	return !(vmx_get_rflags(vcpu) & X86_EFLAGS_IF) ||</blue>
<blue>	       (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &</blue>
		(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
<blue>}</blue>

static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
{
<blue>	if (to_vmx(vcpu)->nested.nested_run_pending)</blue>
		return -EBUSY;

       /*
        * An IRQ must not be injected into L2 if it&#x27;s supposed to VM-Exit,
        * e.g. if the IRQ arrived asynchronously after checking nested events.
        */
<blue>	if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))</blue>
		return -EBUSY;

<blue>	return !vmx_interrupt_blocked(vcpu);</blue>
<blue>}</blue>

<yellow>static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)</yellow>
<blue>{</blue>
	void __user *ret;

<blue>	if (enable_unrestricted_guest)</blue>
		return 0;

<yellow>	mutex_lock(&kvm->slots_lock);</yellow>
	ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
				      PAGE_SIZE * 3);
	mutex_unlock(&amp;kvm-&gt;slots_lock);

	if (IS_ERR(ret))
<yellow>		return PTR_ERR(ret);</yellow>

<yellow>	to_kvm_vmx(kvm)->tss_addr = addr;</yellow>

<yellow>	return init_rmode_tss(kvm, ret);</yellow>
}

static int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
{
<blue>	to_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;</blue>
	return 0;
}

static bool rmode_exception(struct kvm_vcpu *vcpu, int vec)
{
<yellow>	switch (vec) {</yellow>
	case BP_VECTOR:
		/*
		 * Update instruction length as we may reinject the exception
		 * from user space while in guest debugging mode.
		 */
<yellow>		to_vmx(vcpu)->vcpu.arch.event_exit_inst_len =</yellow>
<yellow>			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);</yellow>
		if (vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_USE_SW_BP)
			return false;
		fallthrough;
	case DB_VECTOR:
<yellow>		return !(vcpu->guest_debug &</yellow>
			(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP));
	case DE_VECTOR:
	case OF_VECTOR:
	case BR_VECTOR:
	case UD_VECTOR:
	case DF_VECTOR:
	case SS_VECTOR:
	case GP_VECTOR:
	case MF_VECTOR:
		return true;
	}
	return false;
}

static int handle_rmode_exception(struct kvm_vcpu *vcpu,
				  int vec, u32 err_code)
{
	/*
	 * Instruction with address size override prefix opcode 0x67
	 * Cause the #SS fault with 0 error code in VM86 mode.
	 */
<yellow>	if (((vec == GP_VECTOR) || (vec == SS_VECTOR)) && err_code == 0) {</yellow>
<yellow>		if (kvm_emulate_instruction(vcpu, 0)) {</yellow>
<yellow>			if (vcpu->arch.halt_request) {</yellow>
				vcpu-&gt;arch.halt_request = 0;
<yellow>				return kvm_emulate_halt_noskip(vcpu);</yellow>
			}
			return 1;
		}
		return 0;
	}

	/*
	 * Forward all other exceptions that are valid in real mode.
	 * FIXME: Breaks guest debugging in real mode, needs to be fixed with
	 *        the required debugging infrastructure rework.
	 */
<yellow>	kvm_queue_exception(vcpu, vec);</yellow>
	return 1;
}

static int handle_machine_check(struct kvm_vcpu *vcpu)
{
	/* handled by vmx_vcpu_run() */
	return 1;
<yellow>}</yellow>

/*
 * If the host has split lock detection disabled, then #AC is
 * unconditionally injected into the guest, which is the pre split lock
 * detection behaviour.
 *
 * If the host has split lock detection enabled then #AC is
 * only injected into the guest when:
 *  - Guest CPL == 3 (user mode)
 *  - Guest has #AC detection enabled in CR0
 *  - Guest EFLAGS has AC bit set
 */
<yellow>bool vmx_guest_inject_ac(struct kvm_vcpu *vcpu)</yellow>
<yellow>{</yellow>
<yellow>	if (!boot_cpu_has(X86_FEATURE_SPLIT_LOCK_DETECT))</yellow>
		return true;

<yellow>	return vmx_get_cpl(vcpu) == 3 && kvm_read_cr0_bits(vcpu, X86_CR0_AM) &&</yellow>
<yellow>	       (kvm_get_rflags(vcpu) & X86_EFLAGS_AC);</yellow>
<yellow>}</yellow>

static int handle_exception_nmi(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<blue>	struct kvm_run *kvm_run = vcpu->run;</blue>
	u32 intr_info, ex_no, error_code;
	unsigned long cr2, dr6;
	u32 vect_info;

	vect_info = vmx-&gt;idt_vectoring_info;
	intr_info = vmx_get_intr_info(vcpu);

<blue>	if (is_machine_check(intr_info) || is_nmi(intr_info))</blue>
		return 1; /* handled by handle_exception_nmi_irqoff() */

	/*
	 * Queue the exception here instead of in handle_nm_fault_irqoff().
	 * This ensures the nested_vmx check is not skipped so vmexit can
	 * be reflected to L1 (when it intercepts #NM) before reaching this
	 * point.
	 */
<blue>	if (is_nm_fault(intr_info)) {</blue>
<yellow>		kvm_queue_exception(vcpu, NM_VECTOR);</yellow>
		return 1;
	}

<blue>	if (is_invalid_opcode(intr_info))</blue>
<blue>		return handle_ud(vcpu);</blue>

	error_code = 0;
<blue>	if (intr_info & INTR_INFO_DELIVER_CODE_MASK)</blue>
<yellow>		error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);</yellow>

<blue>	if (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {</blue>
<yellow>		WARN_ON_ONCE(!enable_vmware_backdoor);</yellow>

		/*
		 * VMware backdoor emulation on #GP interception only handles
		 * IN{S}, OUT{S}, and RDPMC, none of which generate a non-zero
		 * error code on #GP.
		 */
<yellow>		if (error_code) {</yellow>
<yellow>			kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);</yellow>
			return 1;
		}
<yellow>		return kvm_emulate_instruction(vcpu, EMULTYPE_VMWARE_GP);</yellow>
	}

	/*
	 * The #PF with PFEC.RSVD = 1 indicates the guest is accessing
	 * MMIO, it is better to report an internal error.
	 * See the comments in vmx_handle_exit.
	 */
<blue>	if ((vect_info & VECTORING_INFO_VALID_MASK) &&</blue>
<yellow>	    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {</yellow>
<yellow>		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;</yellow>
		vcpu-&gt;run-&gt;internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;
		vcpu-&gt;run-&gt;internal.ndata = 4;
		vcpu-&gt;run-&gt;internal.data[0] = vect_info;
		vcpu-&gt;run-&gt;internal.data[1] = intr_info;
		vcpu-&gt;run-&gt;internal.data[2] = error_code;
		vcpu-&gt;run-&gt;internal.data[3] = vcpu-&gt;arch.last_vmentry_cpu;
		return 0;
	}

<blue>	if (is_page_fault(intr_info)) {</blue>
<yellow>		cr2 = vmx_get_exit_qual(vcpu);</yellow>
<yellow>		if (enable_ept && !vcpu->arch.apf.host_apf_flags) {</yellow>
			/*
			 * EPT will cause page fault only if we need to
			 * detect illegal GPAs.
			 */
<yellow>			WARN_ON_ONCE(!allow_smaller_maxphyaddr);</yellow>
<yellow>			kvm_fixup_and_inject_pf_error(vcpu, cr2, error_code);</yellow>
			return 1;
		} else
<yellow>			return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);</yellow>
	}

<blue>	ex_no = intr_info & INTR_INFO_VECTOR_MASK;</blue>

<yellow>	if (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))</yellow>
<yellow>		return handle_rmode_exception(vcpu, ex_no, error_code);</yellow>

<blue>	switch (ex_no) {</blue>
	case DB_VECTOR:
<blue>		dr6 = vmx_get_exit_qual(vcpu);</blue>
		if (!(vcpu-&gt;guest_debug &amp;
		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
			/*
			 * If the #DB was due to ICEBP, a.k.a. INT1, skip the
			 * instruction.  ICEBP generates a trap-like #DB, but
			 * despite its interception control being tied to #DB,
			 * is an instruction intercept, i.e. the VM-Exit occurs
			 * on the ICEBP itself.  Use the inner &quot;skip&quot; helper to
			 * avoid single-step #DB and MTF updates, as ICEBP is
			 * higher priority.  Note, skipping ICEBP still clears
			 * STI and MOVSS blocking.
			 *
			 * For all other #DBs, set vmcs.PENDING_DBG_EXCEPTIONS.BS
			 * if single-step is enabled in RFLAGS and STI or MOVSS
			 * blocking is active, as the CPU doesn&#x27;t set the bit
			 * on VM-Exit due to #DB interception.  VM-Entry has a
			 * consistency check that a single-step #DB is pending
			 * in this scenario as the previous instruction cannot
			 * have toggled RFLAGS.TF 0=&gt;1 (because STI and POP/MOV
			 * don&#x27;t modify RFLAGS), therefore the one instruction
			 * delay when activating single-step breakpoints must
			 * have already expired.  Note, the CPU sets/clears BS
			 * as appropriate for all other VM-Exits types.
			 */
<blue>			if (is_icebp(intr_info))</blue>
<blue>				WARN_ON(!skip_emulated_instruction(vcpu));</blue>
<blue>			else if ((vmx_get_rflags(vcpu) & X86_EFLAGS_TF) &&</blue>
<blue>				 (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &</blue>
				  (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)))
<yellow>				vmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS,</yellow>
<yellow>					    vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS) | DR6_BS);</yellow>

<blue>			kvm_queue_exception_p(vcpu, DB_VECTOR, dr6);</blue>
			return 1;
		}
<blue>		kvm_run->debug.arch.dr6 = dr6 | DR6_ACTIVE_LOW;</blue>
<blue>		kvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);</blue>
		fallthrough;
	case BP_VECTOR:
		/*
		 * Update instruction length as we may reinject #BP from
		 * user space while in guest debugging mode. Reading it for
		 * #DB as well causes no harm, it is not used in that case.
		 */
<blue>		vmx->vcpu.arch.event_exit_inst_len =</blue>
<blue>			vmcs_read32(VM_EXIT_INSTRUCTION_LEN);</blue>
		kvm_run-&gt;exit_reason = KVM_EXIT_DEBUG;
		kvm_run-&gt;debug.arch.pc = kvm_get_linear_rip(vcpu);
		kvm_run-&gt;debug.arch.exception = ex_no;
		break;
	case AC_VECTOR:
<yellow>		if (vmx_guest_inject_ac(vcpu)) {</yellow>
<yellow>			kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);</yellow>
			return 1;
		}

		/*
		 * Handle split lock. Depending on detection mode this will
		 * either warn and disable split lock detection for this
		 * task or force SIGBUS on it.
		 */
<yellow>		if (handle_guest_split_lock(kvm_rip_read(vcpu)))</yellow>
			return 1;
		fallthrough;
	default:
<yellow>		kvm_run->exit_reason = KVM_EXIT_EXCEPTION;</yellow>
		kvm_run-&gt;ex.exception = ex_no;
		kvm_run-&gt;ex.error_code = error_code;
		break;
	}
	return 0;
<blue>}</blue>

static __always_inline int handle_external_interrupt(struct kvm_vcpu *vcpu)
{
<blue>	++vcpu->stat.irq_exits;</blue>
	return 1;
}

static int handle_triple_fault(struct kvm_vcpu *vcpu)
{
<blue>	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;</blue>
	vcpu-&gt;mmio_needed = 0;
	return 0;
}

<blue>static int handle_io(struct kvm_vcpu *vcpu)</blue>
{
	unsigned long exit_qualification;
	int size, in, string;
	unsigned port;

<blue>	exit_qualification = vmx_get_exit_qual(vcpu);</blue>
	string = (exit_qualification &amp; 16) != 0;

	++vcpu-&gt;stat.io_exits;

	if (string)
<blue>		return kvm_emulate_instruction(vcpu, 0);</blue>

	port = exit_qualification &gt;&gt; 16;
	size = (exit_qualification &amp; 7) + 1;
<blue>	in = (exit_qualification & 8) != 0;</blue>

	return kvm_fast_pio(vcpu, size, port, in);
<blue>}</blue>

static void
vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
{
	/*
	 * Patch in the VMCALL instruction:
	 */
<blue>	hypercall[0] = 0x0f;</blue>
	hypercall[1] = 0x01;
	hypercall[2] = 0xc1;
}

/* called to set cr0 as appropriate for a mov-to-cr0 exit. */
static int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)
{
<blue>	if (is_guest_mode(vcpu)) {</blue>
<yellow>		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</yellow>
		unsigned long orig_val = val;

		/*
		 * We get here when L2 changed cr0 in a way that did not change
		 * any of L1&#x27;s shadowed bits (see nested_vmx_exit_handled_cr),
		 * but did change L0 shadowed bits. So we first calculate the
		 * effective cr0 value that L1 would like to write into the
		 * hardware. It consists of the L2-owned bits from the new
		 * value combined with the L1-owned bits from L1&#x27;s guest_cr0.
		 */
		val = (val &amp; ~vmcs12-&gt;cr0_guest_host_mask) |
			(vmcs12-&gt;guest_cr0 &amp; vmcs12-&gt;cr0_guest_host_mask);

<yellow>		if (!nested_guest_cr0_valid(vcpu, val))</yellow>
			return 1;

<yellow>		if (kvm_set_cr0(vcpu, val))</yellow>
			return 1;
<yellow>		vmcs_writel(CR0_READ_SHADOW, orig_val);</yellow>
		return 0;
	} else {
<blue>		if (to_vmx(vcpu)->nested.vmxon &&</blue>
<blue>		    !nested_host_cr0_valid(vcpu, val))</blue>
			return 1;

<blue>		return kvm_set_cr0(vcpu, val);</blue>
	}
}

static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
{
<blue>	if (is_guest_mode(vcpu)) {</blue>
<yellow>		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</yellow>
		unsigned long orig_val = val;

		/* analogously to handle_set_cr0 */
		val = (val &amp; ~vmcs12-&gt;cr4_guest_host_mask) |
			(vmcs12-&gt;guest_cr4 &amp; vmcs12-&gt;cr4_guest_host_mask);
		if (kvm_set_cr4(vcpu, val))
			return 1;
<yellow>		vmcs_writel(CR4_READ_SHADOW, orig_val);</yellow>
		return 0;
	} else
<blue>		return kvm_set_cr4(vcpu, val);</blue>
}

static int handle_desc(struct kvm_vcpu *vcpu)
{
<yellow>	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));</yellow>
<yellow>	return kvm_emulate_instruction(vcpu, 0);</yellow>
}

static int handle_cr(struct kvm_vcpu *vcpu)
<blue>{</blue>
	unsigned long exit_qualification, val;
	int cr;
	int reg;
	int err;
	int ret;

<blue>	exit_qualification = vmx_get_exit_qual(vcpu);</blue>
	cr = exit_qualification &amp; 15;
	reg = (exit_qualification &gt;&gt; 8) &amp; 15;
	switch ((exit_qualification &gt;&gt; 4) &amp; 3) {
	case 0: /* mov to cr */
<blue>		val = kvm_register_read(vcpu, reg);</blue>
<blue>		trace_kvm_cr_write(cr, val);</blue>
<blue>		switch (cr) {</blue>
		case 0:
<blue>			err = handle_set_cr0(vcpu, val);</blue>
			return kvm_complete_insn_gp(vcpu, err);
		case 3:
<yellow>			WARN_ON_ONCE(enable_unrestricted_guest);</yellow>

<yellow>			err = kvm_set_cr3(vcpu, val);</yellow>
			return kvm_complete_insn_gp(vcpu, err);
		case 4:
<blue>			err = handle_set_cr4(vcpu, val);</blue>
<blue>			return kvm_complete_insn_gp(vcpu, err);</blue>
		case 8: {
<blue>				u8 cr8_prev = kvm_get_cr8(vcpu);</blue>
<blue>				u8 cr8 = (u8)val;</blue>
				err = kvm_set_cr8(vcpu, cr8);
				ret = kvm_complete_insn_gp(vcpu, err);
<blue>				if (lapic_in_kernel(vcpu))</blue>
					return ret;
				if (cr8_prev &lt;= cr8)
					return ret;
				/*
				 * TODO: we might be squashing a
				 * KVM_GUESTDBG_SINGLESTEP-triggered
				 * KVM_EXIT_DEBUG here.
				 */
<blue>				vcpu->run->exit_reason = KVM_EXIT_SET_TPR;</blue>
				return 0;
			}
		}
		break;
	case 2: /* clts */
<yellow>		KVM_BUG(1, vcpu->kvm, "Guest always owns CR0.TS");</yellow>
		return -EIO;
	case 1: /*mov from cr*/
<blue>		switch (cr) {</blue>
		case 3:
<yellow>			WARN_ON_ONCE(enable_unrestricted_guest);</yellow>

<yellow>			val = kvm_read_cr3(vcpu);</yellow>
			kvm_register_write(vcpu, reg, val);
<yellow>			trace_kvm_cr_read(cr, val);</yellow>
			return kvm_skip_emulated_instruction(vcpu);
		case 8:
<blue>			val = kvm_get_cr8(vcpu);</blue>
			kvm_register_write(vcpu, reg, val);
<yellow>			trace_kvm_cr_read(cr, val);</yellow>
<blue>			return kvm_skip_emulated_instruction(vcpu);</blue>
		}
		break;
	case 3: /* lmsw */
<blue>		val = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;</blue>
<blue>		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);</blue>
<blue>		kvm_lmsw(vcpu, val);</blue>

		return kvm_skip_emulated_instruction(vcpu);
	default:
		break;
	}
<yellow>	vcpu->run->exit_reason = 0;</yellow>
<yellow>	vcpu_unimpl(vcpu, "unhandled control register: op %d cr %d\n",</yellow>
	       (int)(exit_qualification &gt;&gt; 4) &amp; 3, cr);
	return 0;
}

static int handle_dr(struct kvm_vcpu *vcpu)
<blue>{</blue>
	unsigned long exit_qualification;
	int dr, dr7, reg;
	int err = 1;

<blue>	exit_qualification = vmx_get_exit_qual(vcpu);</blue>
	dr = exit_qualification &amp; DEBUG_REG_ACCESS_NUM;

	/* First, if DR does not exist, trigger UD */
	if (!kvm_require_dr(vcpu, dr))
		return 1;

<blue>	if (vmx_get_cpl(vcpu) > 0)</blue>
		goto out;

<blue>	dr7 = vmcs_readl(GUEST_DR7);</blue>
<blue>	if (dr7 & DR7_GD) {</blue>
		/*
		 * As the vm-exit takes precedence over the debug trap, we
		 * need to emulate the latter, either for the host or the
		 * guest debugging itself.
		 */
<blue>		if (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {</blue>
<blue>			vcpu->run->debug.arch.dr6 = DR6_BD | DR6_ACTIVE_LOW;</blue>
			vcpu-&gt;run-&gt;debug.arch.dr7 = dr7;
			vcpu-&gt;run-&gt;debug.arch.pc = kvm_get_linear_rip(vcpu);
			vcpu-&gt;run-&gt;debug.arch.exception = DB_VECTOR;
			vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_DEBUG;
			return 0;
		} else {
<blue>			kvm_queue_exception_p(vcpu, DB_VECTOR, DR6_BD);</blue>
			return 1;
		}
	}

<blue>	if (vcpu->guest_debug == 0) {</blue>
<blue>		exec_controls_clearbit(to_vmx(vcpu), CPU_BASED_MOV_DR_EXITING);</blue>

		/*
		 * No more DR vmexits; force a reload of the debug registers
		 * and reenter on this instruction.  The next vmexit will
		 * retrieve the full state of the debug registers.
		 */
<blue>		vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;</blue>
		return 1;
	}

<blue>	reg = DEBUG_REG_ACCESS_REG(exit_qualification);</blue>
	if (exit_qualification &amp; TYPE_MOV_FROM_DR) {
		unsigned long val;

<blue>		kvm_get_dr(vcpu, dr, &val);</blue>
		kvm_register_write(vcpu, reg, val);
		err = 0;
	} else {
<blue>		err = kvm_set_dr(vcpu, dr, kvm_register_read(vcpu, reg));</blue>
	}

out:
<blue>	return kvm_complete_insn_gp(vcpu, err);</blue>
}

static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
{
<blue>	get_debugreg(vcpu->arch.db[0], 0);</blue>
	get_debugreg(vcpu-&gt;arch.db[1], 1);
	get_debugreg(vcpu-&gt;arch.db[2], 2);
	get_debugreg(vcpu-&gt;arch.db[3], 3);
	get_debugreg(vcpu-&gt;arch.dr6, 6);
<blue>	vcpu->arch.dr7 = vmcs_readl(GUEST_DR7);</blue>

	vcpu-&gt;arch.switch_db_regs &amp;= ~KVM_DEBUGREG_WONT_EXIT;
<blue>	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_MOV_DR_EXITING);</blue>

	/*
	 * exc_debug expects dr6 to be cleared after it runs, avoid that it sees
	 * a stale dr6 from the guest.
	 */
<blue>	set_debugreg(DR6_RESERVED, 6);</blue>
}

static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
{
<blue>	vmcs_writel(GUEST_DR7, val);</blue>
<blue>}</blue>

static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
{
<yellow>	kvm_apic_update_ppr(vcpu);</yellow>
	return 1;
}

static int handle_interrupt_window(struct kvm_vcpu *vcpu)
{
<yellow>	exec_controls_clearbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);</yellow>

<yellow>	kvm_make_request(KVM_REQ_EVENT, vcpu);</yellow>

	++vcpu-&gt;stat.irq_window_exits;
	return 1;
}

static int handle_invlpg(struct kvm_vcpu *vcpu)
{
<yellow>	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);</yellow>

	kvm_mmu_invlpg(vcpu, exit_qualification);
	return kvm_skip_emulated_instruction(vcpu);
}

static int handle_apic_access(struct kvm_vcpu *vcpu)
{
<yellow>	if (likely(fasteoi)) {</yellow>
<yellow>		unsigned long exit_qualification = vmx_get_exit_qual(vcpu);</yellow>
		int access_type, offset;

		access_type = exit_qualification &amp; APIC_ACCESS_TYPE;
<yellow>		offset = exit_qualification & APIC_ACCESS_OFFSET;</yellow>
		/*
		 * Sane guest uses MOV to write EOI, with written value
		 * not cared. So make a short-circuit here by avoiding
		 * heavy instruction emulation.
		 */
		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &amp;&amp;
		    (offset == APIC_EOI)) {
<yellow>			kvm_lapic_set_eoi(vcpu);</yellow>
			return kvm_skip_emulated_instruction(vcpu);
		}
	}
<yellow>	return kvm_emulate_instruction(vcpu, 0);</yellow>
<yellow>}</yellow>

static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
{
<yellow>	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);</yellow>
	int vector = exit_qualification &amp; 0xff;

	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
	kvm_apic_set_eoi_accelerated(vcpu, vector);
	return 1;
}

static int handle_apic_write(struct kvm_vcpu *vcpu)
{
<yellow>	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);</yellow>

	/*
	 * APIC-write VM-Exit is trap-like, KVM doesn&#x27;t need to advance RIP and
	 * hardware has done any necessary aliasing, offset adjustments, etc...
	 * for the access.  I.e. the correct value has already been  written to
	 * the vAPIC page for the correct 16-byte chunk.  KVM needs only to
	 * retrieve the register value and emulate the access.
	 */
	u32 offset = exit_qualification &amp; 0xff0;

	kvm_apic_write_nodecode(vcpu, offset);
	return 1;
}

static int handle_task_switch(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	unsigned long exit_qualification;
	bool has_error_code = false;
	u32 error_code = 0;
	u16 tss_selector;
	int reason, type, idt_v, idt_index;

<blue>	idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);</blue>
<yellow>	idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);</yellow>
	type = (vmx-&gt;idt_vectoring_info &amp; VECTORING_INFO_TYPE_MASK);

	exit_qualification = vmx_get_exit_qual(vcpu);

	reason = (u32)exit_qualification &gt;&gt; 30;
<yellow>	if (reason == TASK_SWITCH_GATE && idt_v) {</yellow>
<yellow>		switch (type) {</yellow>
		case INTR_TYPE_NMI_INTR:
<yellow>			vcpu->arch.nmi_injected = false;</yellow>
<yellow>			vmx_set_nmi_mask(vcpu, true);</yellow>
			break;
		case INTR_TYPE_EXT_INTR:
		case INTR_TYPE_SOFT_INTR:
<yellow>			kvm_clear_interrupt_queue(vcpu);</yellow>
			break;
		case INTR_TYPE_HARD_EXCEPTION:
<yellow>			if (vmx->idt_vectoring_info &</yellow>
			    VECTORING_INFO_DELIVER_CODE_MASK) {
				has_error_code = true;
				error_code =
<yellow>					vmcs_read32(IDT_VECTORING_ERROR_CODE);</yellow>
			}
			fallthrough;
		case INTR_TYPE_SOFT_EXCEPTION:
<yellow>			kvm_clear_exception_queue(vcpu);</yellow>
			break;
		default:
			break;
		}
	}
	tss_selector = exit_qualification;

<blue>	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&</blue>
		       type != INTR_TYPE_EXT_INTR &amp;&amp;
		       type != INTR_TYPE_NMI_INTR))
<blue>		WARN_ON(!skip_emulated_instruction(vcpu));</blue>

	/*
	 * TODO: What about debug traps on tss switch?
	 *       Are we supposed to inject them and update dr6?
	 */
<blue>	return kvm_task_switch(vcpu, tss_selector,</blue>
			       type == INTR_TYPE_SOFT_INTR ? idt_index : -1,
			       reason, has_error_code, error_code);
}

static int handle_ept_violation(struct kvm_vcpu *vcpu)
{
	unsigned long exit_qualification;
	gpa_t gpa;
	u64 error_code;

<blue>	exit_qualification = vmx_get_exit_qual(vcpu);</blue>

	/*
	 * EPT violation happened while executing iret from NMI,
	 * &quot;blocked by NMI&quot; bit has to be set before next VM entry.
	 * There are errata that may cause this bit to not be set:
	 * AAK134, BY25.
	 */
<blue>	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&</blue>
			enable_vnmi &amp;&amp;
<blue>			(exit_qualification & INTR_INFO_UNBLOCK_NMI))</blue>
<yellow>		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);</yellow>

<blue>	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);</blue>
<blue>	trace_kvm_page_fault(vcpu, gpa, exit_qualification);</blue>

	/* Is it a read fault? */
	error_code = (exit_qualification &amp; EPT_VIOLATION_ACC_READ)
		     ? PFERR_USER_MASK : 0;
	/* Is it a write fault? */
	error_code |= (exit_qualification &amp; EPT_VIOLATION_ACC_WRITE)
		      ? PFERR_WRITE_MASK : 0;
	/* Is it a fetch fault? */
	error_code |= (exit_qualification &amp; EPT_VIOLATION_ACC_INSTR)
		      ? PFERR_FETCH_MASK : 0;
	/* ept page table entry is present? */
<blue>	error_code |= (exit_qualification & EPT_VIOLATION_RWX_MASK)</blue>
		      ? PFERR_PRESENT_MASK : 0;

	error_code |= (exit_qualification &amp; EPT_VIOLATION_GVA_TRANSLATED) != 0 ?
	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;

<blue>	vcpu->arch.exit_qualification = exit_qualification;</blue>

	/*
	 * Check that the GPA doesn&#x27;t exceed physical memory limits, as that is
	 * a guest page fault.  We have to emulate the instruction here, because
	 * if the illegal address is that of a paging structure, then
	 * EPT_VIOLATION_ACC_WRITE bit is set.  Alternatively, if supported we
	 * would also use advanced VM-exit information for EPT violations to
	 * reconstruct the page fault error code.
	 */
<blue>	if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))</blue>
<yellow>		return kvm_emulate_instruction(vcpu, 0);</yellow>

<blue>	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);</blue>
<blue>}</blue>

static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
{
	gpa_t gpa;

<blue>	if (!vmx_can_emulate_instruction(vcpu, EMULTYPE_PF, NULL, 0))</blue>
		return 1;

	/*
	 * A nested guest cannot optimize MMIO vmexits, because we have an
	 * nGPA here instead of the required GPA.
	 */
<blue>	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);</blue>
<blue>	if (!is_guest_mode(vcpu) &&</blue>
<blue>	    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {</blue>
<yellow>		trace_kvm_fast_mmio(gpa);</yellow>
<yellow>		return kvm_skip_emulated_instruction(vcpu);</yellow>
	}

<blue>	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);</blue>
<blue>}</blue>

static int handle_nmi_window(struct kvm_vcpu *vcpu)
{
<yellow>	if (KVM_BUG_ON(!enable_vnmi, vcpu->kvm))</yellow>
		return -EIO;

<yellow>	exec_controls_clearbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);</yellow>
<yellow>	++vcpu->stat.nmi_window_exits;</yellow>
	kvm_make_request(KVM_REQ_EVENT, vcpu);

	return 1;
<yellow>}</yellow>

static bool vmx_emulation_required_with_pending_exception(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	return vmx->emulation_required && !vmx->rmode.vm86_active &&</blue>
<blue>	       (kvm_is_exception_pending(vcpu) || vcpu->arch.exception.injected);</blue>
<blue>}</blue>

static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	bool intr_window_requested;
	unsigned count = 130;

<yellow>	intr_window_requested = exec_controls_get(vmx) &</yellow>
				CPU_BASED_INTR_WINDOW_EXITING;

<yellow>	while (vmx->emulation_required && count-- != 0) {</yellow>
<yellow>		if (intr_window_requested && !vmx_interrupt_blocked(vcpu))</yellow>
			return handle_interrupt_window(&amp;vmx-&gt;vcpu);

<yellow>		if (kvm_test_request(KVM_REQ_EVENT, vcpu))</yellow>
			return 1;

<yellow>		if (!kvm_emulate_instruction(vcpu, 0))</yellow>
			return 0;

<yellow>		if (vmx_emulation_required_with_pending_exception(vcpu)) {</yellow>
<yellow>			kvm_prepare_emulation_failure_exit(vcpu);</yellow>
			return 0;
		}

<yellow>		if (vcpu->arch.halt_request) {</yellow>
			vcpu-&gt;arch.halt_request = 0;
<yellow>			return kvm_emulate_halt_noskip(vcpu);</yellow>
		}

		/*
		 * Note, return 1 and not 0, vcpu_run() will invoke
		 * xfer_to_guest_mode() which will create a proper return
		 * code.
		 */
<yellow>		if (__xfer_to_guest_mode_work_pending())</yellow>
			return 1;
	}

	return 1;
}

static int vmx_vcpu_pre_run(struct kvm_vcpu *vcpu)
{
<blue>	if (vmx_emulation_required_with_pending_exception(vcpu)) {</blue>
<yellow>		kvm_prepare_emulation_failure_exit(vcpu);</yellow>
		return 0;
	}

	return 1;
<blue>}</blue>

static void grow_ple_window(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<yellow>	unsigned int old = vmx->ple_window;</yellow>

<yellow>	vmx->ple_window = __grow_ple_window(old, ple_window,</yellow>
					    ple_window_grow,
					    ple_window_max);

	if (vmx-&gt;ple_window != old) {
<yellow>		vmx->ple_window_dirty = true;</yellow>
<yellow>		trace_kvm_ple_window_update(vcpu->vcpu_id,</yellow>
					    vmx-&gt;ple_window, old);
	}
}

static void shrink_ple_window(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<yellow>	unsigned int old = vmx->ple_window;</yellow>

<yellow>	vmx->ple_window = __shrink_ple_window(old, ple_window,</yellow>
					      ple_window_shrink,
					      ple_window);

	if (vmx-&gt;ple_window != old) {
<yellow>		vmx->ple_window_dirty = true;</yellow>
<yellow>		trace_kvm_ple_window_update(vcpu->vcpu_id,</yellow>
					    vmx-&gt;ple_window, old);
	}
}

/*
 * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE
 * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
 */
static int handle_pause(struct kvm_vcpu *vcpu)
{
<yellow>	if (!kvm_pause_in_guest(vcpu->kvm))</yellow>
<yellow>		grow_ple_window(vcpu);</yellow>

	/*
	 * Intel sdm vol3 ch-25.1.3 says: The &quot;PAUSE-loop exiting&quot;
	 * VM-execution control is ignored if CPL &gt; 0. OTOH, KVM
	 * never set PAUSE_EXITING and just set PLE if supported,
	 * so the vcpu must be CPL=0 if it gets a PAUSE exit.
	 */
<yellow>	kvm_vcpu_on_spin(vcpu, true);</yellow>
	return kvm_skip_emulated_instruction(vcpu);
}

static int handle_monitor_trap(struct kvm_vcpu *vcpu)
{
	return 1;
}

<yellow>static int handle_invpcid(struct kvm_vcpu *vcpu)</yellow>
<yellow>{</yellow>
	u32 vmx_instruction_info;
	unsigned long type;
	gva_t gva;
	struct {
		u64 pcid;
		u64 gla;
	} operand;
	int gpr_index;

<yellow>	if (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {</yellow>
<yellow>		kvm_queue_exception(vcpu, UD_VECTOR);</yellow>
		return 1;
	}

<yellow>	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);</yellow>
<yellow>	gpr_index = vmx_get_instr_info_reg2(vmx_instruction_info);</yellow>
<yellow>	type = kvm_register_read(vcpu, gpr_index);</yellow>

	/* According to the Intel instruction reference, the memory operand
	 * is read even if it isn&#x27;t needed (e.g., for type==all)
	 */
<yellow>	if (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),</yellow>
				vmx_instruction_info, false,
				sizeof(operand), &amp;gva))
		return 1;

<yellow>	return kvm_handle_invpcid(vcpu, type, gva);</yellow>
}

<yellow>static int handle_pml_full(struct kvm_vcpu *vcpu)</yellow>
{
	unsigned long exit_qualification;

<yellow>	trace_kvm_pml_full(vcpu->vcpu_id);</yellow>

<yellow>	exit_qualification = vmx_get_exit_qual(vcpu);</yellow>

	/*
	 * PML buffer FULL happened while executing iret from NMI,
	 * &quot;blocked by NMI&quot; bit has to be set before next VM entry.
	 */
<yellow>	if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&</yellow>
			enable_vnmi &amp;&amp;
<yellow>			(exit_qualification & INTR_INFO_UNBLOCK_NMI))</yellow>
<yellow>		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,</yellow>
				GUEST_INTR_STATE_NMI);

	/*
	 * PML buffer already flushed at beginning of VMEXIT. Nothing to do
	 * here.., and there&#x27;s no userspace involvement needed for PML.
	 */
	return 1;
<yellow>}</yellow>

<blue>static fastpath_t handle_fastpath_preemption_timer(struct kvm_vcpu *vcpu)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (!vmx->req_immediate_exit &&</blue>
<blue>	    !unlikely(vmx->loaded_vmcs->hv_timer_soft_disabled)) {</blue>
<blue>		kvm_lapic_expired_hv_timer(vcpu);</blue>
		return EXIT_FASTPATH_REENTER_GUEST;
	}

<blue>	return EXIT_FASTPATH_NONE;</blue>
<blue>}</blue>

static int handle_preemption_timer(struct kvm_vcpu *vcpu)
{
<blue>	handle_fastpath_preemption_timer(vcpu);</blue>
	return 1;
<yellow>}</yellow>

/*
 * When nested=0, all VMX instruction VM Exits filter here.  The handlers
 * are overwritten by nested_vmx_setup() when nested=1.
 */
static int handle_vmx_instruction(struct kvm_vcpu *vcpu)
{
<yellow>	kvm_queue_exception(vcpu, UD_VECTOR);</yellow>
	return 1;
}

#ifndef CONFIG_X86_SGX_KVM
static int handle_encls(struct kvm_vcpu *vcpu)
{
	/*
	 * SGX virtualization is disabled.  There is no software enable bit for
	 * SGX, so KVM intercepts all ENCLS leafs and injects a #UD to prevent
	 * the guest from executing ENCLS (when SGX is supported by hardware).
	 */
	kvm_queue_exception(vcpu, UD_VECTOR);
	return 1;
}
#endif /* CONFIG_X86_SGX_KVM */

static int handle_bus_lock_vmexit(struct kvm_vcpu *vcpu)
{
	/*
	 * Hardware may or may not set the BUS_LOCK_DETECTED flag on BUS_LOCK
	 * VM-Exits. Unconditionally set the flag here and leave the handling to
	 * vmx_handle_exit().
	 */
<yellow>	to_vmx(vcpu)->exit_reason.bus_lock_detected = true;</yellow>
	return 1;
}

static int handle_notify(struct kvm_vcpu *vcpu)
{
<yellow>	unsigned long exit_qual = vmx_get_exit_qual(vcpu);</yellow>
	bool context_invalid = exit_qual &amp; NOTIFY_VM_CONTEXT_INVALID;

	++vcpu-&gt;stat.notify_window_exits;

	/*
	 * Notify VM exit happened while executing iret from NMI,
	 * &quot;blocked by NMI&quot; bit has to be set before next VM entry.
	 */
<yellow>	if (enable_vnmi && (exit_qual & INTR_INFO_UNBLOCK_NMI))</yellow>
<yellow>		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,</yellow>
			      GUEST_INTR_STATE_NMI);

<yellow>	if (vcpu->kvm->arch.notify_vmexit_flags & KVM_X86_NOTIFY_VMEXIT_USER ||</yellow>
	    context_invalid) {
<yellow>		vcpu->run->exit_reason = KVM_EXIT_NOTIFY;</yellow>
		vcpu-&gt;run-&gt;notify.flags = context_invalid ?
					  KVM_NOTIFY_CONTEXT_INVALID : 0;
		return 0;
	}

	return 1;
<yellow>}</yellow>

/*
 * The exit handlers return 1 if the exit was handled fully and guest execution
 * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
 * to be done to userspace and return 0.
 */
static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,
	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
	[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,
	[EXIT_REASON_NMI_WINDOW]	      = handle_nmi_window,
	[EXIT_REASON_IO_INSTRUCTION]          = handle_io,
	[EXIT_REASON_CR_ACCESS]               = handle_cr,
	[EXIT_REASON_DR_ACCESS]               = handle_dr,
	[EXIT_REASON_CPUID]                   = kvm_emulate_cpuid,
	[EXIT_REASON_MSR_READ]                = kvm_emulate_rdmsr,
	[EXIT_REASON_MSR_WRITE]               = kvm_emulate_wrmsr,
	[EXIT_REASON_INTERRUPT_WINDOW]        = handle_interrupt_window,
	[EXIT_REASON_HLT]                     = kvm_emulate_halt,
	[EXIT_REASON_INVD]		      = kvm_emulate_invd,
	[EXIT_REASON_INVLPG]		      = handle_invlpg,
	[EXIT_REASON_RDPMC]                   = kvm_emulate_rdpmc,
	[EXIT_REASON_VMCALL]                  = kvm_emulate_hypercall,
	[EXIT_REASON_VMCLEAR]		      = handle_vmx_instruction,
	[EXIT_REASON_VMLAUNCH]		      = handle_vmx_instruction,
	[EXIT_REASON_VMPTRLD]		      = handle_vmx_instruction,
	[EXIT_REASON_VMPTRST]		      = handle_vmx_instruction,
	[EXIT_REASON_VMREAD]		      = handle_vmx_instruction,
	[EXIT_REASON_VMRESUME]		      = handle_vmx_instruction,
	[EXIT_REASON_VMWRITE]		      = handle_vmx_instruction,
	[EXIT_REASON_VMOFF]		      = handle_vmx_instruction,
	[EXIT_REASON_VMON]		      = handle_vmx_instruction,
	[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,
	[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,
	[EXIT_REASON_APIC_WRITE]              = handle_apic_write,
	[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,
	[EXIT_REASON_WBINVD]                  = kvm_emulate_wbinvd,
	[EXIT_REASON_XSETBV]                  = kvm_emulate_xsetbv,
	[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,
	[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,
	[EXIT_REASON_GDTR_IDTR]		      = handle_desc,
	[EXIT_REASON_LDTR_TR]		      = handle_desc,
	[EXIT_REASON_EPT_VIOLATION]	      = handle_ept_violation,
	[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,
	[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,
	[EXIT_REASON_MWAIT_INSTRUCTION]	      = kvm_emulate_mwait,
	[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,
	[EXIT_REASON_MONITOR_INSTRUCTION]     = kvm_emulate_monitor,
	[EXIT_REASON_INVEPT]                  = handle_vmx_instruction,
	[EXIT_REASON_INVVPID]                 = handle_vmx_instruction,
	[EXIT_REASON_RDRAND]                  = kvm_handle_invalid_op,
	[EXIT_REASON_RDSEED]                  = kvm_handle_invalid_op,
	[EXIT_REASON_PML_FULL]		      = handle_pml_full,
	[EXIT_REASON_INVPCID]                 = handle_invpcid,
	[EXIT_REASON_VMFUNC]		      = handle_vmx_instruction,
	[EXIT_REASON_PREEMPTION_TIMER]	      = handle_preemption_timer,
	[EXIT_REASON_ENCLS]		      = handle_encls,
	[EXIT_REASON_BUS_LOCK]                = handle_bus_lock_vmexit,
	[EXIT_REASON_NOTIFY]		      = handle_notify,
};

static const int kvm_vmx_max_exit_handlers =
	ARRAY_SIZE(kvm_vmx_exit_handlers);

<blue>static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,</blue>
			      u64 *info1, u64 *info2,
			      u32 *intr_info, u32 *error_code)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	*reason = vmx->exit_reason.full;</blue>
	*info1 = vmx_get_exit_qual(vcpu);
	if (!(vmx-&gt;exit_reason.failed_vmentry)) {
<blue>		*info2 = vmx->idt_vectoring_info;</blue>
		*intr_info = vmx_get_intr_info(vcpu);
		if (is_exception_with_error_code(*intr_info))
<yellow>			*error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);</yellow>
		else
<blue>			*error_code = 0;</blue>
	} else {
<yellow>		*info2 = 0;</yellow>
		*intr_info = 0;
		*error_code = 0;
	}
}

static void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)
{
<blue>	if (vmx->pml_pg) {</blue>
<blue>		__free_page(vmx->pml_pg);</blue>
		vmx-&gt;pml_pg = NULL;
	}
}

static void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u64 *pml_buf;
	u16 pml_idx;

<blue>	pml_idx = vmcs_read16(GUEST_PML_INDEX);</blue>

	/* Do nothing if PML buffer is empty */
<blue>	if (pml_idx == (PML_ENTITY_NUM - 1))</blue>
		return;

	/* PML index always points to next available PML buffer entity */
<blue>	if (pml_idx >= PML_ENTITY_NUM)</blue>
		pml_idx = 0;
	else
<blue>		pml_idx++;</blue>

<blue>	pml_buf = page_address(vmx->pml_pg);</blue>
	for (; pml_idx &lt; PML_ENTITY_NUM; pml_idx++) {
		u64 gpa;

<blue>		gpa = pml_buf[pml_idx];</blue>
<yellow>		WARN_ON(gpa & (PAGE_SIZE - 1));</yellow>
<blue>		kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);</blue>
	}

	/* reset PML index */
<blue>	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);</blue>
}

static void vmx_dump_sel(char *name, uint32_t sel)
{
<yellow>	pr_err("%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",</yellow>
	       name, vmcs_read16(sel),
	       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),
	       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),
	       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));
}

static void vmx_dump_dtsel(char *name, uint32_t limit)
{
<yellow>	pr_err("%s                           limit=0x%08x, base=0x%016lx\n",</yellow>
	       name, vmcs_read32(limit),
	       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));
}

static void vmx_dump_msrs(char *name, struct vmx_msrs *m)
{
	unsigned int i;
	struct vmx_msr_entry *e;

<yellow>	pr_err("MSR %s:\n", name);</yellow>
<yellow>	for (i = 0, e = m->val; i < m->nr; ++i, ++e)</yellow>
<yellow>		pr_err("  %2d: msr=0x%08x value=0x%016llx\n", i, e->index, e->value);</yellow>
<yellow>}</yellow>

void dump_vmcs(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u32 vmentry_ctl, vmexit_ctl;
	u32 cpu_based_exec_ctrl, pin_based_exec_ctrl, secondary_exec_control;
	u64 tertiary_exec_control;
	unsigned long cr4;
	int efer_slot;

<blue>	if (!dump_invalid_vmcs) {</blue>
<blue>		pr_warn_ratelimited("set kvm_intel.dump_invalid_vmcs=1 to dump internal KVM state.\n");</blue>
		return;
	}

<yellow>	vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);</yellow>
<yellow>	vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);</yellow>
<yellow>	cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);</yellow>
<yellow>	pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);</yellow>
<yellow>	cr4 = vmcs_readl(GUEST_CR4);</yellow>

<yellow>	if (cpu_has_secondary_exec_ctrls())</yellow>
<yellow>		secondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);</yellow>
	else
		secondary_exec_control = 0;

<yellow>	if (cpu_has_tertiary_exec_ctrls())</yellow>
<yellow>		tertiary_exec_control = vmcs_read64(TERTIARY_VM_EXEC_CONTROL);</yellow>
	else
		tertiary_exec_control = 0;

<yellow>	pr_err("VMCS %p, last attempted VM-entry on CPU %d\n",</yellow>
	       vmx-&gt;loaded_vmcs-&gt;vmcs, vcpu-&gt;arch.last_vmentry_cpu);
	pr_err(&quot;*** Guest State ***\n&quot;);
<yellow>	pr_err("CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",</yellow>
	       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),
	       vmcs_readl(CR0_GUEST_HOST_MASK));
<yellow>	pr_err("CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",</yellow>
	       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));
<yellow>	pr_err("CR3 = 0x%016lx\n", vmcs_readl(GUEST_CR3));</yellow>
	if (cpu_has_vmx_ept()) {
<yellow>		pr_err("PDPTR0 = 0x%016llx  PDPTR1 = 0x%016llx\n",</yellow>
		       vmcs_read64(GUEST_PDPTR0), vmcs_read64(GUEST_PDPTR1));
<yellow>		pr_err("PDPTR2 = 0x%016llx  PDPTR3 = 0x%016llx\n",</yellow>
		       vmcs_read64(GUEST_PDPTR2), vmcs_read64(GUEST_PDPTR3));
	}
<yellow>	pr_err("RSP = 0x%016lx  RIP = 0x%016lx\n",</yellow>
	       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
<yellow>	pr_err("RFLAGS=0x%08lx         DR7 = 0x%016lx\n",</yellow>
	       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));
<yellow>	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",</yellow>
	       vmcs_readl(GUEST_SYSENTER_ESP),
	       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));
	vmx_dump_sel(&quot;CS:  &quot;, GUEST_CS_SELECTOR);
	vmx_dump_sel(&quot;DS:  &quot;, GUEST_DS_SELECTOR);
	vmx_dump_sel(&quot;SS:  &quot;, GUEST_SS_SELECTOR);
	vmx_dump_sel(&quot;ES:  &quot;, GUEST_ES_SELECTOR);
	vmx_dump_sel(&quot;FS:  &quot;, GUEST_FS_SELECTOR);
	vmx_dump_sel(&quot;GS:  &quot;, GUEST_GS_SELECTOR);
	vmx_dump_dtsel(&quot;GDTR:&quot;, GUEST_GDTR_LIMIT);
	vmx_dump_sel(&quot;LDTR:&quot;, GUEST_LDTR_SELECTOR);
	vmx_dump_dtsel(&quot;IDTR:&quot;, GUEST_IDTR_LIMIT);
	vmx_dump_sel(&quot;TR:  &quot;, GUEST_TR_SELECTOR);
	efer_slot = vmx_find_loadstore_msr_slot(&amp;vmx-&gt;msr_autoload.guest, MSR_EFER);
	if (vmentry_ctl &amp; VM_ENTRY_LOAD_IA32_EFER)
<yellow>		pr_err("EFER= 0x%016llx\n", vmcs_read64(GUEST_IA32_EFER));</yellow>
<yellow>	else if (efer_slot >= 0)</yellow>
<yellow>		pr_err("EFER= 0x%016llx (autoload)\n",</yellow>
		       vmx-&gt;msr_autoload.guest.val[efer_slot].value);
	else if (vmentry_ctl &amp; VM_ENTRY_IA32E_MODE)
<yellow>		pr_err("EFER= 0x%016llx (effective)\n",</yellow>
		       vcpu-&gt;arch.efer | (EFER_LMA | EFER_LME));
	else
<yellow>		pr_err("EFER= 0x%016llx (effective)\n",</yellow>
		       vcpu-&gt;arch.efer &amp; ~(EFER_LMA | EFER_LME));
<yellow>	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT)</yellow>
<yellow>		pr_err("PAT = 0x%016llx\n", vmcs_read64(GUEST_IA32_PAT));</yellow>
<yellow>	pr_err("DebugCtl = 0x%016llx  DebugExceptions = 0x%016lx\n",</yellow>
	       vmcs_read64(GUEST_IA32_DEBUGCTL),
	       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));
	if (cpu_has_load_perf_global_ctrl() &amp;&amp;
<yellow>	    vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)</yellow>
<yellow>		pr_err("PerfGlobCtl = 0x%016llx\n",</yellow>
		       vmcs_read64(GUEST_IA32_PERF_GLOBAL_CTRL));
<yellow>	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)</yellow>
<yellow>		pr_err("BndCfgS = 0x%016llx\n", vmcs_read64(GUEST_BNDCFGS));</yellow>
<yellow>	pr_err("Interruptibility = %08x  ActivityState = %08x\n",</yellow>
	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
	       vmcs_read32(GUEST_ACTIVITY_STATE));
	if (secondary_exec_control &amp; SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
<yellow>		pr_err("InterruptStatus = %04x\n",</yellow>
		       vmcs_read16(GUEST_INTR_STATUS));
<yellow>	if (vmcs_read32(VM_ENTRY_MSR_LOAD_COUNT) > 0)</yellow>
<yellow>		vmx_dump_msrs("guest autoload", &vmx->msr_autoload.guest);</yellow>
<yellow>	if (vmcs_read32(VM_EXIT_MSR_STORE_COUNT) > 0)</yellow>
<yellow>		vmx_dump_msrs("guest autostore", &vmx->msr_autostore.guest);</yellow>

<yellow>	pr_err("*** Host State ***\n");</yellow>
<yellow>	pr_err("RIP = 0x%016lx  RSP = 0x%016lx\n",</yellow>
	       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));
<yellow>	pr_err("CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\n",</yellow>
	       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),
	       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),
	       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),
	       vmcs_read16(HOST_TR_SELECTOR));
<yellow>	pr_err("FSBase=%016lx GSBase=%016lx TRBase=%016lx\n",</yellow>
	       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),
	       vmcs_readl(HOST_TR_BASE));
<yellow>	pr_err("GDTBase=%016lx IDTBase=%016lx\n",</yellow>
	       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));
<yellow>	pr_err("CR0=%016lx CR3=%016lx CR4=%016lx\n",</yellow>
	       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),
	       vmcs_readl(HOST_CR4));
<yellow>	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",</yellow>
	       vmcs_readl(HOST_IA32_SYSENTER_ESP),
	       vmcs_read32(HOST_IA32_SYSENTER_CS),
	       vmcs_readl(HOST_IA32_SYSENTER_EIP));
	if (vmexit_ctl &amp; VM_EXIT_LOAD_IA32_EFER)
<yellow>		pr_err("EFER= 0x%016llx\n", vmcs_read64(HOST_IA32_EFER));</yellow>
<yellow>	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT)</yellow>
<yellow>		pr_err("PAT = 0x%016llx\n", vmcs_read64(HOST_IA32_PAT));</yellow>
<yellow>	if (cpu_has_load_perf_global_ctrl() &&</yellow>
<yellow>	    vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)</yellow>
<yellow>		pr_err("PerfGlobCtl = 0x%016llx\n",</yellow>
		       vmcs_read64(HOST_IA32_PERF_GLOBAL_CTRL));
<yellow>	if (vmcs_read32(VM_EXIT_MSR_LOAD_COUNT) > 0)</yellow>
<yellow>		vmx_dump_msrs("host autoload", &vmx->msr_autoload.host);</yellow>

<yellow>	pr_err("*** Control State ***\n");</yellow>
	pr_err(&quot;CPUBased=0x%08x SecondaryExec=0x%08x TertiaryExec=0x%016llx\n&quot;,
	       cpu_based_exec_ctrl, secondary_exec_control, tertiary_exec_control);
	pr_err(&quot;PinBased=0x%08x EntryControls=%08x ExitControls=%08x\n&quot;,
	       pin_based_exec_ctrl, vmentry_ctl, vmexit_ctl);
<yellow>	pr_err("ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\n",</yellow>
	       vmcs_read32(EXCEPTION_BITMAP),
	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),
	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));
<yellow>	pr_err("VMEntry: intr_info=%08x errcode=%08x ilen=%08x\n",</yellow>
	       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
	       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),
	       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
<yellow>	pr_err("VMExit: intr_info=%08x errcode=%08x ilen=%08x\n",</yellow>
	       vmcs_read32(VM_EXIT_INTR_INFO),
	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
<yellow>	pr_err("        reason=%08x qualification=%016lx\n",</yellow>
	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
<yellow>	pr_err("IDTVectoring: info=%08x errcode=%08x\n",</yellow>
	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
<yellow>	pr_err("TSC Offset = 0x%016llx\n", vmcs_read64(TSC_OFFSET));</yellow>
	if (secondary_exec_control &amp; SECONDARY_EXEC_TSC_SCALING)
<yellow>		pr_err("TSC Multiplier = 0x%016llx\n",</yellow>
		       vmcs_read64(TSC_MULTIPLIER));
<yellow>	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW) {</yellow>
<yellow>		if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {</yellow>
<yellow>			u16 status = vmcs_read16(GUEST_INTR_STATUS);</yellow>
<yellow>			pr_err("SVI|RVI = %02x|%02x ", status >> 8, status & 0xff);</yellow>
		}
<yellow>		pr_cont("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));</yellow>
		if (secondary_exec_control &amp; SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)
<yellow>			pr_err("APIC-access addr = 0x%016llx ", vmcs_read64(APIC_ACCESS_ADDR));</yellow>
<yellow>		pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));</yellow>
	}
<yellow>	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)</yellow>
<yellow>		pr_err("PostedIntrVec = 0x%02x\n", vmcs_read16(POSTED_INTR_NV));</yellow>
<yellow>	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))</yellow>
<yellow>		pr_err("EPT pointer = 0x%016llx\n", vmcs_read64(EPT_POINTER));</yellow>
<yellow>	if (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)</yellow>
<yellow>		pr_err("PLE Gap=%08x Window=%08x\n",</yellow>
		       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));
<yellow>	if (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)</yellow>
<yellow>		pr_err("Virtual processor ID = 0x%04x\n",</yellow>
		       vmcs_read16(VIRTUAL_PROCESSOR_ID));
<blue>}</blue>

/*
 * The guest has exited.  See if we can fix it or if we need userspace
 * assistance.
 */
static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
<blue>	union vmx_exit_reason exit_reason = vmx->exit_reason;</blue>
	u32 vectoring_info = vmx-&gt;idt_vectoring_info;
	u16 exit_handler_index;

	/*
	 * Flush logged GPAs PML buffer, this will make dirty_bitmap more
	 * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before
	 * querying dirty_bitmap, we only need to kick all vcpus out of guest
	 * mode as if vcpus is in root mode, the PML buffer must has been
	 * flushed already.  Note, PML is never enabled in hardware while
	 * running L2.
	 */
<blue>	if (enable_pml && !is_guest_mode(vcpu))</blue>
<blue>		vmx_flush_pml_buffer(vcpu);</blue>

	/*
	 * KVM should never reach this point with a pending nested VM-Enter.
	 * More specifically, short-circuiting VM-Entry to emulate L2 due to
	 * invalid guest state should never happen as that means KVM knowingly
	 * allowed a nested VM-Enter with an invalid vmcs12.  More below.
	 */
<blue>	if (KVM_BUG_ON(vmx->nested.nested_run_pending, vcpu->kvm))</blue>
		return -EIO;

<blue>	if (is_guest_mode(vcpu)) {</blue>
		/*
		 * PML is never enabled when running L2, bail immediately if a
		 * PML full exit occurs as something is horribly wrong.
		 */
<blue>		if (exit_reason.basic == EXIT_REASON_PML_FULL)</blue>
			goto unexpected_vmexit;

		/*
		 * The host physical addresses of some pages of guest memory
		 * are loaded into the vmcs02 (e.g. vmcs12&#x27;s Virtual APIC
		 * Page). The CPU may write to these pages via their host
		 * physical address while L2 is running, bypassing any
		 * address-translation-based dirty tracking (e.g. EPT write
		 * protection).
		 *
		 * Mark them dirty on every exit from L2 to prevent them from
		 * getting out of sync with dirty tracking.
		 */
		nested_mark_vmcs12_pages_dirty(vcpu);

		/*
		 * Synthesize a triple fault if L2 state is invalid.  In normal
		 * operation, nested VM-Enter rejects any attempt to enter L2
		 * with invalid state.  However, those checks are skipped if
		 * state is being stuffed via RSM or KVM_SET_NESTED_STATE.  If
		 * L2 state is invalid, it means either L1 modified SMRAM state
		 * or userspace provided bad state.  Synthesize TRIPLE_FAULT as
		 * doing so is architecturally allowed in the RSM case, and is
		 * the least awful solution for the userspace case without
		 * risking false positives.
		 */
<blue>		if (vmx->emulation_required) {</blue>
<blue>			nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);</blue>
			return 1;
		}

<blue>		if (nested_vmx_reflect_vmexit(vcpu))</blue>
			return 1;
	}

	/* If guest state is invalid, start emulating.  L2 is handled above. */
<blue>	if (vmx->emulation_required)</blue>
<yellow>		return handle_invalid_guest_state(vcpu);</yellow>

<blue>	if (exit_reason.failed_vmentry) {</blue>
		dump_vmcs(vcpu);
<blue>		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;</blue>
		vcpu-&gt;run-&gt;fail_entry.hardware_entry_failure_reason
			= exit_reason.full;
		vcpu-&gt;run-&gt;fail_entry.cpu = vcpu-&gt;arch.last_vmentry_cpu;
		return 0;
	}

<blue>	if (unlikely(vmx->fail)) {</blue>
		dump_vmcs(vcpu);
<blue>		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;</blue>
<blue>		vcpu->run->fail_entry.hardware_entry_failure_reason</blue>
<blue>			= vmcs_read32(VM_INSTRUCTION_ERROR);</blue>
		vcpu-&gt;run-&gt;fail_entry.cpu = vcpu-&gt;arch.last_vmentry_cpu;
		return 0;
	}

	/*
	 * Note:
	 * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by
	 * delivery event since it indicates guest is accessing MMIO.
	 * The vm-exit can be triggered again after return to guest that
	 * will cause infinite loop.
	 */
<blue>	if ((vectoring_info & VECTORING_INFO_VALID_MASK) &&</blue>
	    (exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &amp;&amp;
	     exit_reason.basic != EXIT_REASON_EPT_VIOLATION &amp;&amp;
	     exit_reason.basic != EXIT_REASON_PML_FULL &amp;&amp;
	     exit_reason.basic != EXIT_REASON_APIC_ACCESS &amp;&amp;
	     exit_reason.basic != EXIT_REASON_TASK_SWITCH &amp;&amp;
	     exit_reason.basic != EXIT_REASON_NOTIFY)) {
		int ndata = 3;

<blue>		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;</blue>
		vcpu-&gt;run-&gt;internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;
		vcpu-&gt;run-&gt;internal.data[0] = vectoring_info;
		vcpu-&gt;run-&gt;internal.data[1] = exit_reason.full;
		vcpu-&gt;run-&gt;internal.data[2] = vcpu-&gt;arch.exit_qualification;
		if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {
<blue>			vcpu->run->internal.data[ndata++] =</blue>
<blue>				vmcs_read64(GUEST_PHYSICAL_ADDRESS);</blue>
		}
<blue>		vcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;</blue>
		vcpu-&gt;run-&gt;internal.ndata = ndata;
		return 0;
	}

<blue>	if (unlikely(!enable_vnmi &&</blue>
		     vmx-&gt;loaded_vmcs-&gt;soft_vnmi_blocked)) {
<yellow>		if (!vmx_interrupt_blocked(vcpu)) {</yellow>
<yellow>			vmx->loaded_vmcs->soft_vnmi_blocked = 0;</yellow>
<yellow>		} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&</yellow>
<yellow>			   vcpu->arch.nmi_pending) {</yellow>
			/*
			 * This CPU don&#x27;t support us in finding the end of an
			 * NMI-blocked window if the guest runs with IRQs
			 * disabled. So we pull the trigger after 1 s of
			 * futile waiting, but inform the user about this.
			 */
<yellow>			printk(KERN_WARNING "%s: Breaking out of NMI-blocked "</yellow>
			       &quot;state on VCPU %d after 1 s timeout\n&quot;,
			       __func__, vcpu-&gt;vcpu_id);
			vmx-&gt;loaded_vmcs-&gt;soft_vnmi_blocked = 0;
		}
	}

<blue>	if (exit_fastpath != EXIT_FASTPATH_NONE)</blue>
		return 1;

<blue>	if (exit_reason.basic >= kvm_vmx_max_exit_handlers)</blue>
		goto unexpected_vmexit;
#ifdef CONFIG_RETPOLINE
<blue>	if (exit_reason.basic == EXIT_REASON_MSR_WRITE)</blue>
<blue>		return kvm_emulate_wrmsr(vcpu);</blue>
<blue>	else if (exit_reason.basic == EXIT_REASON_PREEMPTION_TIMER)</blue>
<blue>		return handle_preemption_timer(vcpu);</blue>
<blue>	else if (exit_reason.basic == EXIT_REASON_INTERRUPT_WINDOW)</blue>
<yellow>		return handle_interrupt_window(vcpu);</yellow>
<blue>	else if (exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)</blue>
<blue>		return handle_external_interrupt(vcpu);</blue>
<blue>	else if (exit_reason.basic == EXIT_REASON_HLT)</blue>
<blue>		return kvm_emulate_halt(vcpu);</blue>
<blue>	else if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG)</blue>
<blue>		return handle_ept_misconfig(vcpu);</blue>
#endif

<blue>	exit_handler_index = array_index_nospec((u16)exit_reason.basic,</blue>
						kvm_vmx_max_exit_handlers);
	if (!kvm_vmx_exit_handlers[exit_handler_index])
		goto unexpected_vmexit;

<blue>	return kvm_vmx_exit_handlers[exit_handler_index](vcpu);</blue>

unexpected_vmexit:
<yellow>	vcpu_unimpl(vcpu, "vmx: unexpected exit reason 0x%x\n",</yellow>
		    exit_reason.full);
	dump_vmcs(vcpu);
<yellow>	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;</yellow>
	vcpu-&gt;run-&gt;internal.suberror =
			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
	vcpu-&gt;run-&gt;internal.ndata = 2;
	vcpu-&gt;run-&gt;internal.data[0] = exit_reason.full;
	vcpu-&gt;run-&gt;internal.data[1] = vcpu-&gt;arch.last_vmentry_cpu;
	return 0;
}

static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
{
<blue>	int ret = __vmx_handle_exit(vcpu, exit_fastpath);</blue>

	/*
	 * Exit to user space when bus lock detected to inform that there is
	 * a bus lock in guest.
	 */
<blue>	if (to_vmx(vcpu)->exit_reason.bus_lock_detected) {</blue>
<yellow>		if (ret > 0)</yellow>
<yellow>			vcpu->run->exit_reason = KVM_EXIT_X86_BUS_LOCK;</yellow>

		vcpu-&gt;run-&gt;flags |= KVM_RUN_X86_BUS_LOCK;
		return 0;
	}
	return ret;
<blue>}</blue>

/*
 * Software based L1D cache flush which is used when microcode providing
 * the cache control MSR is not loaded.
 *
 * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
 * flush it is required to read in 64 KiB because the replacement algorithm
 * is not exactly LRU. This could be sized at runtime via topology
 * information but as all relevant affected CPUs have 32KiB L1D cache size
 * there is no point in doing so.
 */
static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
{
	int size = PAGE_SIZE &lt;&lt; L1D_CACHE_ORDER;

	/*
	 * This code is only executed when the flush mode is &#x27;cond&#x27; or
	 * &#x27;always&#x27;
	 */
	if (static_branch_likely(&amp;vmx_l1d_flush_cond)) {
		bool flush_l1d;

		/*
		 * Clear the per-vcpu flush bit, it gets set again
		 * either from vcpu_run() or from one of the unsafe
		 * VMEXIT handlers.
		 */
		flush_l1d = vcpu-&gt;arch.l1tf_flush_l1d;
		vcpu-&gt;arch.l1tf_flush_l1d = false;

		/*
		 * Clear the per-cpu flush bit, it gets set again from
		 * the interrupt handlers.
		 */
		flush_l1d |= kvm_get_cpu_l1tf_flush_l1d();
		kvm_clear_cpu_l1tf_flush_l1d();

		if (!flush_l1d)
			return;
	}

	vcpu-&gt;stat.l1d_flush++;

	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
		native_wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
		return;
	}

	asm volatile(
		/* First ensure the pages are in the TLB */
		&quot;xorl	%%eax, %%eax\n&quot;
		&quot;.Lpopulate_tlb:\n\t&quot;
		&quot;movzbl	(%[flush_pages], %%&quot; _ASM_AX &quot;), %%ecx\n\t&quot;
		&quot;addl	$4096, %%eax\n\t&quot;
		&quot;cmpl	%%eax, %[size]\n\t&quot;
		&quot;jne	.Lpopulate_tlb\n\t&quot;
		&quot;xorl	%%eax, %%eax\n\t&quot;
		&quot;cpuid\n\t&quot;
		/* Now fill the cache */
		&quot;xorl	%%eax, %%eax\n&quot;
		&quot;.Lfill_cache:\n&quot;
		&quot;movzbl	(%[flush_pages], %%&quot; _ASM_AX &quot;), %%ecx\n\t&quot;
		&quot;addl	$64, %%eax\n\t&quot;
		&quot;cmpl	%%eax, %[size]\n\t&quot;
		&quot;jne	.Lfill_cache\n\t&quot;
		&quot;lfence\n&quot;
		:: [flush_pages] &quot;r&quot; (vmx_l1d_flush_pages),
		    [size] &quot;r&quot; (size)
		: &quot;eax&quot;, &quot;ebx&quot;, &quot;ecx&quot;, &quot;edx&quot;);
}

static void vmx_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
{
<blue>	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</blue>
	int tpr_threshold;

	if (is_guest_mode(vcpu) &amp;&amp;
<yellow>		nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))</yellow>
		return;

<blue>	tpr_threshold = (irr == -1 || tpr < irr) ? 0 : irr;</blue>
<blue>	if (is_guest_mode(vcpu))</blue>
<yellow>		to_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;</yellow>
	else
<blue>		vmcs_write32(TPR_THRESHOLD, tpr_threshold);</blue>
<blue>}</blue>

<blue>void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u32 sec_exec_control;

<blue>	if (!lapic_in_kernel(vcpu))</blue>
		return;

<blue>	if (!flexpriority_enabled &&</blue>
<yellow>	    !cpu_has_vmx_virtualize_x2apic_mode())</yellow>
		return;

	/* Postpone execution until vmcs01 is the current VMCS. */
<blue>	if (is_guest_mode(vcpu)) {</blue>
<yellow>		vmx->nested.change_vmcs01_virtual_apic_mode = true;</yellow>
		return;
	}

<blue>	sec_exec_control = secondary_exec_controls_get(vmx);</blue>
	sec_exec_control &amp;= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
			      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);

	switch (kvm_get_apic_mode(vcpu)) {
	case LAPIC_MODE_INVALID:
<yellow>		WARN_ONCE(true, "Invalid local APIC state");</yellow>
		break;
	case LAPIC_MODE_DISABLED:
		break;
	case LAPIC_MODE_XAPIC:
<yellow>		if (flexpriority_enabled) {</yellow>
<yellow>			sec_exec_control |=</yellow>
				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
			kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);

			/*
			 * Flush the TLB, reloading the APIC access page will
			 * only do so if its physical address has changed, but
			 * the guest may have inserted a non-APIC mapping into
			 * the TLB while the APIC access page was disabled.
			 */
			kvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);
		}
		break;
	case LAPIC_MODE_X2APIC:
<yellow>		if (cpu_has_vmx_virtualize_x2apic_mode())</yellow>
<yellow>			sec_exec_control |=</yellow>
				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;
		break;
	}
<blue>	secondary_exec_controls_set(vmx, sec_exec_control);</blue>

<blue>	vmx_update_msr_bitmap_x2apic(vcpu);</blue>
<blue>}</blue>

static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
{
	struct page *page;

	/* Defer reload until vmcs01 is the current VMCS. */
<blue>	if (is_guest_mode(vcpu)) {</blue>
<yellow>		to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;</yellow>
		return;
	}

<blue>	if (!(secondary_exec_controls_get(to_vmx(vcpu)) &</blue>
	    SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
		return;

<blue>	page = gfn_to_page(vcpu->kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);</blue>
	if (is_error_page(page))
		return;

<blue>	vmcs_write64(APIC_ACCESS_ADDR, page_to_phys(page));</blue>
<blue>	vmx_flush_tlb_current(vcpu);</blue>

	/*
	 * Do not pin apic access page in memory, the MMU notifier
	 * will call us again if it is migrated or swapped out.
	 */
<blue>	put_page(page);</blue>
<blue>}</blue>

static void vmx_hwapic_isr_update(int max_isr)
{
	u16 status;
	u8 old;

<blue>	if (max_isr == -1)</blue>
		max_isr = 0;

<blue>	status = vmcs_read16(GUEST_INTR_STATUS);</blue>
	old = status &gt;&gt; 8;
<blue>	if (max_isr != old) {</blue>
<blue>		status &= 0xff;</blue>
		status |= max_isr &lt;&lt; 8;
<blue>		vmcs_write16(GUEST_INTR_STATUS, status);</blue>
	}
<blue>}</blue>

static void vmx_set_rvi(int vector)
{
	u16 status;
	u8 old;

<blue>	if (vector == -1)</blue>
		vector = 0;

<blue>	status = vmcs_read16(GUEST_INTR_STATUS);</blue>
	old = (u8)status &amp; 0xff;
<blue>	if ((u8)vector != old) {</blue>
<blue>		status &= ~0xff;</blue>
		status |= (u8)vector;
<blue>		vmcs_write16(GUEST_INTR_STATUS, status);</blue>
	}
<blue>}</blue>

static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
{
	/*
	 * When running L2, updating RVI is only relevant when
	 * vmcs12 virtual-interrupt-delivery enabled.
	 * However, it can be enabled only when L1 also
	 * intercepts external-interrupts and in that case
	 * we should not update vmcs02 RVI but instead intercept
	 * interrupt. Therefore, do nothing when running L2.
	 */
<blue>	if (!is_guest_mode(vcpu))</blue>
<blue>		vmx_set_rvi(max_irr);</blue>
<blue>}</blue>

static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
<blue>{</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	int max_irr;
	bool got_posted_interrupt;

<blue>	if (KVM_BUG_ON(!enable_apicv, vcpu->kvm))</blue>
		return -EIO;

<blue>	if (pi_test_on(&vmx->pi_desc)) {</blue>
<blue>		pi_clear_on(&vmx->pi_desc);</blue>
		/*
		 * IOMMU can write to PID.ON, so the barrier matters even on UP.
		 * But on x86 this is just a compiler barrier anyway.
		 */
		smp_mb__after_atomic();
		got_posted_interrupt =
			kvm_apic_update_irr(vcpu, vmx-&gt;pi_desc.pir, &amp;max_irr);
	} else {
<blue>		max_irr = kvm_lapic_find_highest_irr(vcpu);</blue>
		got_posted_interrupt = false;
	}

	/*
	 * Newly recognized interrupts are injected via either virtual interrupt
	 * delivery (RVI) or KVM_REQ_EVENT.  Virtual interrupt delivery is
	 * disabled in two cases:
	 *
	 * 1) If L2 is running and the vCPU has a new pending interrupt.  If L1
	 * wants to exit on interrupts, KVM_REQ_EVENT is needed to synthesize a
	 * VM-Exit to L1.  If L1 doesn&#x27;t want to exit, the interrupt is injected
	 * into L2, but KVM doesn&#x27;t use virtual interrupt delivery to inject
	 * interrupts into L2, and so KVM_REQ_EVENT is again needed.
	 *
	 * 2) If APICv is disabled for this vCPU, assigned devices may still
	 * attempt to post interrupts.  The posted interrupt vector will cause
	 * a VM-Exit and the subsequent entry will call sync_pir_to_irr.
	 */
<blue>	if (!is_guest_mode(vcpu) && kvm_vcpu_apicv_active(vcpu))</blue>
<blue>		vmx_set_rvi(max_irr);</blue>
<blue>	else if (got_posted_interrupt)</blue>
<yellow>		kvm_make_request(KVM_REQ_EVENT, vcpu);</yellow>

<blue>	return max_irr;</blue>
}

static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
{
<blue>	if (!kvm_vcpu_apicv_active(vcpu))</blue>
		return;

<blue>	vmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);</blue>
<blue>	vmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);</blue>
<blue>	vmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);</blue>
<blue>	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);</blue>
<blue>}</blue>

static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	pi_clear_on(&vmx->pi_desc);</blue>
	memset(vmx-&gt;pi_desc.pir, 0, sizeof(vmx-&gt;pi_desc.pir));
}

void vmx_do_interrupt_nmi_irqoff(unsigned long entry);

static void handle_interrupt_nmi_irqoff(struct kvm_vcpu *vcpu,
					unsigned long entry)
{
	bool is_nmi = entry == (unsigned long)asm_exc_nmi_noist;

<blue>	kvm_before_interrupt(vcpu, is_nmi ? KVM_HANDLING_NMI : KVM_HANDLING_IRQ);</blue>
	vmx_do_interrupt_nmi_irqoff(entry);
	kvm_after_interrupt(vcpu);
}

static void handle_nm_fault_irqoff(struct kvm_vcpu *vcpu)
{
	/*
	 * Save xfd_err to guest_fpu before interrupt is enabled, so the
	 * MSR value is not clobbered by the host activity before the guest
	 * has chance to consume it.
	 *
	 * Do not blindly read xfd_err here, since this exception might
	 * be caused by L1 interception on a platform which doesn&#x27;t
	 * support xfd at all.
	 *
	 * Do it conditionally upon guest_fpu::xfd. xfd_err matters
	 * only when xfd contains a non-zero value.
	 *
	 * Queuing exception is done in vmx_handle_exit. See comment there.
	 */
<yellow>	if (vcpu->arch.guest_fpu.fpstate->xfd)</yellow>
<yellow>		rdmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);</yellow>
}

static void handle_exception_nmi_irqoff(struct vcpu_vmx *vmx)
{
	const unsigned long nmi_entry = (unsigned long)asm_exc_nmi_noist;
<blue>	u32 intr_info = vmx_get_intr_info(&vmx->vcpu);</blue>

	/* if exit due to PF check for async PF */
	if (is_page_fault(intr_info))
<yellow>		vmx->vcpu.arch.apf.host_apf_flags = kvm_read_and_reset_apf_flags();</yellow>
	/* if exit due to NM, handle before interrupts are enabled */
<blue>	else if (is_nm_fault(intr_info))</blue>
<yellow>		handle_nm_fault_irqoff(&vmx->vcpu);</yellow>
	/* Handle machine checks before interrupts are enabled */
<blue>	else if (is_machine_check(intr_info))</blue>
<yellow>		kvm_machine_check();</yellow>
	/* We need to handle NMIs before interrupts are enabled */
<blue>	else if (is_nmi(intr_info))</blue>
<yellow>		handle_interrupt_nmi_irqoff(&vmx->vcpu, nmi_entry);</yellow>
}

static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu)
{
<blue>	u32 intr_info = vmx_get_intr_info(vcpu);</blue>
	unsigned int vector = intr_info &amp; INTR_INFO_VECTOR_MASK;
	gate_desc *desc = (gate_desc *)host_idt_base + vector;

<yellow>	if (KVM_BUG(!is_external_intr(intr_info), vcpu->kvm,</yellow>
	    &quot;KVM: unexpected VM-Exit interrupt info: 0x%x&quot;, intr_info))
		return;

<blue>	handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));</blue>
	vcpu-&gt;arch.at_instruction_boundary = true;
}

<blue>static void vmx_handle_exit_irqoff(struct kvm_vcpu *vcpu)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (vmx->emulation_required)</blue>
		return;

<blue>	if (vmx->exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)</blue>
<blue>		handle_external_interrupt_irqoff(vcpu);</blue>
<blue>	else if (vmx->exit_reason.basic == EXIT_REASON_EXCEPTION_NMI)</blue>
<blue>		handle_exception_nmi_irqoff(vmx);</blue>
<blue>}</blue>

/*
 * The kvm parameter can be NULL (module initialization, or invocation before
 * VM creation). Be sure to check the kvm parameter before using it.
 */
<blue>static bool vmx_has_emulated_msr(struct kvm *kvm, u32 index)</blue>
{
<blue>	switch (index) {</blue>
	case MSR_IA32_SMBASE:
		/*
		 * We cannot do SMM unless we can run the guest in big
		 * real mode.
		 */
<blue>		return enable_unrestricted_guest || emulate_invalid_guest_state;</blue>
	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
<yellow>		return nested;</yellow>
	case MSR_AMD64_VIRT_SPEC_CTRL:
	case MSR_AMD64_TSC_RATIO:
		/* This is AMD only.  */
		return false;
	default:
		return true;
	}
<blue>}</blue>

static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
{
	u32 exit_intr_info;
	bool unblock_nmi;
	u8 vector;
	bool idtv_info_valid;

	idtv_info_valid = vmx-&gt;idt_vectoring_info &amp; VECTORING_INFO_VALID_MASK;

<yellow>	if (enable_vnmi) {</yellow>
<blue>		if (vmx->loaded_vmcs->nmi_known_unmasked)</blue>
			return;

<blue>		exit_intr_info = vmx_get_intr_info(&vmx->vcpu);</blue>
<blue>		unblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;</blue>
<yellow>		vector = exit_intr_info & INTR_INFO_VECTOR_MASK;</yellow>
		/*
		 * SDM 3: 27.7.1.2 (September 2008)
		 * Re-set bit &quot;block by NMI&quot; before VM entry if vmexit caused by
		 * a guest IRET fault.
		 * SDM 3: 23.2.2 (September 2008)
		 * Bit 12 is undefined in any of the following cases:
		 *  If the VM exit sets the valid bit in the IDT-vectoring
		 *   information field.
		 *  If the VM exit is due to a double fault.
		 */
		if ((exit_intr_info &amp; INTR_INFO_VALID_MASK) &amp;&amp; unblock_nmi &amp;&amp;
<yellow>		    vector != DF_VECTOR && !idtv_info_valid)</yellow>
<yellow>			vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,</yellow>
				      GUEST_INTR_STATE_NMI);
		else
<blue>			vmx->loaded_vmcs->nmi_known_unmasked =</blue>
<blue>				!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)</blue>
<blue>				  & GUEST_INTR_STATE_NMI);</blue>
<yellow>	} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))</yellow>
		vmx-&gt;loaded_vmcs-&gt;vnmi_blocked_time +=
<yellow>			ktime_to_ns(ktime_sub(ktime_get(),</yellow>
					      vmx-&gt;loaded_vmcs-&gt;entry_time));
}

static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
				      u32 idt_vectoring_info,
				      int instr_len_field,
				      int error_code_field)
{
	u8 vector;
	int type;
	bool idtv_info_valid;

	idtv_info_valid = idt_vectoring_info &amp; VECTORING_INFO_VALID_MASK;

<blue>	vcpu->arch.nmi_injected = false;</blue>
	kvm_clear_exception_queue(vcpu);
	kvm_clear_interrupt_queue(vcpu);

	if (!idtv_info_valid)
		return;

<blue>	kvm_make_request(KVM_REQ_EVENT, vcpu);</blue>

	vector = idt_vectoring_info &amp; VECTORING_INFO_VECTOR_MASK;
	type = idt_vectoring_info &amp; VECTORING_INFO_TYPE_MASK;

	switch (type) {
	case INTR_TYPE_NMI_INTR:
<blue>		vcpu->arch.nmi_injected = true;</blue>
		/*
		 * SDM 3: 27.7.1.2 (September 2008)
		 * Clear bit &quot;block by NMI&quot; before VM entry if a NMI
		 * delivery faulted.
		 */
<blue>		vmx_set_nmi_mask(vcpu, false);</blue>
		break;
	case INTR_TYPE_SOFT_EXCEPTION:
<blue>		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);</blue>
		fallthrough;
	case INTR_TYPE_HARD_EXCEPTION:
<blue>		if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {</blue>
<blue>			u32 err = vmcs_read32(error_code_field);</blue>
<blue>			kvm_requeue_exception_e(vcpu, vector, err);</blue>
		} else
<blue>			kvm_requeue_exception(vcpu, vector);</blue>
		break;
	case INTR_TYPE_SOFT_INTR:
<blue>		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);</blue>
		fallthrough;
	case INTR_TYPE_EXT_INTR:
<blue>		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);</blue>
		break;
	default:
		break;
	}
<blue>}</blue>

static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
{
<blue>	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,</blue>
				  VM_EXIT_INSTRUCTION_LEN,
				  IDT_VECTORING_ERROR_CODE);
}

static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
{
<blue>	__vmx_complete_interrupts(vcpu,</blue>
				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
				  VM_ENTRY_INSTRUCTION_LEN,
				  VM_ENTRY_EXCEPTION_ERROR_CODE);

<blue>	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);</blue>
<blue>}</blue>

static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
{
	int i, nr_msrs;
	struct perf_guest_switch_msr *msrs;
	struct kvm_pmu *pmu = vcpu_to_pmu(&amp;vmx-&gt;vcpu);

<blue>	pmu->host_cross_mapped_mask = 0;</blue>
	if (pmu-&gt;pebs_enable &amp; pmu-&gt;global_ctrl)
<yellow>		intel_pmu_cross_mapped_check(pmu);</yellow>

	/* Note, nr_msrs may be garbage if perf_guest_get_msrs() returns NULL. */
<blue>	msrs = perf_guest_get_msrs(&nr_msrs, (void *)pmu);</blue>
	if (!msrs)
		return;

<blue>	for (i = 0; i < nr_msrs; i++)</blue>
<blue>		if (msrs[i].host == msrs[i].guest)</blue>
<blue>			clear_atomic_switch_msr(vmx, msrs[i].msr);</blue>
		else
<yellow>			add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,</yellow>
					msrs[i].host, false);
}

static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	u64 tscl;
	u32 delta_tsc;

<blue>	if (vmx->req_immediate_exit) {</blue>
<blue>		vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, 0);</blue>
<blue>		vmx->loaded_vmcs->hv_timer_soft_disabled = false;</blue>
<blue>	} else if (vmx->hv_deadline_tsc != -1) {</blue>
<blue>		tscl = rdtsc();</blue>
		if (vmx-&gt;hv_deadline_tsc &gt; tscl)
			/* set_hv_timer ensures the delta fits in 32-bits */
<blue>			delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >></blue>
				cpu_preemption_timer_multi);
		else
			delta_tsc = 0;

<blue>		vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, delta_tsc);</blue>
<blue>		vmx->loaded_vmcs->hv_timer_soft_disabled = false;</blue>
<blue>	} else if (!vmx->loaded_vmcs->hv_timer_soft_disabled) {</blue>
<blue>		vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, -1);</blue>
<blue>		vmx->loaded_vmcs->hv_timer_soft_disabled = true;</blue>
	}
}

void noinstr vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
{
	if (unlikely(host_rsp != vmx-&gt;loaded_vmcs-&gt;host_state.rsp)) {
		vmx-&gt;loaded_vmcs-&gt;host_state.rsp = host_rsp;
		vmcs_writel(HOST_RSP, host_rsp);
	}
}

void noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,
					unsigned int flags)
{
	u64 hostval = this_cpu_read(x86_spec_ctrl_current);

	if (!cpu_feature_enabled(X86_FEATURE_MSR_SPEC_CTRL))
		return;

	if (flags &amp; VMX_RUN_SAVE_SPEC_CTRL)
		vmx-&gt;spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);

	/*
	 * If the guest/host SPEC_CTRL values differ, restore the host value.
	 *
	 * For legacy IBRS, the IBRS bit always needs to be written after
	 * transitioning from a less privileged predictor mode, regardless of
	 * whether the guest/host values differ.
	 */
	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
	    vmx-&gt;spec_ctrl != hostval)
		native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);

	barrier_nospec();
}

static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
{
<blue>	switch (to_vmx(vcpu)->exit_reason.basic) {</blue>
	case EXIT_REASON_MSR_WRITE:
<blue>		return handle_fastpath_set_msr_irqoff(vcpu);</blue>
	case EXIT_REASON_PREEMPTION_TIMER:
<blue>		return handle_fastpath_preemption_timer(vcpu);</blue>
	default:
		return EXIT_FASTPATH_NONE;
	}
}

static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
					struct vcpu_vmx *vmx,
					unsigned long flags)
{
	guest_state_enter_irqoff();

	/* L1D Flush includes CPU buffer clear to mitigate MDS */
	if (static_branch_unlikely(&amp;vmx_l1d_should_flush))
		vmx_l1d_flush(vcpu);
	else if (static_branch_unlikely(&amp;mds_user_clear))
		mds_clear_cpu_buffers();
	else if (static_branch_unlikely(&amp;mmio_stale_data_clear) &amp;&amp;
		 kvm_arch_has_assigned_device(vcpu-&gt;kvm))
		mds_clear_cpu_buffers();

	vmx_disable_fb_clear(vmx);

	if (vcpu-&gt;arch.cr2 != native_read_cr2())
		native_write_cr2(vcpu-&gt;arch.cr2);

	vmx-&gt;fail = __vmx_vcpu_run(vmx, (unsigned long *)&amp;vcpu-&gt;arch.regs,
				   flags);

	vcpu-&gt;arch.cr2 = native_read_cr2();

	vmx_enable_fb_clear(vmx);

	guest_state_exit_irqoff();
}

static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
<blue>{</blue>
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	unsigned long cr3, cr4;

	/* Record the guest&#x27;s net vcpu time for enforced NMI injections. */
<blue>	if (unlikely(!enable_vnmi &&</blue>
		     vmx-&gt;loaded_vmcs-&gt;soft_vnmi_blocked))
<yellow>		vmx->loaded_vmcs->entry_time = ktime_get();</yellow>

	/*
	 * Don&#x27;t enter VMX if guest state is invalid, let the exit handler
	 * start emulation until we arrive back to a valid state.  Synthesize a
	 * consistency check VM-Exit due to invalid guest state and bail.
	 */
<blue>	if (unlikely(vmx->emulation_required)) {</blue>
<blue>		vmx->fail = 0;</blue>

		vmx-&gt;exit_reason.full = EXIT_REASON_INVALID_STATE;
		vmx-&gt;exit_reason.failed_vmentry = 1;
		kvm_register_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_1);
		vmx-&gt;exit_qualification = ENTRY_FAIL_DEFAULT;
		kvm_register_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_2);
		vmx-&gt;exit_intr_info = 0;
		return EXIT_FASTPATH_NONE;
	}

<blue>	trace_kvm_entry(vcpu);</blue>

<blue>	if (vmx->ple_window_dirty) {</blue>
<yellow>		vmx->ple_window_dirty = false;</yellow>
<yellow>		vmcs_write32(PLE_WINDOW, vmx->ple_window);</yellow>
	}

	/*
	 * We did this in prepare_switch_to_guest, because it needs to
	 * be within srcu_read_lock.
	 */
<blue>	WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);</blue>

<blue>	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RSP))</blue>
<blue>		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);</blue>
<blue>	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RIP))</blue>
<blue>		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);</blue>
<blue>	vcpu->arch.regs_dirty = 0;</blue>

	/*
	 * Refresh vmcs.HOST_CR3 if necessary.  This must be done immediately
	 * prior to VM-Enter, as the kernel may load a new ASID (PCID) any time
	 * it switches back to the current-&gt;mm, which can occur in KVM context
	 * when switching to a temporary mm to patch kernel code, e.g. if KVM
	 * toggles a static key while handling a VM-Exit.
	 */
	cr3 = __get_current_cr3_fast();
	if (unlikely(cr3 != vmx-&gt;loaded_vmcs-&gt;host_state.cr3)) {
<blue>		vmcs_writel(HOST_CR3, cr3);</blue>
<blue>		vmx->loaded_vmcs->host_state.cr3 = cr3;</blue>
	}

<blue>	cr4 = cr4_read_shadow();</blue>
	if (unlikely(cr4 != vmx-&gt;loaded_vmcs-&gt;host_state.cr4)) {
<blue>		vmcs_writel(HOST_CR4, cr4);</blue>
<blue>		vmx->loaded_vmcs->host_state.cr4 = cr4;</blue>
	}

	/* When KVM_DEBUGREG_WONT_EXIT, dr6 is accessible in guest. */
<blue>	if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))</blue>
<blue>		set_debugreg(vcpu->arch.dr6, 6);</blue>

	/* When single-stepping over STI and MOV SS, we must clear the
	 * corresponding interruptibility bits in the guest state. Otherwise
	 * vmentry fails as it then expects bit 14 (BS) in pending debug
	 * exceptions being set, but that&#x27;s not correct for the guest debugging
	 * case. */
<blue>	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)</blue>
<blue>		vmx_set_interrupt_shadow(vcpu, 0);</blue>

<blue>	kvm_load_guest_xsave_state(vcpu);</blue>

<yellow>	pt_guest_enter(vmx);</yellow>

<blue>	atomic_switch_perf_msrs(vmx);</blue>
<blue>	if (intel_pmu_lbr_is_enabled(vcpu))</blue>
<yellow>		vmx_passthrough_lbr_msrs(vcpu);</yellow>

<blue>	if (enable_preemption_timer)</blue>
<blue>		vmx_update_hv_timer(vcpu);</blue>

<blue>	kvm_wait_lapic_expire(vcpu);</blue>

	/* The actual VMENTER/EXIT is in the .noinstr.text section. */
	vmx_vcpu_enter_exit(vcpu, vmx, __vmx_vcpu_run_flags(vmx));

	/* All fields are clean at this point */
	if (static_branch_unlikely(&amp;enable_evmcs)) {
<yellow>		current_evmcs->hv_clean_fields |=</yellow>
			HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;

<yellow>		current_evmcs->hv_vp_id = kvm_hv_get_vpindex(vcpu);</yellow>
	}

	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
<blue>	if (vmx->host_debugctlmsr)</blue>
<yellow>		update_debugctlmsr(vmx->host_debugctlmsr);</yellow>

#ifndef CONFIG_X86_64
	/*
	 * The sysexit path does not restore ds/es, so we must set them to
	 * a reasonable value ourselves.
	 *
	 * We can&#x27;t defer this to vmx_prepare_switch_to_host() since that
	 * function may be executed in interrupt context, which saves and
	 * restore segments around it, nullifying its effect.
	 */
	loadsegment(ds, __USER_DS);
	loadsegment(es, __USER_DS);
#endif

<blue>	vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;</blue>

<yellow>	pt_guest_exit(vmx);</yellow>

	kvm_load_host_xsave_state(vcpu);

<blue>	if (is_guest_mode(vcpu)) {</blue>
		/*
		 * Track VMLAUNCH/VMRESUME that have made past guest state
		 * checking.
		 */
<blue>		if (vmx->nested.nested_run_pending &&</blue>
<blue>		    !vmx->exit_reason.failed_vmentry)</blue>
<blue>			++vcpu->stat.nested_run;</blue>

<blue>		vmx->nested.nested_run_pending = 0;</blue>
	}

<blue>	vmx->idt_vectoring_info = 0;</blue>

	if (unlikely(vmx-&gt;fail)) {
<blue>		vmx->exit_reason.full = 0xdead;</blue>
		return EXIT_FASTPATH_NONE;
	}

<blue>	vmx->exit_reason.full = vmcs_read32(VM_EXIT_REASON);</blue>
	if (unlikely((u16)vmx-&gt;exit_reason.basic == EXIT_REASON_MCE_DURING_VMENTRY))
		kvm_machine_check();

<blue>	if (likely(!vmx->exit_reason.failed_vmentry))</blue>
<blue>		vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);</blue>

<blue>	trace_kvm_exit(vcpu, KVM_ISA_VMX);</blue>

<blue>	if (unlikely(vmx->exit_reason.failed_vmentry))</blue>
		return EXIT_FASTPATH_NONE;

<blue>	vmx->loaded_vmcs->launched = 1;</blue>

<blue>	vmx_recover_nmi_blocking(vmx);</blue>
<blue>	vmx_complete_interrupts(vmx);</blue>

	if (is_guest_mode(vcpu))
		return EXIT_FASTPATH_NONE;

<blue>	return vmx_exit_handlers_fastpath(vcpu);</blue>
}

static void vmx_vcpu_free(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (enable_pml)</blue>
<blue>		vmx_destroy_pml_buffer(vmx);</blue>
<blue>	free_vpid(vmx->vpid);</blue>
	nested_vmx_free_vcpu(vcpu);
<blue>	free_loaded_vmcs(vmx->loaded_vmcs);</blue>
<blue>}</blue>

static int vmx_vcpu_create(struct kvm_vcpu *vcpu)
<blue>{</blue>
	struct vmx_uret_msr *tsx_ctrl;
	struct vcpu_vmx *vmx;
	int i, err;

	BUILD_BUG_ON(offsetof(struct vcpu_vmx, vcpu) != 0);
	vmx = to_vmx(vcpu);

<blue>	INIT_LIST_HEAD(&vmx->pi_wakeup_list);</blue>

	err = -ENOMEM;

	vmx-&gt;vpid = allocate_vpid();

	/*
	 * If PML is turned on, failure on enabling PML just results in failure
	 * of creating the vcpu, therefore we can simplify PML logic (by
	 * avoiding dealing with cases, such as enabling PML partially on vcpus
	 * for the guest), etc.
	 */
<blue>	if (enable_pml) {</blue>
<blue>		vmx->pml_pg = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);</blue>
		if (!vmx-&gt;pml_pg)
			goto free_vpid;
	}

<blue>	for (i = 0; i < kvm_nr_uret_msrs; ++i)</blue>
<blue>		vmx->guest_uret_msrs[i].mask = -1ull;</blue>
<blue>	if (boot_cpu_has(X86_FEATURE_RTM)) {</blue>
		/*
		 * TSX_CTRL_CPUID_CLEAR is handled in the CPUID interception.
		 * Keep the host value unchanged to avoid changing CPUID bits
		 * under the host kernel&#x27;s feet.
		 */
<yellow>		tsx_ctrl = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);</yellow>
		if (tsx_ctrl)
<yellow>			tsx_ctrl->mask = ~(u64)TSX_CTRL_CPUID_CLEAR;</yellow>
	}

<blue>	err = alloc_loaded_vmcs(&vmx->vmcs01);</blue>
	if (err &lt; 0)
		goto free_pml;

	/*
	 * Use Hyper-V &#x27;Enlightened MSR Bitmap&#x27; feature when KVM runs as a
	 * nested (L1) hypervisor and Hyper-V in L0 supports it. Enable the
	 * feature only for vmcs01, KVM currently isn&#x27;t equipped to realize any
	 * performance benefits from enabling it for vmcs02.
	 */
<blue>	if (IS_ENABLED(CONFIG_HYPERV) && static_branch_unlikely(&enable_evmcs) &&</blue>
<yellow>	    (ms_hyperv.nested_features & HV_X64_NESTED_MSR_BITMAP)) {</yellow>
<yellow>		struct hv_enlightened_vmcs *evmcs = (void *)vmx->vmcs01.vmcs;</yellow>

		evmcs-&gt;hv_enlightenments_control.msr_bitmap = 1;
	}

	/* The MSR bitmap starts with all ones */
<blue>	bitmap_fill(vmx->shadow_msr_intercept.read, MAX_POSSIBLE_PASSTHROUGH_MSRS);</blue>
	bitmap_fill(vmx-&gt;shadow_msr_intercept.write, MAX_POSSIBLE_PASSTHROUGH_MSRS);

	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_TSC, MSR_TYPE_R);
#ifdef CONFIG_X86_64
	vmx_disable_intercept_for_msr(vcpu, MSR_FS_BASE, MSR_TYPE_RW);
	vmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);
	vmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
#endif
	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
<blue>	if (kvm_cstate_in_guest(vcpu->kvm)) {</blue>
<blue>		vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C1_RES, MSR_TYPE_R);</blue>
		vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C3_RESIDENCY, MSR_TYPE_R);
		vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C6_RESIDENCY, MSR_TYPE_R);
		vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C7_RESIDENCY, MSR_TYPE_R);
	}

<blue>	vmx->loaded_vmcs = &vmx->vmcs01;</blue>

<blue>	if (cpu_need_virtualize_apic_accesses(vcpu)) {</blue>
<blue>		err = alloc_apic_access_page(vcpu->kvm);</blue>
		if (err)
			goto free_vmcs;
	}

<blue>	if (enable_ept && !enable_unrestricted_guest) {</blue>
<yellow>		err = init_rmode_identity_map(vcpu->kvm);</yellow>
		if (err)
			goto free_vmcs;
	}

<blue>	if (vmx_can_use_ipiv(vcpu))</blue>
<yellow>		WRITE_ONCE(to_kvm_vmx(vcpu->kvm)->pid_table[vcpu->vcpu_id],</yellow>
			   __pa(&amp;vmx-&gt;pi_desc) | PID_TABLE_ENTRY_VALID);

	return 0;

free_vmcs:
<yellow>	free_loaded_vmcs(vmx->loaded_vmcs);</yellow>
free_pml:
<yellow>	vmx_destroy_pml_buffer(vmx);</yellow>
free_vpid:
<yellow>	free_vpid(vmx->vpid);</yellow>
	return err;
}

#define L1TF_MSG_SMT &quot;L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n&quot;
#define L1TF_MSG_L1D &quot;L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n&quot;

<yellow>static int vmx_vm_init(struct kvm *kvm)</yellow>
{
<blue>	if (!ple_gap)</blue>
<blue>		kvm->arch.pause_in_guest = true;</blue>

<blue>	if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {</blue>
<yellow>		switch (l1tf_mitigation) {</yellow>
		case L1TF_MITIGATION_OFF:
		case L1TF_MITIGATION_FLUSH_NOWARN:
			/* &#x27;I explicitly don&#x27;t care&#x27; is set */
			break;
		case L1TF_MITIGATION_FLUSH:
		case L1TF_MITIGATION_FLUSH_NOSMT:
		case L1TF_MITIGATION_FULL:
			/*
			 * Warn upon starting the first VM in a potentially
			 * insecure environment.
			 */
<yellow>			if (sched_smt_active())</yellow>
<yellow>				pr_warn_once(L1TF_MSG_SMT);</yellow>
<yellow>			if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)</yellow>
<yellow>				pr_warn_once(L1TF_MSG_L1D);</yellow>
			break;
		case L1TF_MITIGATION_FULL_FORCE:
			/* Flush is enforced */
			break;
		}
	}
	return 0;
<blue>}</blue>

static int __init vmx_check_processor_compat(void)
<yellow>{</yellow>
	struct vmcs_config vmcs_conf;
	struct vmx_capability vmx_cap;

<yellow>	if (!this_cpu_has(X86_FEATURE_MSR_IA32_FEAT_CTL) ||</yellow>
	    !this_cpu_has(X86_FEATURE_VMX)) {
		pr_err(&quot;kvm: VMX is disabled on CPU %d\n&quot;, smp_processor_id());
<yellow>		return -EIO;</yellow>
	}

<yellow>	if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0)</yellow>
		return -EIO;
<yellow>	if (nested)</yellow>
<yellow>		nested_vmx_setup_ctls_msrs(&vmcs_conf, vmx_cap.ept);</yellow>
<yellow>	if (memcmp(&vmcs_config, &vmcs_conf, sizeof(struct vmcs_config)) != 0) {</yellow>
		printk(KERN_ERR &quot;kvm: CPU %d feature inconsistency!\n&quot;,
				smp_processor_id());
<yellow>		return -EIO;</yellow>
	}
	return 0;
}

static u8 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
{
	u8 cache;

	/* We wanted to honor guest CD/MTRR/PAT, but doing so could result in
	 * memory aliases with conflicting memory types and sometimes MCEs.
	 * We have to be careful as to what are honored and when.
	 *
	 * For MMIO, guest CD/MTRR are ignored.  The EPT memory type is set to
	 * UC.  The effective memory type is UC or WC depending on guest PAT.
	 * This was historically the source of MCEs and we want to be
	 * conservative.
	 *
	 * When there is no need to deal with noncoherent DMA (e.g., no VT-d
	 * or VT-d has snoop control), guest CD/MTRR/PAT are all ignored.  The
	 * EPT memory type is set to WB.  The effective memory type is forced
	 * WB.
	 *
	 * Otherwise, we trust guest.  Guest CD/MTRR/PAT are all honored.  The
	 * EPT memory type is used to emulate guest CD/MTRR.
	 */

<blue>	if (is_mmio)</blue>
		return MTRR_TYPE_UNCACHABLE &lt;&lt; VMX_EPT_MT_EPTE_SHIFT;

<blue>	if (!kvm_arch_has_noncoherent_dma(vcpu->kvm))</blue>
		return (MTRR_TYPE_WRBACK &lt;&lt; VMX_EPT_MT_EPTE_SHIFT) | VMX_EPT_IPAT_BIT;

<yellow>	if (kvm_read_cr0(vcpu) & X86_CR0_CD) {</yellow>
		if (kvm_check_has_quirk(vcpu-&gt;kvm, KVM_X86_QUIRK_CD_NW_CLEARED))
			cache = MTRR_TYPE_WRBACK;
		else
			cache = MTRR_TYPE_UNCACHABLE;

		return (cache &lt;&lt; VMX_EPT_MT_EPTE_SHIFT) | VMX_EPT_IPAT_BIT;
	}

<yellow>	return kvm_mtrr_get_guest_memory_type(vcpu, gfn) << VMX_EPT_MT_EPTE_SHIFT;</yellow>
<blue>}</blue>

static void vmcs_set_secondary_exec_control(struct vcpu_vmx *vmx, u32 new_ctl)
{
	/*
	 * These bits in the secondary execution controls field
	 * are dynamic, the others are mostly based on the hypervisor
	 * architecture and the guest&#x27;s CPUID.  Do not touch the
	 * dynamic bits.
	 */
	u32 mask =
		SECONDARY_EXEC_SHADOW_VMCS |
		SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
		SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
		SECONDARY_EXEC_DESC;

<blue>	u32 cur_ctl = secondary_exec_controls_get(vmx);</blue>

<blue>	secondary_exec_controls_set(vmx, (new_ctl & ~mask) | (cur_ctl & mask));</blue>
}

/*
 * Generate MSR_IA32_VMX_CR{0,4}_FIXED1 according to CPUID. Only set bits
 * (indicating &quot;allowed-1&quot;) if they are supported in the guest&#x27;s CPUID.
 */
static void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	struct kvm_cpuid_entry2 *entry;

<blue>	vmx->nested.msrs.cr0_fixed1 = 0xffffffff;</blue>
	vmx-&gt;nested.msrs.cr4_fixed1 = X86_CR4_PCE;

#define cr4_fixed1_update(_cr4_mask, _reg, _cpuid_mask) do {		\
	if (entry &amp;&amp; (entry-&gt;_reg &amp; (_cpuid_mask)))			\
		vmx-&gt;nested.msrs.cr4_fixed1 |= (_cr4_mask);	\
} while (0)

	entry = kvm_find_cpuid_entry(vcpu, 0x1);
<blue>	cr4_fixed1_update(X86_CR4_VME,        edx, feature_bit(VME));</blue>
<blue>	cr4_fixed1_update(X86_CR4_PVI,        edx, feature_bit(VME));</blue>
<blue>	cr4_fixed1_update(X86_CR4_TSD,        edx, feature_bit(TSC));</blue>
<blue>	cr4_fixed1_update(X86_CR4_DE,         edx, feature_bit(DE));</blue>
<blue>	cr4_fixed1_update(X86_CR4_PSE,        edx, feature_bit(PSE));</blue>
<blue>	cr4_fixed1_update(X86_CR4_PAE,        edx, feature_bit(PAE));</blue>
<blue>	cr4_fixed1_update(X86_CR4_MCE,        edx, feature_bit(MCE));</blue>
<blue>	cr4_fixed1_update(X86_CR4_PGE,        edx, feature_bit(PGE));</blue>
<blue>	cr4_fixed1_update(X86_CR4_OSFXSR,     edx, feature_bit(FXSR));</blue>
<blue>	cr4_fixed1_update(X86_CR4_OSXMMEXCPT, edx, feature_bit(XMM));</blue>
<blue>	cr4_fixed1_update(X86_CR4_VMXE,       ecx, feature_bit(VMX));</blue>
<blue>	cr4_fixed1_update(X86_CR4_SMXE,       ecx, feature_bit(SMX));</blue>
<blue>	cr4_fixed1_update(X86_CR4_PCIDE,      ecx, feature_bit(PCID));</blue>
<blue>	cr4_fixed1_update(X86_CR4_OSXSAVE,    ecx, feature_bit(XSAVE));</blue>

<blue>	entry = kvm_find_cpuid_entry_index(vcpu, 0x7, 0);</blue>
<blue>	cr4_fixed1_update(X86_CR4_FSGSBASE,   ebx, feature_bit(FSGSBASE));</blue>
<blue>	cr4_fixed1_update(X86_CR4_SMEP,       ebx, feature_bit(SMEP));</blue>
<blue>	cr4_fixed1_update(X86_CR4_SMAP,       ebx, feature_bit(SMAP));</blue>
<blue>	cr4_fixed1_update(X86_CR4_PKE,        ecx, feature_bit(PKU));</blue>
<blue>	cr4_fixed1_update(X86_CR4_UMIP,       ecx, feature_bit(UMIP));</blue>
<blue>	cr4_fixed1_update(X86_CR4_LA57,       ecx, feature_bit(LA57));</blue>

#undef cr4_fixed1_update
}

static void update_intel_pt_cfg(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	struct kvm_cpuid_entry2 *best = NULL;
	int i;

	for (i = 0; i &lt; PT_CPUID_LEAVES; i++) {
<yellow>		best = kvm_find_cpuid_entry_index(vcpu, 0x14, i);</yellow>
		if (!best)
			return;
<yellow>		vmx->pt_desc.caps[CPUID_EAX + i*PT_CPUID_REGS_NUM] = best->eax;</yellow>
		vmx-&gt;pt_desc.caps[CPUID_EBX + i*PT_CPUID_REGS_NUM] = best-&gt;ebx;
		vmx-&gt;pt_desc.caps[CPUID_ECX + i*PT_CPUID_REGS_NUM] = best-&gt;ecx;
		vmx-&gt;pt_desc.caps[CPUID_EDX + i*PT_CPUID_REGS_NUM] = best-&gt;edx;
	}

	/* Get the number of configurable Address Ranges for filtering */
	vmx-&gt;pt_desc.num_address_ranges = intel_pt_validate_cap(vmx-&gt;pt_desc.caps,
						PT_CAP_num_address_ranges);

	/* Initialize and clear the no dependency bits */
	vmx-&gt;pt_desc.ctl_bitmask = ~(RTIT_CTL_TRACEEN | RTIT_CTL_OS |
			RTIT_CTL_USR | RTIT_CTL_TSC_EN | RTIT_CTL_DISRETC |
			RTIT_CTL_BRANCH_EN);

	/*
	 * If CPUID.(EAX=14H,ECX=0):EBX[0]=1 CR3Filter can be set otherwise
	 * will inject an #GP
	 */
	if (intel_pt_validate_cap(vmx-&gt;pt_desc.caps, PT_CAP_cr3_filtering))
<yellow>		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_CR3EN;</yellow>

	/*
	 * If CPUID.(EAX=14H,ECX=0):EBX[1]=1 CYCEn, CycThresh and
	 * PSBFreq can be set
	 */
<yellow>	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc))</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_CYCLEACC |</yellow>
				RTIT_CTL_CYC_THRESH | RTIT_CTL_PSB_FREQ);

	/*
	 * If CPUID.(EAX=14H,ECX=0):EBX[3]=1 MTCEn and MTCFreq can be set
	 */
<yellow>	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc))</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_MTC_EN |</yellow>
					      RTIT_CTL_MTC_RANGE);

	/* If CPUID.(EAX=14H,ECX=0):EBX[4]=1 FUPonPTW and PTWEn can be set */
<yellow>	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_ptwrite))</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_FUP_ON_PTW |</yellow>
							RTIT_CTL_PTW_EN);

	/* If CPUID.(EAX=14H,ECX=0):EBX[5]=1 PwrEvEn can be set */
<yellow>	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_power_event_trace))</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_PWR_EVT_EN;</yellow>

	/* If CPUID.(EAX=14H,ECX=0):ECX[0]=1 ToPA can be set */
<yellow>	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_topa_output))</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_TOPA;</yellow>

	/* If CPUID.(EAX=14H,ECX=0):ECX[3]=1 FabricEn can be set */
<yellow>	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_output_subsys))</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_FABRIC_EN;</yellow>

	/* unmask address range configure area */
<yellow>	for (i = 0; i < vmx->pt_desc.num_address_ranges; i++)</yellow>
<yellow>		vmx->pt_desc.ctl_bitmask &= ~(0xfULL << (32 + i * 4));</yellow>
}

static void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	/* xsaves_enabled is recomputed in vmx_compute_secondary_exec_control(). */
<blue>	vcpu->arch.xsaves_enabled = false;</blue>

	vmx_setup_uret_msrs(vmx);

	if (cpu_has_secondary_exec_ctrls())
<blue>		vmcs_set_secondary_exec_control(vmx,</blue>
						vmx_secondary_exec_control(vmx));

<blue>	if (nested_vmx_allowed(vcpu))</blue>
<blue>		vmx->msr_ia32_feature_control_valid_bits |=</blue>
			FEAT_CTL_VMX_ENABLED_INSIDE_SMX |
			FEAT_CTL_VMX_ENABLED_OUTSIDE_SMX;
	else
<blue>		vmx->msr_ia32_feature_control_valid_bits &=</blue>
			~(FEAT_CTL_VMX_ENABLED_INSIDE_SMX |
			  FEAT_CTL_VMX_ENABLED_OUTSIDE_SMX);

	if (nested_vmx_allowed(vcpu))
<blue>		nested_vmx_cr_fixed1_bits_update(vcpu);</blue>

<blue>	if (boot_cpu_has(X86_FEATURE_INTEL_PT) &&</blue>
<yellow>			guest_cpuid_has(vcpu, X86_FEATURE_INTEL_PT))</yellow>
<yellow>		update_intel_pt_cfg(vcpu);</yellow>

<blue>	if (boot_cpu_has(X86_FEATURE_RTM)) {</blue>
		struct vmx_uret_msr *msr;
<yellow>		msr = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);</yellow>
		if (msr) {
<yellow>			bool enabled = guest_cpuid_has(vcpu, X86_FEATURE_RTM);</yellow>
<yellow>			vmx_set_guest_uret_msr(vmx, msr, enabled ? 0 : TSX_CTRL_RTM_DISABLE);</yellow>
		}
	}

<blue>	if (kvm_cpu_cap_has(X86_FEATURE_XFD))</blue>
<yellow>		vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,</yellow>
<yellow>					  !guest_cpuid_has(vcpu, X86_FEATURE_XFD));</yellow>


<blue>	set_cr4_guest_host_mask(vmx);</blue>

	vmx_write_encls_bitmap(vcpu, NULL);
<blue>	if (guest_cpuid_has(vcpu, X86_FEATURE_SGX))</blue>
<blue>		vmx->msr_ia32_feature_control_valid_bits |= FEAT_CTL_SGX_ENABLED;</blue>
	else
<blue>		vmx->msr_ia32_feature_control_valid_bits &= ~FEAT_CTL_SGX_ENABLED;</blue>

<blue>	if (guest_cpuid_has(vcpu, X86_FEATURE_SGX_LC))</blue>
<blue>		vmx->msr_ia32_feature_control_valid_bits |=</blue>
			FEAT_CTL_SGX_LC_ENABLED;
	else
<blue>		vmx->msr_ia32_feature_control_valid_bits &=</blue>
			~FEAT_CTL_SGX_LC_ENABLED;

	/* Refresh #PF interception to account for MAXPHYADDR changes. */
	vmx_update_exception_bitmap(vcpu);
}

static __init void vmx_set_cpu_caps(void)
{
<yellow>	kvm_set_cpu_caps();</yellow>

	/* CPUID 0x1 */
<yellow>	if (nested)</yellow>
<yellow>		kvm_cpu_cap_set(X86_FEATURE_VMX);</yellow>

	/* CPUID 0x7 */
<yellow>	if (kvm_mpx_supported())</yellow>
<yellow>		kvm_cpu_cap_check_and_set(X86_FEATURE_MPX);</yellow>
<yellow>	if (!cpu_has_vmx_invpcid())</yellow>
<yellow>		kvm_cpu_cap_clear(X86_FEATURE_INVPCID);</yellow>
<yellow>	if (vmx_pt_mode_is_host_guest())</yellow>
<yellow>		kvm_cpu_cap_check_and_set(X86_FEATURE_INTEL_PT);</yellow>
<yellow>	if (vmx_pebs_supported()) {</yellow>
<yellow>		kvm_cpu_cap_check_and_set(X86_FEATURE_DS);</yellow>
<yellow>		kvm_cpu_cap_check_and_set(X86_FEATURE_DTES64);</yellow>
	}

<yellow>	if (!enable_pmu)</yellow>
<yellow>		kvm_cpu_cap_clear(X86_FEATURE_PDCM);</yellow>

<yellow>	if (!enable_sgx) {</yellow>
<yellow>		kvm_cpu_cap_clear(X86_FEATURE_SGX);</yellow>
		kvm_cpu_cap_clear(X86_FEATURE_SGX_LC);
		kvm_cpu_cap_clear(X86_FEATURE_SGX1);
		kvm_cpu_cap_clear(X86_FEATURE_SGX2);
	}

<yellow>	if (vmx_umip_emulated())</yellow>
<yellow>		kvm_cpu_cap_set(X86_FEATURE_UMIP);</yellow>

	/* CPUID 0xD.1 */
<yellow>	kvm_caps.supported_xss = 0;</yellow>
	if (!cpu_has_vmx_xsaves())
<yellow>		kvm_cpu_cap_clear(X86_FEATURE_XSAVES);</yellow>

	/* CPUID 0x80000001 and 0x7 (RDPID) */
<yellow>	if (!cpu_has_vmx_rdtscp()) {</yellow>
<yellow>		kvm_cpu_cap_clear(X86_FEATURE_RDTSCP);</yellow>
		kvm_cpu_cap_clear(X86_FEATURE_RDPID);
	}

<yellow>	if (cpu_has_vmx_waitpkg())</yellow>
<yellow>		kvm_cpu_cap_check_and_set(X86_FEATURE_WAITPKG);</yellow>
}

static void vmx_request_immediate_exit(struct kvm_vcpu *vcpu)
{
<blue>	to_vmx(vcpu)->req_immediate_exit = true;</blue>
}

static int vmx_check_intercept_io(struct kvm_vcpu *vcpu,
				  struct x86_instruction_info *info)
{
	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
	unsigned short port;
	bool intercept;
	int size;

<blue>	if (info->intercept == x86_intercept_in ||</blue>
	    info-&gt;intercept == x86_intercept_ins) {
<yellow>		port = info->src_val;</yellow>
		size = info-&gt;dst_bytes;
	} else {
<blue>		port = info->dst_val;</blue>
		size = info-&gt;src_bytes;
	}

	/*
	 * If the &#x27;use IO bitmaps&#x27; VM-execution control is 0, IO instruction
	 * VM-exits depend on the &#x27;unconditional IO exiting&#x27; VM-execution
	 * control.
	 *
	 * Otherwise, IO instruction VM-exits are controlled by the IO bitmaps.
	 */
<blue>	if (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))</blue>
<blue>		intercept = nested_cpu_has(vmcs12,</blue>
					   CPU_BASED_UNCOND_IO_EXITING);
	else
<yellow>		intercept = nested_vmx_check_io_bitmaps(vcpu, port, size);</yellow>

	/* FIXME: produce nested vmexit and return X86EMUL_INTERCEPTED.  */
<blue>	return intercept ? X86EMUL_UNHANDLEABLE : X86EMUL_CONTINUE;</blue>
}

static int vmx_check_intercept(struct kvm_vcpu *vcpu,
			       struct x86_instruction_info *info,
			       enum x86_intercept_stage stage,
			       struct x86_exception *exception)
{
<blue>	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</blue>

	switch (info-&gt;intercept) {
	/*
	 * RDPID causes #UD if disabled through secondary execution controls.
	 * Because it is marked as EmulateOnUD, we need to intercept it here.
	 * Note, RDPID is hidden behind ENABLE_RDTSCP.
	 */
	case x86_intercept_rdpid:
<yellow>		if (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_RDTSCP)) {</yellow>
<yellow>			exception->vector = UD_VECTOR;</yellow>
			exception-&gt;error_code_valid = false;
			return X86EMUL_PROPAGATE_FAULT;
		}
		break;

	case x86_intercept_in:
	case x86_intercept_ins:
	case x86_intercept_out:
	case x86_intercept_outs:
<blue>		return vmx_check_intercept_io(vcpu, info);</blue>

	case x86_intercept_lgdt:
	case x86_intercept_lidt:
	case x86_intercept_lldt:
	case x86_intercept_ltr:
	case x86_intercept_sgdt:
	case x86_intercept_sidt:
	case x86_intercept_sldt:
	case x86_intercept_str:
<blue>		if (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC))</blue>
			return X86EMUL_CONTINUE;

		/* FIXME: produce nested vmexit and return X86EMUL_INTERCEPTED.  */
		break;

	/* TODO: check more intercepts... */
	default:
		break;
	}

	return X86EMUL_UNHANDLEABLE;
}

#ifdef CONFIG_X86_64
/* (a &lt;&lt; shift) / divisor, return 1 if overflow otherwise 0 */
static inline int u64_shl_div_u64(u64 a, unsigned int shift,
				  u64 divisor, u64 *result)
{
<blue>	u64 low = a << shift, high = a >> (64 - shift);</blue>

	/* To avoid the overflow on divq */
	if (high &gt;= divisor)
		return 1;

	/* Low hold the result, high hold rem which is discarded */
	asm(&quot;divq %2\n\t&quot; : &quot;=a&quot; (low), &quot;=d&quot; (high) :
	    &quot;rm&quot; (divisor), &quot;0&quot; (low), &quot;1&quot; (high));
	*result = low;

	return 0;
}

static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,
			    bool *expired)
{
	struct vcpu_vmx *vmx;
	u64 tscl, guest_tscl, delta_tsc, lapic_timer_advance_cycles;
<blue>	struct kvm_timer *ktimer = &vcpu->arch.apic->lapic_timer;</blue>

	vmx = to_vmx(vcpu);
	tscl = rdtsc();
	guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
	delta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;
<blue>	lapic_timer_advance_cycles = nsec_to_cycles(vcpu,</blue>
						    ktimer-&gt;timer_advance_ns);

	if (delta_tsc &gt; lapic_timer_advance_cycles)
		delta_tsc -= lapic_timer_advance_cycles;
	else
		delta_tsc = 0;

	/* Convert to host delta tsc if tsc scaling is enabled */
<blue>	if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&</blue>
<blue>	    delta_tsc && u64_shl_div_u64(delta_tsc,</blue>
<blue>				kvm_caps.tsc_scaling_ratio_frac_bits,</blue>
				vcpu-&gt;arch.l1_tsc_scaling_ratio, &amp;delta_tsc))
		return -ERANGE;

	/*
	 * If the delta tsc can&#x27;t fit in the 32 bit after the multi shift,
	 * we can&#x27;t use the preemption timer.
	 * It&#x27;s possible that it fits on later vmentries, but checking
	 * on every vmentry is costly so we just use an hrtimer.
	 */
<blue>	if (delta_tsc >> (cpu_preemption_timer_multi + 32))</blue>
		return -ERANGE;

<blue>	vmx->hv_deadline_tsc = tscl + delta_tsc;</blue>
<blue>	*expired = !delta_tsc;</blue>
	return 0;
<blue>}</blue>

static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
{
<blue>	to_vmx(vcpu)->hv_deadline_tsc = -1;</blue>
}
#endif

static void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)
{
<blue>	if (!kvm_pause_in_guest(vcpu->kvm))</blue>
<yellow>		shrink_ple_window(vcpu);</yellow>
<blue>}</blue>

<blue>void vmx_update_cpu_dirty_logging(struct kvm_vcpu *vcpu)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

<blue>	if (is_guest_mode(vcpu)) {</blue>
<blue>		vmx->nested.update_vmcs01_cpu_dirty_logging = true;</blue>
		return;
	}

	/*
	 * Note, cpu_dirty_logging_count can be changed concurrent with this
	 * code, but in that case another update request will be made and so
	 * the guest will never run with a stale PML value.
	 */
<blue>	if (vcpu->kvm->arch.cpu_dirty_logging_count)</blue>
<blue>		secondary_exec_controls_setbit(vmx, SECONDARY_EXEC_ENABLE_PML);</blue>
	else
<blue>		secondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_ENABLE_PML);</blue>
<blue>}</blue>

static void vmx_setup_mce(struct kvm_vcpu *vcpu)
{
<blue>	if (vcpu->arch.mcg_cap & MCG_LMCE_P)</blue>
<blue>		to_vmx(vcpu)->msr_ia32_feature_control_valid_bits |=</blue>
			FEAT_CTL_LMCE_ENABLED;
	else
<blue>		to_vmx(vcpu)->msr_ia32_feature_control_valid_bits &=</blue>
			~FEAT_CTL_LMCE_ENABLED;
}

static int vmx_smi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
{
	/* we need a nested vmexit to enter SMM, postpone if run is pending */
<blue>	if (to_vmx(vcpu)->nested.nested_run_pending)</blue>
		return -EBUSY;
<blue>	return !is_smm(vcpu);</blue>
<blue>}</blue>

static int vmx_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);

	/*
	 * TODO: Implement custom flows for forcing the vCPU out/in of L2 on
	 * SMI and RSM.  Using the common VM-Exit + VM-Enter routines is wrong
	 * SMI and RSM only modify state that is saved and restored via SMRAM.
	 * E.g. most MSRs are left untouched, but many are modified by VM-Exit
	 * and VM-Enter, and thus L2&#x27;s values may be corrupted on SMI+RSM.
	 */
<blue>	vmx->nested.smm.guest_mode = is_guest_mode(vcpu);</blue>
<blue>	if (vmx->nested.smm.guest_mode)</blue>
<blue>		nested_vmx_vmexit(vcpu, -1, 0, 0);</blue>

<blue>	vmx->nested.smm.vmxon = vmx->nested.vmxon;</blue>
	vmx-&gt;nested.vmxon = false;
<blue>	vmx_clear_hlt(vcpu);</blue>
	return 0;
<blue>}</blue>

<blue>static int vmx_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)</blue>
{
	struct vcpu_vmx *vmx = to_vmx(vcpu);
	int ret;

<blue>	if (vmx->nested.smm.vmxon) {</blue>
<blue>		vmx->nested.vmxon = true;</blue>
		vmx-&gt;nested.smm.vmxon = false;
	}

<blue>	if (vmx->nested.smm.guest_mode) {</blue>
<blue>		ret = nested_vmx_enter_non_root_mode(vcpu, false);</blue>
		if (ret)
			return ret;

<blue>		vmx->nested.nested_run_pending = 1;</blue>
		vmx-&gt;nested.smm.guest_mode = false;
	}
	return 0;
<blue>}</blue>

static void vmx_enable_smi_window(struct kvm_vcpu *vcpu)
{
	/* RSM will cause a vmexit anyway.  */
<blue>}</blue>

static bool vmx_apic_init_signal_blocked(struct kvm_vcpu *vcpu)
{
<blue>	return to_vmx(vcpu)->nested.vmxon && !is_guest_mode(vcpu);</blue>
<blue>}</blue>

<blue>static void vmx_migrate_timers(struct kvm_vcpu *vcpu)</blue>
{
<blue>	if (is_guest_mode(vcpu)) {</blue>
<blue>		struct hrtimer *timer = &to_vmx(vcpu)->nested.preemption_timer;</blue>

		if (hrtimer_try_to_cancel(timer) == 1)
<yellow>			hrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED);</yellow>
	}
<blue>}</blue>

static void vmx_hardware_unsetup(void)
{
<yellow>	kvm_set_posted_intr_wakeup_handler(NULL);</yellow>

<yellow>	if (nested)</yellow>
<yellow>		nested_vmx_hardware_unsetup();</yellow>

<yellow>	free_kvm_area();</yellow>
}

static bool vmx_check_apicv_inhibit_reasons(enum kvm_apicv_inhibit reason)
{
	ulong supported = BIT(APICV_INHIBIT_REASON_DISABLE) |
			  BIT(APICV_INHIBIT_REASON_ABSENT) |
			  BIT(APICV_INHIBIT_REASON_HYPERV) |
			  BIT(APICV_INHIBIT_REASON_BLOCKIRQ) |
			  BIT(APICV_INHIBIT_REASON_APIC_ID_MODIFIED) |
			  BIT(APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);

<blue>	return supported & BIT(reason);</blue>
}

static void vmx_vm_destroy(struct kvm *kvm)
{
	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);

<yellow>	free_pages((unsigned long)kvm_vmx->pid_table, vmx_get_pid_table_order(kvm));</yellow>
}

static struct kvm_x86_ops vmx_x86_ops __initdata = {
	.name = &quot;kvm_intel&quot;,

	.hardware_unsetup = vmx_hardware_unsetup,

	.hardware_enable = vmx_hardware_enable,
	.hardware_disable = vmx_hardware_disable,
	.has_emulated_msr = vmx_has_emulated_msr,

	.vm_size = sizeof(struct kvm_vmx),
	.vm_init = vmx_vm_init,
	.vm_destroy = vmx_vm_destroy,

	.vcpu_precreate = vmx_vcpu_precreate,
	.vcpu_create = vmx_vcpu_create,
	.vcpu_free = vmx_vcpu_free,
	.vcpu_reset = vmx_vcpu_reset,

	.prepare_switch_to_guest = vmx_prepare_switch_to_guest,
	.vcpu_load = vmx_vcpu_load,
	.vcpu_put = vmx_vcpu_put,

	.update_exception_bitmap = vmx_update_exception_bitmap,
	.get_msr_feature = vmx_get_msr_feature,
	.get_msr = vmx_get_msr,
	.set_msr = vmx_set_msr,
	.get_segment_base = vmx_get_segment_base,
	.get_segment = vmx_get_segment,
	.set_segment = vmx_set_segment,
	.get_cpl = vmx_get_cpl,
	.get_cs_db_l_bits = vmx_get_cs_db_l_bits,
	.set_cr0 = vmx_set_cr0,
	.is_valid_cr4 = vmx_is_valid_cr4,
	.set_cr4 = vmx_set_cr4,
	.set_efer = vmx_set_efer,
	.get_idt = vmx_get_idt,
	.set_idt = vmx_set_idt,
	.get_gdt = vmx_get_gdt,
	.set_gdt = vmx_set_gdt,
	.set_dr7 = vmx_set_dr7,
	.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs,
	.cache_reg = vmx_cache_reg,
	.get_rflags = vmx_get_rflags,
	.set_rflags = vmx_set_rflags,
	.get_if_flag = vmx_get_if_flag,

	.flush_tlb_all = vmx_flush_tlb_all,
	.flush_tlb_current = vmx_flush_tlb_current,
	.flush_tlb_gva = vmx_flush_tlb_gva,
	.flush_tlb_guest = vmx_flush_tlb_guest,

	.vcpu_pre_run = vmx_vcpu_pre_run,
	.vcpu_run = vmx_vcpu_run,
	.handle_exit = vmx_handle_exit,
	.skip_emulated_instruction = vmx_skip_emulated_instruction,
	.update_emulated_instruction = vmx_update_emulated_instruction,
	.set_interrupt_shadow = vmx_set_interrupt_shadow,
	.get_interrupt_shadow = vmx_get_interrupt_shadow,
	.patch_hypercall = vmx_patch_hypercall,
	.inject_irq = vmx_inject_irq,
	.inject_nmi = vmx_inject_nmi,
	.inject_exception = vmx_inject_exception,
	.cancel_injection = vmx_cancel_injection,
	.interrupt_allowed = vmx_interrupt_allowed,
	.nmi_allowed = vmx_nmi_allowed,
	.get_nmi_mask = vmx_get_nmi_mask,
	.set_nmi_mask = vmx_set_nmi_mask,
	.enable_nmi_window = vmx_enable_nmi_window,
	.enable_irq_window = vmx_enable_irq_window,
	.update_cr8_intercept = vmx_update_cr8_intercept,
	.set_virtual_apic_mode = vmx_set_virtual_apic_mode,
	.set_apic_access_page_addr = vmx_set_apic_access_page_addr,
	.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl,
	.load_eoi_exitmap = vmx_load_eoi_exitmap,
	.apicv_post_state_restore = vmx_apicv_post_state_restore,
	.check_apicv_inhibit_reasons = vmx_check_apicv_inhibit_reasons,
	.hwapic_irr_update = vmx_hwapic_irr_update,
	.hwapic_isr_update = vmx_hwapic_isr_update,
	.guest_apic_has_interrupt = vmx_guest_apic_has_interrupt,
	.sync_pir_to_irr = vmx_sync_pir_to_irr,
	.deliver_interrupt = vmx_deliver_interrupt,
	.dy_apicv_has_pending_interrupt = pi_has_pending_interrupt,

	.set_tss_addr = vmx_set_tss_addr,
	.set_identity_map_addr = vmx_set_identity_map_addr,
	.get_mt_mask = vmx_get_mt_mask,

	.get_exit_info = vmx_get_exit_info,

	.vcpu_after_set_cpuid = vmx_vcpu_after_set_cpuid,

	.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit,

	.get_l2_tsc_offset = vmx_get_l2_tsc_offset,
	.get_l2_tsc_multiplier = vmx_get_l2_tsc_multiplier,
	.write_tsc_offset = vmx_write_tsc_offset,
	.write_tsc_multiplier = vmx_write_tsc_multiplier,

	.load_mmu_pgd = vmx_load_mmu_pgd,

	.check_intercept = vmx_check_intercept,
	.handle_exit_irqoff = vmx_handle_exit_irqoff,

	.request_immediate_exit = vmx_request_immediate_exit,

	.sched_in = vmx_sched_in,

	.cpu_dirty_log_size = PML_ENTITY_NUM,
	.update_cpu_dirty_logging = vmx_update_cpu_dirty_logging,

	.nested_ops = &amp;vmx_nested_ops,

	.pi_update_irte = vmx_pi_update_irte,
	.pi_start_assignment = vmx_pi_start_assignment,

#ifdef CONFIG_X86_64
	.set_hv_timer = vmx_set_hv_timer,
	.cancel_hv_timer = vmx_cancel_hv_timer,
#endif

	.setup_mce = vmx_setup_mce,

	.smi_allowed = vmx_smi_allowed,
	.enter_smm = vmx_enter_smm,
	.leave_smm = vmx_leave_smm,
	.enable_smi_window = vmx_enable_smi_window,

	.can_emulate_instruction = vmx_can_emulate_instruction,
	.apic_init_signal_blocked = vmx_apic_init_signal_blocked,
	.migrate_timers = vmx_migrate_timers,

	.msr_filter_changed = vmx_msr_filter_changed,
	.complete_emulated_msr = kvm_complete_insn_gp,

	.vcpu_deliver_sipi_vector = kvm_vcpu_deliver_sipi_vector,
};

static unsigned int vmx_handle_intel_pt_intr(void)
{
<yellow>	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();</yellow>

	/* &#x27;0&#x27; on failure so that the !PT case can use a RET0 static call. */
<yellow>	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))</yellow>
<yellow>		return 0;</yellow>

<yellow>	kvm_make_request(KVM_REQ_PMI, vcpu);</yellow>
	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
		  (unsigned long *)&amp;vcpu-&gt;arch.pmu.global_status);
<yellow>	return 1;</yellow>
}

static __init void vmx_setup_user_return_msrs(void)
{

	/*
	 * Though SYSCALL is only supported in 64-bit mode on Intel CPUs, kvm
	 * will emulate SYSCALL in legacy mode if the vendor string in guest
	 * CPUID.0:{EBX,ECX,EDX} is &quot;AuthenticAMD&quot; or &quot;AMDisbetter!&quot; To
	 * support this emulation, MSR_STAR is included in the list for i386,
	 * but is never loaded into hardware.  MSR_CSTAR is also never loaded
	 * into hardware and is here purely for emulation purposes.
	 */
	const u32 vmx_uret_msrs_list[] = {
	#ifdef CONFIG_X86_64
		MSR_SYSCALL_MASK, MSR_LSTAR, MSR_CSTAR,
	#endif
		MSR_EFER, MSR_TSC_AUX, MSR_STAR,
		MSR_IA32_TSX_CTRL,
	};
	int i;

	BUILD_BUG_ON(ARRAY_SIZE(vmx_uret_msrs_list) != MAX_NR_USER_RETURN_MSRS);

	for (i = 0; i &lt; ARRAY_SIZE(vmx_uret_msrs_list); ++i)
<yellow>		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);</yellow>
}

static void __init vmx_setup_me_spte_mask(void)
{
	u64 me_mask = 0;

	/*
	 * kvm_get_shadow_phys_bits() returns shadow_phys_bits.  Use
	 * the former to avoid exposing shadow_phys_bits.
	 *
	 * On pre-MKTME system, boot_cpu_data.x86_phys_bits equals to
	 * shadow_phys_bits.  On MKTME and/or TDX capable systems,
	 * boot_cpu_data.x86_phys_bits holds the actual physical address
	 * w/o the KeyID bits, and shadow_phys_bits equals to MAXPHYADDR
	 * reported by CPUID.  Those bits between are KeyID bits.
	 */
<yellow>	if (boot_cpu_data.x86_phys_bits != kvm_get_shadow_phys_bits())</yellow>
<yellow>		me_mask = rsvd_bits(boot_cpu_data.x86_phys_bits,</yellow>
<yellow>			kvm_get_shadow_phys_bits() - 1);</yellow>
	/*
	 * Unlike SME, host kernel doesn&#x27;t support setting up any
	 * MKTME KeyID on Intel platforms.  No memory encryption
	 * bits should be included into the SPTE.
	 */
<yellow>	kvm_mmu_set_me_spte_mask(0, me_mask);</yellow>
}

static struct kvm_x86_init_ops vmx_init_ops __initdata;

static __init int hardware_setup(void)
<yellow>{</yellow>
	unsigned long host_bndcfgs;
	struct desc_ptr dt;
	int r;

<yellow>	store_idt(&dt);</yellow>
	host_idt_base = dt.address;

<yellow>	vmx_setup_user_return_msrs();</yellow>

<yellow>	if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)</yellow>
		return -EIO;

<yellow>	if (cpu_has_perf_global_ctrl_bug())</yellow>
<yellow>		pr_warn_once("kvm: VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL "</yellow>
			     &quot;does not work properly. Using workaround\n&quot;);

<yellow>	if (boot_cpu_has(X86_FEATURE_NX))</yellow>
<yellow>		kvm_enable_efer_bits(EFER_NX);</yellow>

<yellow>	if (boot_cpu_has(X86_FEATURE_MPX)) {</yellow>
<yellow>		rdmsrl(MSR_IA32_BNDCFGS, host_bndcfgs);</yellow>
<yellow>		WARN_ONCE(host_bndcfgs, "KVM: BNDCFGS in host will be lost");</yellow>
	}

<yellow>	if (!cpu_has_vmx_mpx())</yellow>
<yellow>		kvm_caps.supported_xcr0 &= ~(XFEATURE_MASK_BNDREGS |</yellow>
					     XFEATURE_MASK_BNDCSR);

<yellow>	if (!cpu_has_vmx_vpid() || !cpu_has_vmx_invvpid() ||</yellow>
	    !(cpu_has_vmx_invvpid_single() || cpu_has_vmx_invvpid_global()))
<yellow>		enable_vpid = 0;</yellow>

<yellow>	if (!cpu_has_vmx_ept() ||</yellow>
<yellow>	    !cpu_has_vmx_ept_4levels() ||</yellow>
	    !cpu_has_vmx_ept_mt_wb() ||
	    !cpu_has_vmx_invept_global())
<yellow>		enable_ept = 0;</yellow>

	/* NX support is required for shadow paging. */
<yellow>	if (!enable_ept && !boot_cpu_has(X86_FEATURE_NX)) {</yellow>
<yellow>		pr_err_ratelimited("kvm: NX (Execute Disable) not supported\n");</yellow>
		return -EOPNOTSUPP;
	}

<yellow>	if (!cpu_has_vmx_ept_ad_bits() || !enable_ept)</yellow>
<yellow>		enable_ept_ad_bits = 0;</yellow>

<yellow>	if (!cpu_has_vmx_unrestricted_guest() || !enable_ept)</yellow>
<yellow>		enable_unrestricted_guest = 0;</yellow>

<yellow>	if (!cpu_has_vmx_flexpriority())</yellow>
<yellow>		flexpriority_enabled = 0;</yellow>

<yellow>	if (!cpu_has_virtual_nmis())</yellow>
<yellow>		enable_vnmi = 0;</yellow>

#ifdef CONFIG_X86_SGX_KVM
<yellow>	if (!cpu_has_vmx_encls_vmexit())</yellow>
<yellow>		enable_sgx = false;</yellow>
#endif

	/*
	 * set_apic_access_page_addr() is used to reload apic access
	 * page upon invalidation.  No need to do anything if not
	 * using the APIC_ACCESS_ADDR VMCS field.
	 */
<yellow>	if (!flexpriority_enabled)</yellow>
<yellow>		vmx_x86_ops.set_apic_access_page_addr = NULL;</yellow>

<yellow>	if (!cpu_has_vmx_tpr_shadow())</yellow>
<yellow>		vmx_x86_ops.update_cr8_intercept = NULL;</yellow>

#if IS_ENABLED(CONFIG_HYPERV)
<yellow>	if (ms_hyperv.nested_features & HV_X64_NESTED_GUEST_MAPPING_FLUSH</yellow>
<yellow>	    && enable_ept) {</yellow>
<yellow>		vmx_x86_ops.tlb_remote_flush = hv_remote_flush_tlb;</yellow>
		vmx_x86_ops.tlb_remote_flush_with_range =
				hv_remote_flush_tlb_with_range;
	}
#endif

<yellow>	if (!cpu_has_vmx_ple()) {</yellow>
<yellow>		ple_gap = 0;</yellow>
		ple_window = 0;
		ple_window_grow = 0;
		ple_window_max = 0;
		ple_window_shrink = 0;
	}

<yellow>	if (!cpu_has_vmx_apicv())</yellow>
<yellow>		enable_apicv = 0;</yellow>
<yellow>	if (!enable_apicv)</yellow>
<yellow>		vmx_x86_ops.sync_pir_to_irr = NULL;</yellow>

<yellow>	if (!enable_apicv || !cpu_has_vmx_ipiv())</yellow>
<yellow>		enable_ipiv = false;</yellow>

<yellow>	if (cpu_has_vmx_tsc_scaling())</yellow>
<yellow>		kvm_caps.has_tsc_control = true;</yellow>

<yellow>	kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;</yellow>
	kvm_caps.tsc_scaling_ratio_frac_bits = 48;
	kvm_caps.has_bus_lock_exit = cpu_has_vmx_bus_lock_detection();
	kvm_caps.has_notify_vmexit = cpu_has_notify_vmexit();

	set_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */

<yellow>	if (enable_ept)</yellow>
<yellow>		kvm_mmu_set_ept_masks(enable_ept_ad_bits,</yellow>
				      cpu_has_vmx_ept_execute_only());

	/*
	 * Setup shadow_me_value/shadow_me_mask to include MKTME KeyID
	 * bits to shadow_zero_check.
	 */
<yellow>	vmx_setup_me_spte_mask();</yellow>

<yellow>	kvm_configure_mmu(enable_ept, 0, vmx_get_max_tdp_level(),</yellow>
			  ept_caps_to_lpage_level(vmx_capability.ept));

	/*
	 * Only enable PML when hardware supports PML feature, and both EPT
	 * and EPT A/D bit features are enabled -- PML depends on them to work.
	 */
<yellow>	if (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())</yellow>
<yellow>		enable_pml = 0;</yellow>

<yellow>	if (!enable_pml)</yellow>
<yellow>		vmx_x86_ops.cpu_dirty_log_size = 0;</yellow>

<yellow>	if (!cpu_has_vmx_preemption_timer())</yellow>
<yellow>		enable_preemption_timer = false;</yellow>

<yellow>	if (enable_preemption_timer) {</yellow>
		u64 use_timer_freq = 5000ULL * 1000 * 1000;

		cpu_preemption_timer_multi =
			vmcs_config.misc &amp; VMX_MISC_PREEMPTION_TIMER_RATE_MASK;

<yellow>		if (tsc_khz)</yellow>
<yellow>			use_timer_freq = (u64)tsc_khz * 1000;</yellow>
<yellow>		use_timer_freq >>= cpu_preemption_timer_multi;</yellow>

		/*
		 * KVM &quot;disables&quot; the preemption timer by setting it to its max
		 * value.  Don&#x27;t use the timer if it might cause spurious exits
		 * at a rate faster than 0.1 Hz (of uninterrupted guest time).
		 */
		if (use_timer_freq &gt; 0xffffffffu / 10)
<yellow>			enable_preemption_timer = false;</yellow>
	}

<yellow>	if (!enable_preemption_timer) {</yellow>
<yellow>		vmx_x86_ops.set_hv_timer = NULL;</yellow>
		vmx_x86_ops.cancel_hv_timer = NULL;
		vmx_x86_ops.request_immediate_exit = __kvm_request_immediate_exit;
	}

<yellow>	kvm_caps.supported_mce_cap |= MCG_LMCE_P;</yellow>
	kvm_caps.supported_mce_cap |= MCG_CMCI_P;

	if (pt_mode != PT_MODE_SYSTEM &amp;&amp; pt_mode != PT_MODE_HOST_GUEST)
		return -EINVAL;
<yellow>	if (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())</yellow>
		pt_mode = PT_MODE_SYSTEM;
<yellow>	if (pt_mode == PT_MODE_HOST_GUEST)</yellow>
		vmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;
	else
<yellow>		vmx_init_ops.handle_intel_pt_intr = NULL;</yellow>

	setup_default_sgx_lepubkeyhash();

<yellow>	if (nested) {</yellow>
<yellow>		nested_vmx_setup_ctls_msrs(&vmcs_config, vmx_capability.ept);</yellow>

		r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
		if (r)
			return r;
	}

<yellow>	vmx_set_cpu_caps();</yellow>

<yellow>	r = alloc_kvm_area();</yellow>
<yellow>	if (r && nested)</yellow>
<yellow>		nested_vmx_hardware_unsetup();</yellow>

<yellow>	kvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);</yellow>

	return r;
}

static struct kvm_x86_init_ops vmx_init_ops __initdata = {
	.cpu_has_kvm_support = cpu_has_kvm_support,
	.disabled_by_bios = vmx_disabled_by_bios,
	.check_processor_compatibility = vmx_check_processor_compat,
	.hardware_setup = hardware_setup,
	.handle_intel_pt_intr = NULL,

	.runtime_ops = &amp;vmx_x86_ops,
	.pmu_ops = &amp;intel_pmu_ops,
};

static void vmx_cleanup_l1d_flush(void)
{
<yellow>	if (vmx_l1d_flush_pages) {</yellow>
<yellow>		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);</yellow>
		vmx_l1d_flush_pages = NULL;
	}
	/* Restore state so sysfs ignores VMX */
<yellow>	l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;</yellow>
}

static void vmx_exit(void)
{
#ifdef CONFIG_KEXEC_CORE
<yellow>	RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);</yellow>
	synchronize_rcu();
#endif

	kvm_exit();

#if IS_ENABLED(CONFIG_HYPERV)
	if (static_branch_unlikely(&amp;enable_evmcs)) {
		int cpu;
		struct hv_vp_assist_page *vp_ap;
		/*
		 * Reset everything to support using non-enlightened VMCS
		 * access later (e.g. when we reload the module with
		 * enlightened_vmcs=0)
		 */
<yellow>		for_each_online_cpu(cpu) {</yellow>
<yellow>			vp_ap =	hv_get_vp_assist_page(cpu);</yellow>

			if (!vp_ap)
				continue;

<yellow>			vp_ap->nested_control.features.directhypercall = 0;</yellow>
			vp_ap-&gt;current_nested_vmcs = 0;
			vp_ap-&gt;enlighten_vmentry = 0;
		}

<yellow>		static_branch_disable(&enable_evmcs);</yellow>
	}
#endif
<yellow>	vmx_cleanup_l1d_flush();</yellow>

	allow_smaller_maxphyaddr = false;
}
module_exit(vmx_exit);

static int __init vmx_init(void)
{
	int r, cpu;

#if IS_ENABLED(CONFIG_HYPERV)
	/*
	 * Enlightened VMCS usage should be recommended and the host needs
	 * to support eVMCS v1 or above. We can also disable eVMCS support
	 * with module parameter.
	 */
<yellow>	if (enlightened_vmcs &&</yellow>
<yellow>	    ms_hyperv.hints & HV_X64_ENLIGHTENED_VMCS_RECOMMENDED &&</yellow>
<yellow>	    (ms_hyperv.nested_features & HV_X64_ENLIGHTENED_VMCS_VERSION) >=</yellow>
	    KVM_EVMCS_VERSION) {

		/* Check that we have assist pages on all online CPUs */
<yellow>		for_each_online_cpu(cpu) {</yellow>
<yellow>			if (!hv_get_vp_assist_page(cpu)) {</yellow>
<yellow>				enlightened_vmcs = false;</yellow>
				break;
			}
		}

<yellow>		if (enlightened_vmcs) {</yellow>
<yellow>			pr_info("KVM: vmx: using Hyper-V Enlightened VMCS\n");</yellow>
			static_branch_enable(&amp;enable_evmcs);
		}

<yellow>		if (ms_hyperv.nested_features & HV_X64_NESTED_DIRECT_FLUSH)</yellow>
			vmx_x86_ops.enable_direct_tlbflush
<yellow>				= hv_enable_direct_tlbflush;</yellow>

	} else {
<yellow>		enlightened_vmcs = false;</yellow>
	}
#endif

<yellow>	r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),</yellow>
		     __alignof__(struct vcpu_vmx), THIS_MODULE);
	if (r)
		return r;

	/*
	 * Must be called after kvm_init() so enable_ept is properly set
	 * up. Hand the parameter mitigation value in which was stored in
	 * the pre module init parser. If no parameter was given, it will
	 * contain &#x27;auto&#x27; which will be turned into the default &#x27;cond&#x27;
	 * mitigation mode.
	 */
<yellow>	r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);</yellow>
<yellow>	if (r) {</yellow>
		vmx_exit();
<yellow>		return r;</yellow>
	}

<yellow>	vmx_setup_fb_clear_ctrl();</yellow>

<yellow>	for_each_possible_cpu(cpu) {</yellow>
<yellow>		INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));</yellow>

		pi_init_cpu(cpu);
	}

#ifdef CONFIG_KEXEC_CORE
<yellow>	rcu_assign_pointer(crash_vmclear_loaded_vmcss,</yellow>
			   crash_vmclear_local_loaded_vmcss);
#endif
	vmx_check_vmcs12_offsets();

	/*
	 * Shadow paging doesn&#x27;t have a (further) performance penalty
	 * from GUEST_MAXPHYADDR &lt; HOST_MAXPHYADDR so enable it
	 * by default
	 */
<yellow>	if (!enable_ept)</yellow>
<yellow>		allow_smaller_maxphyaddr = true;</yellow>

	return 0;
<yellow>}</yellow>
module_init(vmx_init);


</code></pre></td></tr></table>
</body>
</html>
