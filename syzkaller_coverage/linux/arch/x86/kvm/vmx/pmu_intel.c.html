<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.<br>2.<br>3.<br>4.<br>5.<br>6.<br>7.<br>8.<br>9.<br>10.<br>11.<br>12.<br>13.<br>14.<br>15.<br>16.<br>17.<br>18.<br>19.<br>20.<br>21.<br>22.<br>23.<br>24.<br>25.<br>26.<br>27.<br>28.<br>29.<br>30.<br>31.<br>32.<br>33.<br>34.<br>35.<br>36.<br>37.<br>38.<br>39.<br>40.<br>41.<br>42.<br>43.<br>44.<br>45.<br>46.<br>47.<br>48.<br>49.<br>50.<br>51.<br>52.<br>53.<br>54.<br>55.<br>56.<br>57.<br>58.<br>59.<br>60.<br>61.<br>62.<br>63.<br>64.<br>65.<br>66.<br>67.<br>68.<br>69.<br>70.<br>71.<br>72.<br>73.<br>74.<br>75.<br>76.<br>77.<br>78.<br>79.<br>80.<br>81.<br>82.<br>83.<br>84.<br>85.<br>86.<br>87.<br>88.<br>89.<br>90.<br>91.<br>92.<br>93.<br>94.<br>95.<br>96.<br>97.<br>98.<br>99.<br>100.<br>101.<br>102.<br>103.<br>104.<br>105.<br>106.<br>107.<br>108.<br>109.<br>110.<br>111.<br>112.<br>113.<br>114.<br>115.<br>116.<br>117.<br>118.<br>119.<br>120.<br>121.<br>122.<br>123.<br>124.<br>125.<br>126.<br>127.<br>128.<br>129.<br>130.<br>131.<br>132.<br>133.<br>134.<br>135.<br>136.<br>137.<br>138.<br>139.<br>140.<br>141.<br>142.<br>143.<br>144.<br>145.<br>146.<br>147.<br>148.<br>149.<br>150.<br>151.<br>152.<br>153.<br>154.<br>155.<br>156.<br>157.<br>158.<br>159.<br>160.<br>161.<br>162.<br>163.<br>164.<br>165.<br>166.<br>167.<br>168.<br>169.<br>170.<br>171.<br>172.<br>173.<br>174.<br>175.<br>176.<br>177.<br>178.<br>179.<br>180.<br>181.<br>182.<br>183.<br>184.<br>185.<br>186.<br>187.<br>188.<br>189.<br>190.<br>191.<br>192.<br>193.<br>194.<br>195.<br>196.<br>197.<br>198.<br>199.<br>200.<br>201.<br>202.<br>203.<br>204.<br>205.<br>206.<br>207.<br>208.<br>209.<br>210.<br>211.<br>212.<br>213.<br>214.<br>215.<br>216.<br>217.<br>218.<br>219.<br>220.<br>221.<br>222.<br>223.<br>224.<br>225.<br>226.<br>227.<br>228.<br>229.<br>230.<br>231.<br>232.<br>233.<br>234.<br>235.<br>236.<br>237.<br>238.<br>239.<br>240.<br>241.<br>242.<br>243.<br>244.<br>245.<br>246.<br>247.<br>248.<br>249.<br>250.<br>251.<br>252.<br>253.<br>254.<br>255.<br>256.<br>257.<br>258.<br>259.<br>260.<br>261.<br>262.<br>263.<br>264.<br>265.<br>266.<br>267.<br>268.<br>269.<br>270.<br>271.<br>272.<br>273.<br>274.<br>275.<br>276.<br>277.<br>278.<br>279.<br>280.<br>281.<br>282.<br>283.<br>284.<br>285.<br>286.<br>287.<br>288.<br>289.<br>290.<br>291.<br>292.<br>293.<br>294.<br>295.<br>296.<br>297.<br>298.<br>299.<br>300.<br>301.<br>302.<br>303.<br>304.<br>305.<br>306.<br>307.<br>308.<br>309.<br>310.<br>311.<br>312.<br>313.<br>314.<br>315.<br>316.<br>317.<br>318.<br>319.<br>320.<br>321.<br>322.<br>323.<br>324.<br>325.<br>326.<br>327.<br>328.<br>329.<br>330.<br>331.<br>332.<br>333.<br>334.<br>335.<br>336.<br>337.<br>338.<br>339.<br>340.<br>341.<br>342.<br>343.<br>344.<br>345.<br>346.<br>347.<br>348.<br>349.<br>350.<br>351.<br>352.<br>353.<br>354.<br>355.<br>356.<br>357.<br>358.<br>359.<br>360.<br>361.<br>362.<br>363.<br>364.<br>365.<br>366.<br>367.<br>368.<br>369.<br>370.<br>371.<br>372.<br>373.<br>374.<br>375.<br>376.<br>377.<br>378.<br>379.<br>380.<br>381.<br>382.<br>383.<br>384.<br>385.<br>386.<br>387.<br>388.<br>389.<br>390.<br>391.<br>392.<br>393.<br>394.<br>395.<br>396.<br>397.<br>398.<br>399.<br>400.<br>401.<br>402.<br>403.<br>404.<br>405.<br>406.<br>407.<br>408.<br>409.<br>410.<br>411.<br>412.<br>413.<br>414.<br>415.<br>416.<br>417.<br>418.<br>419.<br>420.<br>421.<br>422.<br>423.<br>424.<br>425.<br>426.<br>427.<br>428.<br>429.<br>430.<br>431.<br>432.<br>433.<br>434.<br>435.<br>436.<br>437.<br>438.<br>439.<br>440.<br>441.<br>442.<br>443.<br>444.<br>445.<br>446.<br>447.<br>448.<br>449.<br>450.<br>451.<br>452.<br>453.<br>454.<br>455.<br>456.<br>457.<br>458.<br>459.<br>460.<br>461.<br>462.<br>463.<br>464.<br>465.<br>466.<br>467.<br>468.<br>469.<br>470.<br>471.<br>472.<br>473.<br>474.<br>475.<br>476.<br>477.<br>478.<br>479.<br>480.<br>481.<br>482.<br>483.<br>484.<br>485.<br>486.<br>487.<br>488.<br>489.<br>490.<br>491.<br>492.<br>493.<br>494.<br>495.<br>496.<br>497.<br>498.<br>499.<br>500.<br>501.<br>502.<br>503.<br>504.<br>505.<br>506.<br>507.<br>508.<br>509.<br>510.<br>511.<br>512.<br>513.<br>514.<br>515.<br>516.<br>517.<br>518.<br>519.<br>520.<br>521.<br>522.<br>523.<br>524.<br>525.<br>526.<br>527.<br>528.<br>529.<br>530.<br>531.<br>532.<br>533.<br>534.<br>535.<br>536.<br>537.<br>538.<br>539.<br>540.<br>541.<br>542.<br>543.<br>544.<br>545.<br>546.<br>547.<br>548.<br>549.<br>550.<br>551.<br>552.<br>553.<br>554.<br>555.<br>556.<br>557.<br>558.<br>559.<br>560.<br>561.<br>562.<br>563.<br>564.<br>565.<br>566.<br>567.<br>568.<br>569.<br>570.<br>571.<br>572.<br>573.<br>574.<br>575.<br>576.<br>577.<br>578.<br>579.<br>580.<br>581.<br>582.<br>583.<br>584.<br>585.<br>586.<br>587.<br>588.<br>589.<br>590.<br>591.<br>592.<br>593.<br>594.<br>595.<br>596.<br>597.<br>598.<br>599.<br>600.<br>601.<br>602.<br>603.<br>604.<br>605.<br>606.<br>607.<br>608.<br>609.<br>610.<br>611.<br>612.<br>613.<br>614.<br>615.<br>616.<br>617.<br>618.<br>619.<br>620.<br>621.<br>622.<br>623.<br>624.<br>625.<br>626.<br>627.<br>628.<br>629.<br>630.<br>631.<br>632.<br>633.<br>634.<br>635.<br>636.<br>637.<br>638.<br>639.<br>640.<br>641.<br>642.<br>643.<br>644.<br>645.<br>646.<br>647.<br>648.<br>649.<br>650.<br>651.<br>652.<br>653.<br>654.<br>655.<br>656.<br>657.<br>658.<br>659.<br>660.<br>661.<br>662.<br>663.<br>664.<br>665.<br>666.<br>667.<br>668.<br>669.<br>670.<br>671.<br>672.<br>673.<br>674.<br>675.<br>676.<br>677.<br>678.<br>679.<br>680.<br>681.<br>682.<br>683.<br>684.<br>685.<br>686.<br>687.<br>688.<br>689.<br>690.<br>691.<br>692.<br>693.<br>694.<br>695.<br>696.<br>697.<br>698.<br>699.<br>700.<br>701.<br>702.<br>703.<br>704.<br>705.<br>706.<br>707.<br>708.<br>709.<br>710.<br>711.<br>712.<br>713.<br>714.<br>715.<br>716.<br>717.<br>718.<br>719.<br>720.<br>721.<br>722.<br>723.<br>724.<br>725.<br>726.<br>727.<br>728.<br>729.<br>730.<br>731.<br>732.<br>733.<br>734.<br>735.<br>736.<br>737.<br>738.<br>739.<br>740.<br>741.<br>742.<br>743.<br>744.<br>745.<br>746.<br>747.<br>748.<br>749.<br>750.<br>751.<br>752.<br>753.<br>754.<br>755.<br>756.<br>757.<br>758.<br>759.<br>760.<br>761.<br>762.<br>763.<br>764.<br>765.<br>766.<br>767.<br>768.<br>769.<br>770.<br>771.<br>772.<br>773.<br>774.<br>775.<br>776.<br>777.<br>778.<br>779.<br>780.<br>781.<br>782.<br>783.<br>784.<br>785.<br>786.<br>787.<br>788.<br>789.<br>790.<br>791.<br>792.<br>793.<br>794.<br>795.<br>796.<br>797.<br>798.<br>799.<br>800.<br>801.<br>802.<br>803.<br>804.<br>805.<br>806.<br>807.<br>808.<br>809.<br>810.<br>811.<br>812.<br>813.<br>814.<br>815.<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * KVM PMU support for Intel CPUs
 *
 * Copyright 2011 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@redhat.com&gt;
 *   Gleb Natapov &lt;gleb@redhat.com&gt;
 */
#include &lt;linux/types.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/perf_event.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &quot;x86.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;lapic.h&quot;
#include &quot;nested.h&quot;
#include &quot;pmu.h&quot;

#define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)

static struct kvm_event_hw_type_mapping intel_arch_events[] = {
	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
	[2] = { 0x3c, 0x01, PERF_COUNT_HW_BUS_CYCLES  },
	[3] = { 0x2e, 0x4f, PERF_COUNT_HW_CACHE_REFERENCES },
	[4] = { 0x2e, 0x41, PERF_COUNT_HW_CACHE_MISSES },
	[5] = { 0xc4, 0x00, PERF_COUNT_HW_BRANCH_INSTRUCTIONS },
	[6] = { 0xc5, 0x00, PERF_COUNT_HW_BRANCH_MISSES },
	/* The above index must match CPUID 0x0A.EBX bit vector */
	[7] = { 0x00, 0x03, PERF_COUNT_HW_REF_CPU_CYCLES },
};

/* mapping between fixed pmc index and intel_arch_events array */
static int fixed_pmc_events[] = {1, 0, 7};

static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
{
	struct kvm_pmc *pmc;
	u8 old_fixed_ctr_ctrl = pmu-&gt;fixed_ctr_ctrl;
	int i;

	pmu-&gt;fixed_ctr_ctrl = data;
<blue>	for (i = 0; i < pmu->nr_arch_fixed_counters; i++) {</blue>
<blue>		u8 new_ctrl = fixed_ctrl_field(data, i);</blue>
<blue>		u8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);</blue>

		if (old_ctrl == new_ctrl)
			continue;

<blue>		pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);</blue>

<blue>		__set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);</blue>
		reprogram_counter(pmc);
	}
}

<blue>static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)</blue>
{
<blue>	if (pmc_idx < INTEL_PMC_IDX_FIXED) {</blue>
<blue>		return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,</blue>
				  MSR_P6_EVNTSEL0);
	} else {
		u32 idx = pmc_idx - INTEL_PMC_IDX_FIXED;

<blue>		return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);</blue>
	}
<blue>}</blue>

static void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
{
	int bit;
	struct kvm_pmc *pmc;

<blue>	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX) {</blue>
<blue>		pmc = intel_pmc_idx_to_pmc(pmu, bit);</blue>
<blue>		if (pmc)</blue>
<blue>			reprogram_counter(pmc);</blue>
	}
<blue>}</blue>

static bool intel_hw_event_available(struct kvm_pmc *pmc)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	u8 event_select = pmc-&gt;eventsel &amp; ARCH_PERFMON_EVENTSEL_EVENT;
	u8 unit_mask = (pmc-&gt;eventsel &amp; ARCH_PERFMON_EVENTSEL_UMASK) &gt;&gt; 8;
	int i;

<yellow>	for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {</yellow>
<yellow>		if (intel_arch_events[i].eventsel != event_select ||</yellow>
<yellow>		    intel_arch_events[i].unit_mask != unit_mask)</yellow>
			continue;

		/* disable event that reported as not present by cpuid */
<yellow>		if ((i < 7) && !(pmu->available_event_types & (1 << i)))</yellow>
			return false;

		break;
	}

	return true;
<yellow>}</yellow>

/* check if a PMC is enabled by comparing it with globl_ctrl bits. */
static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
{
<blue>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</blue>

	if (!intel_pmu_has_perf_global_ctrl(pmu))
		return true;

<blue>	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);</blue>
<blue>}</blue>

static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	bool fixed = idx &amp; (1u &lt;&lt; 30);

	idx &amp;= ~(3u &lt;&lt; 30);

<yellow>	return fixed ? idx < pmu->nr_arch_fixed_counters</yellow>
<yellow>		     : idx < pmu->nr_arch_gp_counters;</yellow>
<yellow>}</yellow>

static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
					    unsigned int idx, u64 *mask)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
<blue>	bool fixed = idx & (1u << 30);</blue>
	struct kvm_pmc *counters;
	unsigned int num_counters;

	idx &amp;= ~(3u &lt;&lt; 30);
	if (fixed) {
<blue>		counters = pmu->fixed_counters;</blue>
		num_counters = pmu-&gt;nr_arch_fixed_counters;
	} else {
<blue>		counters = pmu->gp_counters;</blue>
		num_counters = pmu-&gt;nr_arch_gp_counters;
	}
<blue>	if (idx >= num_counters)</blue>
		return NULL;
<blue>	*mask &= pmu->counter_bitmask[fixed ? KVM_PMC_FIXED : KVM_PMC_GP];</blue>
	return &amp;counters[array_index_nospec(idx, num_counters)];
<blue>}</blue>

static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
{
<blue>	if (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))</blue>
		return 0;

<blue>	return vcpu->arch.perf_capabilities;</blue>
}

static inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)
{
<blue>	return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;</blue>
}

static inline struct kvm_pmc *get_fw_gp_pmc(struct kvm_pmu *pmu, u32 msr)
{
<blue>	if (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))</blue>
		return NULL;

<blue>	return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);</blue>
}

static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
{
	struct x86_pmu_lbr *records = vcpu_to_lbr_records(vcpu);
	bool ret = false;

<blue>	if (!intel_pmu_lbr_is_enabled(vcpu))</blue>
		return ret;

	ret = (index == MSR_LBR_SELECT) || (index == MSR_LBR_TOS) ||
<yellow>		(index >= records->from && index < records->from + records->nr) ||</yellow>
<yellow>		(index >= records->to && index < records->to + records->nr);</yellow>

<yellow>	if (!ret && records->info)</yellow>
<yellow>		ret = (index >= records->info && index < records->info + records->nr);</yellow>

	return ret;
<blue>}</blue>

static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	u64 perf_capabilities;
	int ret;

<blue>	switch (msr) {</blue>
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
	case MSR_CORE_PERF_GLOBAL_STATUS:
	case MSR_CORE_PERF_GLOBAL_CTRL:
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		return intel_pmu_has_perf_global_ctrl(pmu);</blue>
		break;
	case MSR_IA32_PEBS_ENABLE:
<blue>		ret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;</blue>
		break;
	case MSR_IA32_DS_AREA:
<blue>		ret = guest_cpuid_has(vcpu, X86_FEATURE_DS);</blue>
		break;
	case MSR_PEBS_DATA_CFG:
<blue>		perf_capabilities = vcpu_get_perf_capabilities(vcpu);</blue>
		ret = (perf_capabilities &amp; PERF_CAP_PEBS_BASELINE) &amp;&amp;
<yellow>			((perf_capabilities & PERF_CAP_PEBS_FORMAT) > 3);</yellow>
		break;
	default:
<blue>		ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||</blue>
<blue>			get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||</blue>
<blue>			get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||</blue>
<blue>			intel_pmu_is_valid_lbr_msr(vcpu, msr);</blue>
		break;
	}

	return ret;
<blue>}</blue>

<blue>static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)</blue>
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;

<blue>	pmc = get_fixed_pmc(pmu, msr);</blue>
<blue>	pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);</blue>
<blue>	pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);</blue>

	return pmc;
<blue>}</blue>

static inline void intel_pmu_release_guest_lbr_event(struct kvm_vcpu *vcpu)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (lbr_desc->event) {</yellow>
<yellow>		perf_event_release_kernel(lbr_desc->event);</yellow>
		lbr_desc-&gt;event = NULL;
		vcpu_to_pmu(vcpu)-&gt;event_count--;
	}
}

int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)
<yellow>{</yellow>
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct perf_event *event;

	/*
	 * The perf_event_attr is constructed in the minimum efficient way:
	 * - set &#x27;pinned = true&#x27; to make it task pinned so that if another
	 *   cpu pinned event reclaims LBR, the event-&gt;oncpu will be set to -1;
	 * - set &#x27;.exclude_host = true&#x27; to record guest branches behavior;
	 *
	 * - set &#x27;.config = INTEL_FIXED_VLBR_EVENT&#x27; to indicates host perf
	 *   schedule the event without a real HW counter but a fake one;
	 *   check is_guest_lbr_event() and __intel_get_event_constraints();
	 *
	 * - set &#x27;sample_type = PERF_SAMPLE_BRANCH_STACK&#x27; and
	 *   &#x27;branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |
	 *   PERF_SAMPLE_BRANCH_USER&#x27; to configure it as a LBR callstack
	 *   event, which helps KVM to save/restore guest LBR records
	 *   during host context switches and reduces quite a lot overhead,
	 *   check branch_user_callstack() and intel_pmu_lbr_sched_task();
	 */
<yellow>	struct perf_event_attr attr = {</yellow>
		.type = PERF_TYPE_RAW,
		.size = sizeof(attr),
		.config = INTEL_FIXED_VLBR_EVENT,
		.sample_type = PERF_SAMPLE_BRANCH_STACK,
		.pinned = true,
		.exclude_host = true,
		.branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |
					PERF_SAMPLE_BRANCH_USER,
	};

	if (unlikely(lbr_desc-&gt;event)) {
<yellow>		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);</yellow>
		return 0;
	}

<yellow>	event = perf_event_create_kernel_counter(&attr, -1,</yellow>
						current, NULL, NULL);
	if (IS_ERR(event)) {
<yellow>		pr_debug_ratelimited("%s: failed %ld\n",</yellow>
					__func__, PTR_ERR(event));
<yellow>		return PTR_ERR(event);</yellow>
	}
<yellow>	lbr_desc->event = event;</yellow>
	pmu-&gt;event_count++;
	__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu-&gt;pmc_in_use);
	return 0;
}

/*
 * It&#x27;s safe to access LBR msrs from guest when they have not
 * been passthrough since the host would help restore or reset
 * the LBR msrs records when the guest LBR event is scheduled in.
 */
static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
				     struct msr_data *msr_info, bool read)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
<yellow>	u32 index = msr_info->index;</yellow>

	if (!intel_pmu_is_valid_lbr_msr(vcpu, index))
		return false;

<yellow>	if (!lbr_desc->event && intel_pmu_create_guest_lbr_event(vcpu) < 0)</yellow>
		goto dummy;

	/*
	 * Disable irq to ensure the LBR feature doesn&#x27;t get reclaimed by the
	 * host at the time the value is read from the msr, and this avoids the
	 * host LBR value to be leaked to the guest. If LBR has been reclaimed,
	 * return 0 on guest reads.
	 */
<yellow>	local_irq_disable();</yellow>
	if (lbr_desc-&gt;event-&gt;state == PERF_EVENT_STATE_ACTIVE) {
<yellow>		if (read)</yellow>
<yellow>			rdmsrl(index, msr_info->data);</yellow>
		else
<yellow>			wrmsrl(index, msr_info->data);</yellow>
<yellow>		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);</yellow>
		local_irq_enable();
		return true;
	}
<yellow>	clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);</yellow>
	local_irq_enable();

dummy:
<yellow>	if (read)</yellow>
<yellow>		msr_info->data = 0;</yellow>
	return true;
<yellow>}</yellow>

static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;
<blue>	u32 msr = msr_info->index;</blue>

	switch (msr) {
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
<blue>		msr_info->data = pmu->fixed_ctr_ctrl;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_STATUS:
<blue>		msr_info->data = pmu->global_status;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		msr_info->data = pmu->global_ctrl;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		msr_info->data = 0;</blue>
		return 0;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		msr_info->data = pmu->pebs_enable;</yellow>
		return 0;
	case MSR_IA32_DS_AREA:
<yellow>		msr_info->data = pmu->ds_area;</yellow>
		return 0;
	case MSR_PEBS_DATA_CFG:
<yellow>		msr_info->data = pmu->pebs_data_cfg;</yellow>
		return 0;
	default:
<blue>		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||</blue>
<blue>		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {</blue>
<blue>			u64 val = pmc_read_counter(pmc);</blue>
			msr_info-&gt;data =
				val &amp; pmu-&gt;counter_bitmask[KVM_PMC_GP];
			return 0;
<blue>		} else if ((pmc = get_fixed_pmc(pmu, msr))) {</blue>
<blue>			u64 val = pmc_read_counter(pmc);</blue>
			msr_info-&gt;data =
				val &amp; pmu-&gt;counter_bitmask[KVM_PMC_FIXED];
			return 0;
<blue>		} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {</blue>
<blue>			msr_info->data = pmc->eventsel;</blue>
			return 0;
<yellow>		} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, true))</yellow>
			return 0;
	}

	return 1;
}

static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
<blue>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</blue>
	struct kvm_pmc *pmc;
	u32 msr = msr_info-&gt;index;
	u64 data = msr_info-&gt;data;
	u64 reserved_bits, diff;

	switch (msr) {
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
<blue>		if (pmu->fixed_ctr_ctrl == data)</blue>
			return 0;
<blue>		if (!(data & pmu->fixed_ctr_ctrl_mask)) {</blue>
<blue>			reprogram_fixed_counters(pmu, data);</blue>
			return 0;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_STATUS:
<blue>		if (msr_info->host_initiated) {</blue>
<blue>			pmu->global_status = data;</blue>
			return 0;
		}
		break; /* RO MSR */
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		if (pmu->global_ctrl == data)</blue>
			return 0;
<blue>		if (kvm_valid_perf_global_ctrl(pmu, data)) {</blue>
			diff = pmu-&gt;global_ctrl ^ data;
<blue>			pmu->global_ctrl = data;</blue>
			reprogram_counters(pmu, diff);
			return 0;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		if (!(data & pmu->global_ovf_ctrl_mask)) {</blue>
<blue>			if (!msr_info->host_initiated)</blue>
<yellow>				pmu->global_status &= ~data;</yellow>
			return 0;
		}
		break;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		if (pmu->pebs_enable == data)</yellow>
			return 0;
<yellow>		if (!(data & pmu->pebs_enable_mask)) {</yellow>
			diff = pmu-&gt;pebs_enable ^ data;
<yellow>			pmu->pebs_enable = data;</yellow>
			reprogram_counters(pmu, diff);
			return 0;
		}
		break;
	case MSR_IA32_DS_AREA:
<blue>		if (msr_info->host_initiated && data && !guest_cpuid_has(vcpu, X86_FEATURE_DS))</blue>
			return 1;
<blue>		if (is_noncanonical_address(data, vcpu))</blue>
			return 1;
<blue>		pmu->ds_area = data;</blue>
		return 0;
	case MSR_PEBS_DATA_CFG:
<yellow>		if (pmu->pebs_data_cfg == data)</yellow>
			return 0;
<yellow>		if (!(data & pmu->pebs_data_cfg_mask)) {</yellow>
			pmu-&gt;pebs_data_cfg = data;
<yellow>			return 0;</yellow>
		}
		break;
	default:
<blue>		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||</blue>
<blue>		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {</blue>
<blue>			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&</blue>
<blue>			    (data & ~pmu->counter_bitmask[KVM_PMC_GP]))</blue>
				return 1;
<blue>			if (!msr_info->host_initiated &&</blue>
			    !(msr &amp; MSR_PMC_FULL_WIDTH_BIT))
<blue>				data = (s64)(s32)data;</blue>
<blue>			pmc->counter += data - pmc_read_counter(pmc);</blue>
<yellow>			pmc_update_sample_period(pmc);</yellow>
			return 0;
<blue>		} else if ((pmc = get_fixed_pmc(pmu, msr))) {</blue>
<blue>			pmc->counter += data - pmc_read_counter(pmc);</blue>
<yellow>			pmc_update_sample_period(pmc);</yellow>
			return 0;
<blue>		} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {</blue>
<blue>			if (data == pmc->eventsel)</blue>
				return 0;
<blue>			reserved_bits = pmu->reserved_bits;</blue>
			if ((pmc-&gt;idx == 2) &amp;&amp;
<blue>			    (pmu->raw_event_mask & HSW_IN_TX_CHECKPOINTED))</blue>
<yellow>				reserved_bits ^= HSW_IN_TX_CHECKPOINTED;</yellow>
<blue>			if (!(data & reserved_bits)) {</blue>
<blue>				pmc->eventsel = data;</blue>
				reprogram_counter(pmc);
				return 0;
			}
<yellow>		} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, false))</yellow>
			return 0;
	}

	return 1;
}

static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
{
	size_t size = ARRAY_SIZE(fixed_pmc_events);
	struct kvm_pmc *pmc;
	u32 event;
	int i;

	for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++) {
<blue>		pmc = &pmu->fixed_counters[i];</blue>
		event = fixed_pmc_events[array_index_nospec(i, size)];
		pmc-&gt;eventsel = (intel_arch_events[event].unit_mask &lt;&lt; 8) |
			intel_arch_events[event].eventsel;
	}
}

static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
	struct kvm_cpuid_entry2 *entry;
	union cpuid10_eax eax;
	union cpuid10_edx edx;
	u64 perf_capabilities;
	u64 counter_mask;
	int i;

<blue>	pmu->nr_arch_gp_counters = 0;</blue>
	pmu-&gt;nr_arch_fixed_counters = 0;
	pmu-&gt;counter_bitmask[KVM_PMC_GP] = 0;
	pmu-&gt;counter_bitmask[KVM_PMC_FIXED] = 0;
	pmu-&gt;version = 0;
	pmu-&gt;reserved_bits = 0xffffffff00200000ull;
	pmu-&gt;raw_event_mask = X86_RAW_EVENT_MASK;
	pmu-&gt;global_ctrl_mask = ~0ull;
	pmu-&gt;global_ovf_ctrl_mask = ~0ull;
	pmu-&gt;fixed_ctr_ctrl_mask = ~0ull;
	pmu-&gt;pebs_enable_mask = ~0ull;
	pmu-&gt;pebs_data_cfg_mask = ~0ull;

	entry = kvm_find_cpuid_entry(vcpu, 0xa);
<blue>	if (!entry || !vcpu->kvm->arch.enable_pmu)</blue>
		return;
<blue>	eax.full = entry->eax;</blue>
	edx.full = entry-&gt;edx;

	pmu-&gt;version = eax.split.version_id;
	if (!pmu-&gt;version)
		return;

<blue>	pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters,</blue>
					 kvm_pmu_cap.num_counters_gp);
	eax.split.bit_width = min_t(int, eax.split.bit_width,
				    kvm_pmu_cap.bit_width_gp);
<blue>	pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;</blue>
	eax.split.mask_length = min_t(int, eax.split.mask_length,
				      kvm_pmu_cap.events_mask_len);
<blue>	pmu->available_event_types = ~entry->ebx &</blue>
<yellow>					((1ull << eax.split.mask_length) - 1);</yellow>

	if (pmu-&gt;version == 1) {
<blue>		pmu->nr_arch_fixed_counters = 0;</blue>
	} else {
		pmu-&gt;nr_arch_fixed_counters =
<blue>			min3(ARRAY_SIZE(fixed_pmc_events),</blue>
			     (size_t) edx.split.num_counters_fixed,
			     (size_t)kvm_pmu_cap.num_counters_fixed);
		edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed,
						  kvm_pmu_cap.bit_width_fixed);
		pmu-&gt;counter_bitmask[KVM_PMC_FIXED] =
<blue>			((u64)1 << edx.split.bit_width_fixed) - 1;</blue>
<blue>		setup_fixed_pmc_eventsel(pmu);</blue>
	}

	for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++)
<blue>		pmu->fixed_ctr_ctrl_mask &= ~(0xbull << (i * 4));</blue>
<blue>	counter_mask = ~(((1ull << pmu->nr_arch_gp_counters) - 1) |</blue>
<blue>		(((1ull << pmu->nr_arch_fixed_counters) - 1) << INTEL_PMC_IDX_FIXED));</blue>
	pmu-&gt;global_ctrl_mask = counter_mask;
	pmu-&gt;global_ovf_ctrl_mask = pmu-&gt;global_ctrl_mask
			&amp; ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF |
			    MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
	if (vmx_pt_mode_is_host_guest())
<yellow>		pmu->global_ovf_ctrl_mask &=</yellow>
				~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;

<blue>	entry = kvm_find_cpuid_entry_index(vcpu, 7, 0);</blue>
	if (entry &amp;&amp;
<blue>	    (boot_cpu_has(X86_FEATURE_HLE) || boot_cpu_has(X86_FEATURE_RTM)) &&</blue>
<yellow>	    (entry->ebx & (X86_FEATURE_HLE|X86_FEATURE_RTM))) {</yellow>
<yellow>		pmu->reserved_bits ^= HSW_IN_TX;</yellow>
		pmu-&gt;raw_event_mask |= (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED);
	}

<blue>	bitmap_set(pmu->all_valid_pmc_idx,</blue>
		0, pmu-&gt;nr_arch_gp_counters);
	bitmap_set(pmu-&gt;all_valid_pmc_idx,
		INTEL_PMC_MAX_GENERIC, pmu-&gt;nr_arch_fixed_counters);

<blue>	perf_capabilities = vcpu_get_perf_capabilities(vcpu);</blue>
<blue>	if (cpuid_model_is_consistent(vcpu) &&</blue>
<blue>	    (perf_capabilities & PMU_CAP_LBR_FMT))</blue>
<yellow>		x86_perf_get_lbr(&lbr_desc->records);</yellow>
	else
<blue>		lbr_desc->records.nr = 0;</blue>

	if (lbr_desc-&gt;records.nr)
<yellow>		bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);</yellow>

<blue>	if (perf_capabilities & PERF_CAP_PEBS_FORMAT) {</blue>
<yellow>		if (perf_capabilities & PERF_CAP_PEBS_BASELINE) {</yellow>
<yellow>			pmu->pebs_enable_mask = counter_mask;</yellow>
			pmu-&gt;reserved_bits &amp;= ~ICL_EVENTSEL_ADAPTIVE;
			for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++) {
<yellow>				pmu->fixed_ctr_ctrl_mask &=</yellow>
<yellow>					~(1ULL << (INTEL_PMC_IDX_FIXED + i * 4));</yellow>
			}
<yellow>			pmu->pebs_data_cfg_mask = ~0xff00000full;</yellow>
		} else {
			pmu-&gt;pebs_enable_mask =
<yellow>				~((1ull << pmu->nr_arch_gp_counters) - 1);</yellow>
		}
	}
<blue>}</blue>

static void intel_pmu_init(struct kvm_vcpu *vcpu)
{
	int i;
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

	for (i = 0; i &lt; KVM_INTEL_PMC_MAX_GENERIC; i++) {
<blue>		pmu->gp_counters[i].type = KVM_PMC_GP;</blue>
		pmu-&gt;gp_counters[i].vcpu = vcpu;
		pmu-&gt;gp_counters[i].idx = i;
		pmu-&gt;gp_counters[i].current_config = 0;
	}

	for (i = 0; i &lt; KVM_PMC_MAX_FIXED; i++) {
<blue>		pmu->fixed_counters[i].type = KVM_PMC_FIXED;</blue>
		pmu-&gt;fixed_counters[i].vcpu = vcpu;
		pmu-&gt;fixed_counters[i].idx = i + INTEL_PMC_IDX_FIXED;
		pmu-&gt;fixed_counters[i].current_config = 0;
	}

<blue>	vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();</blue>
	lbr_desc-&gt;records.nr = 0;
	lbr_desc-&gt;event = NULL;
	lbr_desc-&gt;msr_passthrough = false;
}

static void intel_pmu_reset(struct kvm_vcpu *vcpu)
<blue>{</blue>
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc = NULL;
	int i;

	for (i = 0; i &lt; KVM_INTEL_PMC_MAX_GENERIC; i++) {
<blue>		pmc = &pmu->gp_counters[i];</blue>

<yellow>		pmc_stop_counter(pmc);</yellow>
<blue>		pmc->counter = pmc->eventsel = 0;</blue>
	}

	for (i = 0; i &lt; KVM_PMC_MAX_FIXED; i++) {
<blue>		pmc = &pmu->fixed_counters[i];</blue>

<yellow>		pmc_stop_counter(pmc);</yellow>
<blue>		pmc->counter = 0;</blue>
	}

<blue>	pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;</blue>

<yellow>	intel_pmu_release_guest_lbr_event(vcpu);</yellow>
}

/*
 * Emulate LBR_On_PMI behavior for 1 &lt; pmu.version &lt; 4.
 *
 * If Freeze_LBR_On_PMI = 1, the LBR is frozen on PMI and
 * the KVM emulates to clear the LBR bit (bit 0) in IA32_DEBUGCTL.
 *
 * Guest needs to re-enable LBR to resume branches recording.
 */
static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
{
<yellow>	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);</yellow>

<yellow>	if (data & DEBUGCTLMSR_FREEZE_LBRS_ON_PMI) {</yellow>
<yellow>		data &= ~DEBUGCTLMSR_LBR;</yellow>
<yellow>		vmcs_write64(GUEST_IA32_DEBUGCTL, data);</yellow>
	}
}

static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
{
<yellow>	u8 version = vcpu_to_pmu(vcpu)->version;</yellow>

	if (!intel_pmu_lbr_is_enabled(vcpu))
		return;

<yellow>	if (version > 1 && version < 4)</yellow>
<yellow>		intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);</yellow>
<yellow>}</yellow>

static void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)
{
	struct x86_pmu_lbr *lbr = vcpu_to_lbr_records(vcpu);
	int i;

<yellow>	for (i = 0; i < lbr->nr; i++) {</yellow>
<yellow>		vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);</yellow>
		vmx_set_intercept_for_msr(vcpu, lbr-&gt;to + i, MSR_TYPE_RW, set);
		if (lbr-&gt;info)
<yellow>			vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);</yellow>
	}

<yellow>	vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);</yellow>
	vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
<yellow>}</yellow>

<yellow>static inline void vmx_disable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)</yellow>
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (!lbr_desc->msr_passthrough)</yellow>
		return;

<yellow>	vmx_update_intercept_for_lbr_msrs(vcpu, true);</yellow>
	lbr_desc-&gt;msr_passthrough = false;
}

static inline void vmx_enable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (lbr_desc->msr_passthrough)</yellow>
		return;

<yellow>	vmx_update_intercept_for_lbr_msrs(vcpu, false);</yellow>
	lbr_desc-&gt;msr_passthrough = true;
}

/*
 * Higher priority host perf events (e.g. cpu pinned) could reclaim the
 * pmu resources (e.g. LBR) that were assigned to the guest. This is
 * usually done via ipi calls (more details in perf_install_in_context).
 *
 * Before entering the non-root mode (with irq disabled here), double
 * confirm that the pmu features enabled to the guest are not reclaimed
 * by higher priority host events. Otherwise, disallow vcpu&#x27;s access to
 * the reclaimed features.
 */
void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (!lbr_desc->event) {</yellow>
<yellow>		vmx_disable_lbr_msrs_passthrough(vcpu);</yellow>
<yellow>		if (vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR)</yellow>
			goto warn;
<yellow>		if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))</yellow>
			goto warn;
		return;
	}

<yellow>	if (lbr_desc->event->state < PERF_EVENT_STATE_ACTIVE) {</yellow>
<yellow>		vmx_disable_lbr_msrs_passthrough(vcpu);</yellow>
<yellow>		__clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);</yellow>
		goto warn;
	} else
<yellow>		vmx_enable_lbr_msrs_passthrough(vcpu);</yellow>

	return;

warn:
<yellow>	pr_warn_ratelimited("kvm: vcpu-%d: fail to passthrough LBR.\n",</yellow>
		vcpu-&gt;vcpu_id);
<yellow>}</yellow>

static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
{
<yellow>	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))</yellow>
<yellow>		intel_pmu_release_guest_lbr_event(vcpu);</yellow>
<yellow>}</yellow>

void intel_pmu_cross_mapped_check(struct kvm_pmu *pmu)
{
	struct kvm_pmc *pmc = NULL;
	int bit, hw_idx;

<yellow>	for_each_set_bit(bit, (unsigned long *)&pmu->global_ctrl,</yellow>
			 X86_PMC_IDX_MAX) {
<yellow>		pmc = intel_pmc_idx_to_pmc(pmu, bit);</yellow>

<yellow>		if (!pmc || !pmc_speculative_in_use(pmc) ||</yellow>
<yellow>		    !intel_pmc_is_enabled(pmc) || !pmc->perf_event)</yellow>
			continue;

		/*
		 * A negative index indicates the event isn&#x27;t mapped to a
		 * physical counter in the host, e.g. due to contention.
		 */
<yellow>		hw_idx = pmc->perf_event->hw.idx;</yellow>
<yellow>		if (hw_idx != pmc->idx && hw_idx > -1)</yellow>
<yellow>			pmu->host_cross_mapped_mask |= BIT_ULL(hw_idx);</yellow>
	}
<yellow>}</yellow>

struct kvm_pmu_ops intel_pmu_ops __initdata = {
	.hw_event_available = intel_hw_event_available,
	.pmc_is_enabled = intel_pmc_is_enabled,
	.pmc_idx_to_pmc = intel_pmc_idx_to_pmc,
	.rdpmc_ecx_to_pmc = intel_rdpmc_ecx_to_pmc,
	.msr_idx_to_pmc = intel_msr_idx_to_pmc,
	.is_valid_rdpmc_ecx = intel_is_valid_rdpmc_ecx,
	.is_valid_msr = intel_is_valid_msr,
	.get_msr = intel_pmu_get_msr,
	.set_msr = intel_pmu_set_msr,
	.refresh = intel_pmu_refresh,
	.init = intel_pmu_init,
	.reset = intel_pmu_reset,
	.deliver_pmi = intel_pmu_deliver_pmi,
	.cleanup = intel_pmu_cleanup,
};


</code></pre></td></tr></table>
</body>
</html>
