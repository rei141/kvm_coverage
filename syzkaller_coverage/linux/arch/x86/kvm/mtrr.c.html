<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * vMTRR implementation
 *
 * Copyright (C) 2006 Qumranet, Inc.
 * Copyright 2010 Red Hat, Inc. and/or its affiliates.
 * Copyright(C) 2015 Intel Corporation.
 *
 * Authors:
 *   Yaniv Kamay  &lt;yaniv@qumranet.com&gt;
 *   Avi Kivity   &lt;avi@qumranet.com&gt;
 *   Marcelo Tosatti &lt;mtosatti@redhat.com&gt;
 *   Paolo Bonzini &lt;pbonzini@redhat.com&gt;
 *   Xiao Guangrong &lt;guangrong.xiao@linux.intel.com&gt;
 */

#include &lt;linux/kvm_host.h&gt;
#include &lt;asm/mtrr.h&gt;

#include &quot;cpuid.h&quot;
#include &quot;mmu.h&quot;

#define IA32_MTRR_DEF_TYPE_E		(1ULL &lt;&lt; 11)
#define IA32_MTRR_DEF_TYPE_FE		(1ULL &lt;&lt; 10)
#define IA32_MTRR_DEF_TYPE_TYPE_MASK	(0xff)

static bool msr_mtrr_valid(unsigned msr)
{
<blue>	switch (msr) {</blue>
	case 0x200 ... 0x200 + 2 * KVM_NR_VAR_MTRR - 1:
	case MSR_MTRRfix64K_00000:
	case MSR_MTRRfix16K_80000:
	case MSR_MTRRfix16K_A0000:
	case MSR_MTRRfix4K_C0000:
	case MSR_MTRRfix4K_C8000:
	case MSR_MTRRfix4K_D0000:
	case MSR_MTRRfix4K_D8000:
	case MSR_MTRRfix4K_E0000:
	case MSR_MTRRfix4K_E8000:
	case MSR_MTRRfix4K_F0000:
	case MSR_MTRRfix4K_F8000:
	case MSR_MTRRdefType:
	case MSR_IA32_CR_PAT:
		return true;
	}
	return false;
}

static bool valid_mtrr_type(unsigned t)
{
<blue>	return t < 8 && (1 << t) & 0x73; /* 0, 1, 4, 5, 6 */</blue>
}

<blue>bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)</blue>
{
	int i;
	u64 mask;

<blue>	if (!msr_mtrr_valid(msr))</blue>
		return false;

<blue>	if (msr == MSR_IA32_CR_PAT) {</blue>
<yellow>		return kvm_pat_valid(data);</yellow>
<blue>	} else if (msr == MSR_MTRRdefType) {</blue>
<blue>		if (data & ~0xcff)</blue>
			return false;
<blue>		return valid_mtrr_type(data & 0xff);</blue>
<blue>	} else if (msr >= MSR_MTRRfix64K_00000 && msr <= MSR_MTRRfix4K_F8000) {</blue>
<blue>		for (i = 0; i < 8 ; i++)</blue>
<blue>			if (!valid_mtrr_type((data >> (i * 8)) & 0xff))</blue>
				return false;
		return true;
	}

	/* variable MTRRs */
<blue>	WARN_ON(!(msr >= 0x200 && msr < 0x200 + 2 * KVM_NR_VAR_MTRR));</blue>

	mask = kvm_vcpu_reserved_gpa_bits_raw(vcpu);
<blue>	if ((msr & 1) == 0) {</blue>
		/* MTRR base */
<blue>		if (!valid_mtrr_type(data & 0xff))</blue>
			return false;
<blue>		mask |= 0xf00;</blue>
	} else
		/* MTRR mask */
<blue>		mask |= 0x7ff;</blue>

<blue>	return (data & mask) == 0;</blue>
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_mtrr_valid);

static bool mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
{
<yellow>	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E);</yellow>
}

static bool fixed_mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
{
<yellow>	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE);</yellow>
}

static u8 mtrr_default_type(struct kvm_mtrr *mtrr_state)
{
<yellow>	return mtrr_state->deftype & IA32_MTRR_DEF_TYPE_TYPE_MASK;</yellow>
}

static u8 mtrr_disabled_type(struct kvm_vcpu *vcpu)
{
	/*
	 * Intel SDM 11.11.2.2: all MTRRs are disabled when
	 * IA32_MTRR_DEF_TYPE.E bit is cleared, and the UC
	 * memory type is applied to all of physical memory.
	 *
	 * However, virtual machines can be run with CPUID such that
	 * there are no MTRRs.  In that case, the firmware will never
	 * enable MTRRs and it is obviously undesirable to run the
	 * guest entirely with UC memory and we use WB.
	 */
<yellow>	if (guest_cpuid_has(vcpu, X86_FEATURE_MTRR))</yellow>
		return MTRR_TYPE_UNCACHABLE;
	else
		return MTRR_TYPE_WRBACK;
}

/*
* Three terms are used in the following code:
* - segment, it indicates the address segments covered by fixed MTRRs.
* - unit, it corresponds to the MSR entry in the segment.
* - range, a range is covered in one memory cache type.
*/
struct fixed_mtrr_segment {
	u64 start;
	u64 end;

	int range_shift;

	/* the start position in kvm_mtrr.fixed_ranges[]. */
	int range_start;
};

static struct fixed_mtrr_segment fixed_seg_table[] = {
	/* MSR_MTRRfix64K_00000, 1 unit. 64K fixed mtrr. */
	{
		.start = 0x0,
		.end = 0x80000,
		.range_shift = 16, /* 64K */
		.range_start = 0,
	},

	/*
	 * MSR_MTRRfix16K_80000 ... MSR_MTRRfix16K_A0000, 2 units,
	 * 16K fixed mtrr.
	 */
	{
		.start = 0x80000,
		.end = 0xc0000,
		.range_shift = 14, /* 16K */
		.range_start = 8,
	},

	/*
	 * MSR_MTRRfix4K_C0000 ... MSR_MTRRfix4K_F8000, 8 units,
	 * 4K fixed mtrr.
	 */
	{
		.start = 0xc0000,
		.end = 0x100000,
		.range_shift = 12, /* 12K */
		.range_start = 24,
	}
};

/*
 * The size of unit is covered in one MSR, one MSR entry contains
 * 8 ranges so that unit size is always 8 * 2^range_shift.
 */
static u64 fixed_mtrr_seg_unit_size(int seg)
{
<yellow>	return 8 << fixed_seg_table[seg].range_shift;</yellow>
}

<blue>static bool fixed_msr_to_seg_unit(u32 msr, int *seg, int *unit)</blue>
{
<blue>	switch (msr) {</blue>
	case MSR_MTRRfix64K_00000:
		*seg = 0;
		*unit = 0;
		break;
	case MSR_MTRRfix16K_80000 ... MSR_MTRRfix16K_A0000:
		*seg = 1;
<blue>		*unit = array_index_nospec(</blue>
			msr - MSR_MTRRfix16K_80000,
			MSR_MTRRfix16K_A0000 - MSR_MTRRfix16K_80000 + 1);
		break;
	case MSR_MTRRfix4K_C0000 ... MSR_MTRRfix4K_F8000:
		*seg = 2;
<blue>		*unit = array_index_nospec(</blue>
			msr - MSR_MTRRfix4K_C0000,
			MSR_MTRRfix4K_F8000 - MSR_MTRRfix4K_C0000 + 1);
		break;
	default:
		return false;
	}

	return true;
}

static void fixed_mtrr_seg_unit_range(int seg, int unit, u64 *start, u64 *end)
{
<yellow>	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];</yellow>
<yellow>	u64 unit_size = fixed_mtrr_seg_unit_size(seg);</yellow>

	*start = mtrr_seg-&gt;start + unit * unit_size;
	*end = *start + unit_size;
<yellow>	WARN_ON(*end > mtrr_seg->end);</yellow>
}

static int fixed_mtrr_seg_unit_range_index(int seg, int unit)
{
	struct fixed_mtrr_segment *mtrr_seg = &amp;fixed_seg_table[seg];

<blue>	WARN_ON(mtrr_seg->start + unit * fixed_mtrr_seg_unit_size(seg)</blue>
		&gt; mtrr_seg-&gt;end);

	/* each unit has 8 ranges. */
<blue>	return mtrr_seg->range_start + 8 * unit;</blue>
}

static int fixed_mtrr_seg_end_range_index(int seg)
{
	struct fixed_mtrr_segment *mtrr_seg = &amp;fixed_seg_table[seg];
	int n;

<yellow>	n = (mtrr_seg->end - mtrr_seg->start) >> mtrr_seg->range_shift;</yellow>
<yellow>	return mtrr_seg->range_start + n - 1;</yellow>
}

static bool fixed_msr_to_range(u32 msr, u64 *start, u64 *end)
{
	int seg, unit;

<yellow>	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))</yellow>
		return false;

<yellow>	fixed_mtrr_seg_unit_range(seg, unit, start, end);</yellow>
	return true;
}

static int fixed_msr_to_range_index(u32 msr)
{
	int seg, unit;

<blue>	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))</blue>
		return -1;

<blue>	return fixed_mtrr_seg_unit_range_index(seg, unit);</blue>
<blue>}</blue>

static int fixed_mtrr_addr_to_seg(u64 addr)
{
	struct fixed_mtrr_segment *mtrr_seg;
	int seg, seg_num = ARRAY_SIZE(fixed_seg_table);

<yellow>	for (seg = 0; seg < seg_num; seg++) {</yellow>
<yellow>		mtrr_seg = &fixed_seg_table[seg];</yellow>
<yellow>		if (mtrr_seg->start <= addr && addr < mtrr_seg->end)</yellow>
			return seg;
	}

	return -1;
}

static int fixed_mtrr_addr_seg_to_range_index(u64 addr, int seg)
{
	struct fixed_mtrr_segment *mtrr_seg;
	int index;

	mtrr_seg = &amp;fixed_seg_table[seg];
	index = mtrr_seg-&gt;range_start;
<yellow>	index += (addr - mtrr_seg->start) >> mtrr_seg->range_shift;</yellow>
	return index;
}

static u64 fixed_mtrr_range_end_addr(int seg, int index)
{
	struct fixed_mtrr_segment *mtrr_seg = &amp;fixed_seg_table[seg];
	int pos = index - mtrr_seg-&gt;range_start;

<yellow>	return mtrr_seg->start + ((pos + 1) << mtrr_seg->range_shift);</yellow>
}

static void var_mtrr_range(struct kvm_mtrr_range *range, u64 *start, u64 *end)
{
	u64 mask;

<yellow>	*start = range->base & PAGE_MASK;</yellow>

<yellow>	mask = range->mask & PAGE_MASK;</yellow>

	/* This cannot overflow because writing to the reserved bits of
	 * variable MTRRs causes a #GP.
	 */
	*end = (*start | ~mask) + 1;
}

static void update_mtrr(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_mtrr *mtrr_state = &amp;vcpu-&gt;arch.mtrr_state;
	gfn_t start, end;
	int index;

<blue>	if (msr == MSR_IA32_CR_PAT || !tdp_enabled ||</blue>
<blue>	      !kvm_arch_has_noncoherent_dma(vcpu->kvm))</blue>
		return;

<yellow>	if (!mtrr_is_enabled(mtrr_state) && msr != MSR_MTRRdefType)</yellow>
		return;

	/* fixed MTRRs. */
<yellow>	if (fixed_msr_to_range(msr, &start, &end)) {</yellow>
<yellow>		if (!fixed_mtrr_is_enabled(mtrr_state))</yellow>
			return;
<yellow>	} else if (msr == MSR_MTRRdefType) {</yellow>
		start = 0x0;
		end = ~0ULL;
	} else {
		/* variable range MTRRs. */
<yellow>		index = (msr - 0x200) / 2;</yellow>
		var_mtrr_range(&amp;mtrr_state-&gt;var_ranges[index], &amp;start, &amp;end);
	}

<yellow>	kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));</yellow>
}

static bool var_mtrr_range_is_valid(struct kvm_mtrr_range *range)
{
<blue>	return (range->mask & (1 << 11)) != 0;</blue>
}

static void set_var_mtrr_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
{
	struct kvm_mtrr *mtrr_state = &amp;vcpu-&gt;arch.mtrr_state;
	struct kvm_mtrr_range *tmp, *cur;
	int index, is_mtrr_mask;

<blue>	index = (msr - 0x200) / 2;</blue>
	is_mtrr_mask = msr - 0x200 - 2 * index;
	cur = &amp;mtrr_state-&gt;var_ranges[index];

	/* remove the entry if it&#x27;s in the list. */
	if (var_mtrr_range_is_valid(cur))
<yellow>		list_del(&mtrr_state->var_ranges[index].node);</yellow>

	/*
	 * Set all illegal GPA bits in the mask, since those bits must
	 * implicitly be 0.  The bits are then cleared when reading them.
	 */
<blue>	if (!is_mtrr_mask)</blue>
<blue>		cur->base = data;</blue>
	else
<blue>		cur->mask = data | kvm_vcpu_reserved_gpa_bits_raw(vcpu);</blue>

	/* add it to the list if it&#x27;s enabled. */
<blue>	if (var_mtrr_range_is_valid(cur)) {</blue>
<blue>		list_for_each_entry(tmp, &mtrr_state->head, node)</blue>
<yellow>			if (cur->base >= tmp->base)</yellow>
				break;
<blue>		list_add_tail(&cur->node, &tmp->node);</blue>
	}
}

int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
{
	int index;

<blue>	if (!kvm_mtrr_valid(vcpu, msr, data))</blue>
		return 1;

<blue>	index = fixed_msr_to_range_index(msr);</blue>
	if (index &gt;= 0)
<blue>		*(u64 *)&vcpu->arch.mtrr_state.fixed_ranges[index] = data;</blue>
<blue>	else if (msr == MSR_MTRRdefType)</blue>
<blue>		vcpu->arch.mtrr_state.deftype = data;</blue>
<blue>	else if (msr == MSR_IA32_CR_PAT)</blue>
<yellow>		vcpu->arch.pat = data;</yellow>
	else
<blue>		set_var_mtrr_msr(vcpu, msr, data);</blue>

<blue>	update_mtrr(vcpu, msr);</blue>
	return 0;
<blue>}</blue>

int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
{
	int index;

	/* MSR_MTRRcap is a readonly MSR. */
<blue>	if (msr == MSR_MTRRcap) {</blue>
		/*
		 * SMRR = 0
		 * WC = 1
		 * FIX = 1
		 * VCNT = KVM_NR_VAR_MTRR
		 */
<blue>		*pdata = 0x500 | KVM_NR_VAR_MTRR;</blue>
		return 0;
	}

<blue>	if (!msr_mtrr_valid(msr))</blue>
		return 1;

<blue>	index = fixed_msr_to_range_index(msr);</blue>
	if (index &gt;= 0)
<blue>		*pdata = *(u64 *)&vcpu->arch.mtrr_state.fixed_ranges[index];</blue>
<blue>	else if (msr == MSR_MTRRdefType)</blue>
<blue>		*pdata = vcpu->arch.mtrr_state.deftype;</blue>
<blue>	else if (msr == MSR_IA32_CR_PAT)</blue>
<blue>		*pdata = vcpu->arch.pat;</blue>
	else {	/* Variable MTRRs */
		int is_mtrr_mask;

<blue>		index = (msr - 0x200) / 2;</blue>
		is_mtrr_mask = msr - 0x200 - 2 * index;
		if (!is_mtrr_mask)
<blue>			*pdata = vcpu->arch.mtrr_state.var_ranges[index].base;</blue>
		else
<blue>			*pdata = vcpu->arch.mtrr_state.var_ranges[index].mask;</blue>

		*pdata &amp;= ~kvm_vcpu_reserved_gpa_bits_raw(vcpu);
	}

	return 0;
<blue>}</blue>

void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu)
{
<blue>	INIT_LIST_HEAD(&vcpu->arch.mtrr_state.head);</blue>
}

struct mtrr_iter {
	/* input fields. */
	struct kvm_mtrr *mtrr_state;
	u64 start;
	u64 end;

	/* output fields. */
	int mem_type;
	/* mtrr is completely disabled? */
	bool mtrr_disabled;
	/* [start, end) is not fully covered in MTRRs? */
	bool partial_map;

	/* private fields. */
	union {
		/* used for fixed MTRRs. */
		struct {
			int index;
			int seg;
		};

		/* used for var MTRRs. */
		struct {
			struct kvm_mtrr_range *range;
			/* max address has been covered in var MTRRs. */
			u64 start_max;
		};
	};

	bool fixed;
};

static bool mtrr_lookup_fixed_start(struct mtrr_iter *iter)
{
	int seg, index;

	if (!fixed_mtrr_is_enabled(iter-&gt;mtrr_state))
		return false;

<yellow>	seg = fixed_mtrr_addr_to_seg(iter->start);</yellow>
	if (seg &lt; 0)
		return false;

<yellow>	iter->fixed = true;</yellow>
<yellow>	index = fixed_mtrr_addr_seg_to_range_index(iter->start, seg);</yellow>
<yellow>	iter->index = index;</yellow>
	iter-&gt;seg = seg;
	return true;
}

static bool match_var_range(struct mtrr_iter *iter,
			    struct kvm_mtrr_range *range)
{
	u64 start, end;

<yellow>	var_mtrr_range(range, &start, &end);</yellow>
<yellow>	if (!(start >= iter->end || end <= iter->start)) {</yellow>
		iter-&gt;range = range;

		/*
		 * the function is called when we do kvm_mtrr.head walking.
		 * Range has the minimum base address which interleaves
		 * [looker-&gt;start_max, looker-&gt;end).
		 */
<yellow>		iter->partial_map |= iter->start_max < start;</yellow>

		/* update the max address has been covered. */
		iter-&gt;start_max = max(iter-&gt;start_max, end);
		return true;
	}

	return false;
}

<yellow>static void __mtrr_lookup_var_next(struct mtrr_iter *iter)</yellow>
{
<yellow>	struct kvm_mtrr *mtrr_state = iter->mtrr_state;</yellow>

<yellow>	list_for_each_entry_continue(iter->range, &mtrr_state->head, node)</yellow>
<yellow>		if (match_var_range(iter, iter->range))</yellow>
			return;

	iter-&gt;range = NULL;
<yellow>	iter->partial_map |= iter->start_max < iter->end;</yellow>
<yellow>}</yellow>

static void mtrr_lookup_var_start(struct mtrr_iter *iter)
{
<yellow>	struct kvm_mtrr *mtrr_state = iter->mtrr_state;</yellow>

<yellow>	iter->fixed = false;</yellow>
	iter-&gt;start_max = iter-&gt;start;
	iter-&gt;range = NULL;
	iter-&gt;range = list_prepare_entry(iter-&gt;range, &amp;mtrr_state-&gt;head, node);

	__mtrr_lookup_var_next(iter);
}

static void mtrr_lookup_fixed_next(struct mtrr_iter *iter)
{
	/* terminate the lookup. */
<yellow>	if (fixed_mtrr_range_end_addr(iter->seg, iter->index) >= iter->end) {</yellow>
<yellow>		iter->fixed = false;</yellow>
		iter-&gt;range = NULL;
		return;
	}

<yellow>	iter->index++;</yellow>

	/* have looked up for all fixed MTRRs. */
	if (iter-&gt;index &gt;= ARRAY_SIZE(iter-&gt;mtrr_state-&gt;fixed_ranges))
<yellow>		return mtrr_lookup_var_start(iter);</yellow>

	/* switch to next segment. */
<yellow>	if (iter->index > fixed_mtrr_seg_end_range_index(iter->seg))</yellow>
<yellow>		iter->seg++;</yellow>
}

static void mtrr_lookup_var_next(struct mtrr_iter *iter)
{
<yellow>	__mtrr_lookup_var_next(iter);</yellow>
}

static void mtrr_lookup_start(struct mtrr_iter *iter)
{
<yellow>	if (!mtrr_is_enabled(iter->mtrr_state)) {</yellow>
<yellow>		iter->mtrr_disabled = true;</yellow>
		return;
	}

<yellow>	if (!mtrr_lookup_fixed_start(iter))</yellow>
<yellow>		mtrr_lookup_var_start(iter);</yellow>
<yellow>}</yellow>

static void mtrr_lookup_init(struct mtrr_iter *iter,
			     struct kvm_mtrr *mtrr_state, u64 start, u64 end)
{
	iter-&gt;mtrr_state = mtrr_state;
	iter-&gt;start = start;
	iter-&gt;end = end;
	iter-&gt;mtrr_disabled = false;
	iter-&gt;partial_map = false;
	iter-&gt;fixed = false;
	iter-&gt;range = NULL;

	mtrr_lookup_start(iter);
}

<yellow>static bool mtrr_lookup_okay(struct mtrr_iter *iter)</yellow>
{
<yellow>	if (iter->fixed) {</yellow>
<yellow>		iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];</yellow>
		return true;
	}

<yellow>	if (iter->range) {</yellow>
<yellow>		iter->mem_type = iter->range->base & 0xff;</yellow>
		return true;
	}

	return false;
}

static void mtrr_lookup_next(struct mtrr_iter *iter)
{
<yellow>	if (iter->fixed)</yellow>
<yellow>		mtrr_lookup_fixed_next(iter);</yellow>
	else
<yellow>		mtrr_lookup_var_next(iter);</yellow>
<yellow>}</yellow>

#define mtrr_for_each_mem_type(_iter_, _mtrr_, _gpa_start_, _gpa_end_) \
	for (mtrr_lookup_init(_iter_, _mtrr_, _gpa_start_, _gpa_end_); \
	     mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))

u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
<yellow>{</yellow>
	struct kvm_mtrr *mtrr_state = &amp;vcpu-&gt;arch.mtrr_state;
	struct mtrr_iter iter;
	u64 start, end;
	int type = -1;
	const int wt_wb_mask = (1 &lt;&lt; MTRR_TYPE_WRBACK)
			       | (1 &lt;&lt; MTRR_TYPE_WRTHROUGH);

<yellow>	start = gfn_to_gpa(gfn);</yellow>
	end = start + PAGE_SIZE;

<yellow>	mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {</yellow>
		int curr_type = iter.mem_type;

		/*
		 * Please refer to Intel SDM Volume 3: 11.11.4.1 MTRR
		 * Precedences.
		 */

<yellow>		if (type == -1) {</yellow>
			type = curr_type;
			continue;
		}

		/*
		 * If two or more variable memory ranges match and the
		 * memory types are identical, then that memory type is
		 * used.
		 */
<yellow>		if (type == curr_type)</yellow>
			continue;

		/*
		 * If two or more variable memory ranges match and one of
		 * the memory types is UC, the UC memory type used.
		 */
<yellow>		if (curr_type == MTRR_TYPE_UNCACHABLE)</yellow>
			return MTRR_TYPE_UNCACHABLE;

		/*
		 * If two or more variable memory ranges match and the
		 * memory types are WT and WB, the WT memory type is used.
		 */
<yellow>		if (((1 << type) & wt_wb_mask) &&</yellow>
<yellow>		      ((1 << curr_type) & wt_wb_mask)) {</yellow>
			type = MTRR_TYPE_WRTHROUGH;
			continue;
		}

		/*
		 * For overlaps not defined by the above rules, processor
		 * behavior is undefined.
		 */

		/* We use WB for this undefined behavior. :( */
		return MTRR_TYPE_WRBACK;
	}

<yellow>	if (iter.mtrr_disabled)</yellow>
<yellow>		return mtrr_disabled_type(vcpu);</yellow>

	/* not contained in any MTRRs. */
<yellow>	if (type == -1)</yellow>
<yellow>		return mtrr_default_type(mtrr_state);</yellow>

	/*
	 * We just check one page, partially covered by MTRRs is
	 * impossible.
	 */
<yellow>	WARN_ON(iter.partial_map);</yellow>

<yellow>	return type;</yellow>
}
EXPORT_SYMBOL_GPL(kvm_mtrr_get_guest_memory_type);

bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
					  int page_num)
<yellow>{</yellow>
<yellow>	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;</yellow>
	struct mtrr_iter iter;
	u64 start, end;
	int type = -1;

	start = gfn_to_gpa(gfn);
	end = gfn_to_gpa(gfn + page_num);
<yellow>	mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {</yellow>
<yellow>		if (type == -1) {</yellow>
<yellow>			type = iter.mem_type;</yellow>
			continue;
		}

<yellow>		if (type != iter.mem_type)</yellow>
			return false;
	}

<yellow>	if (iter.mtrr_disabled)</yellow>
		return true;

<yellow>	if (!iter.partial_map)</yellow>
		return true;

<yellow>	if (type == -1)</yellow>
		return true;

<yellow>	return type == mtrr_default_type(mtrr_state);</yellow>
}


</code></pre></td></tr></table>
</body>
</html>
