<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">/*
 * 8253/8254 interval timer emulation
 *
 * Copyright (c) 2003-2004 Fabrice Bellard
 * Copyright (c) 2006 Intel Corporation
 * Copyright (c) 2007 Keir Fraser, XenSource Inc
 * Copyright (c) 2008 Intel Corporation
 * Copyright 2009 Red Hat, Inc. and/or its affiliates.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the &quot;Software&quot;), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 * Authors:
 *   Sheng Yang &lt;sheng.yang@intel.com&gt;
 *   Based on QEMU and Xen.
 */

#define pr_fmt(fmt) &quot;pit: &quot; fmt

#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/slab.h&gt;

#include &quot;ioapic.h&quot;
#include &quot;irq.h&quot;
#include &quot;i8254.h&quot;
#include &quot;x86.h&quot;

#ifndef CONFIG_X86_64
#define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
#else
#define mod_64(x, y) ((x) % (y))
#endif

#define RW_STATE_LSB 1
#define RW_STATE_MSB 2
#define RW_STATE_WORD0 3
#define RW_STATE_WORD1 4

static void pit_set_gate(struct kvm_pit *pit, int channel, u32 val)
{
	struct kvm_kpit_channel_state *c = &amp;pit-&gt;pit_state.channels[channel];

	switch (c-&gt;mode) {
	default:
	case 0:
	case 4:
		/* XXX: just disable/enable counting */
		break;
	case 1:
	case 2:
	case 3:
	case 5:
		/* Restart counting on rising edge. */
<yellow>		if (c->gate < val)</yellow>
<yellow>			c->count_load_time = ktime_get();</yellow>
		break;
	}

<yellow>	c->gate = val;</yellow>
}

static int pit_get_gate(struct kvm_pit *pit, int channel)
{
	return pit-&gt;pit_state.channels[channel].gate;
}

static s64 __kpit_elapsed(struct kvm_pit *pit)
{
	s64 elapsed;
	ktime_t remaining;
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;

<yellow>	if (!ps->period)</yellow>
		return 0;

	/*
	 * The Counter does not stop when it reaches zero. In
	 * Modes 0, 1, 4, and 5 the Counter ``wraps around&#x27;&#x27; to
	 * the highest count, either FFFF hex for binary counting
	 * or 9999 for BCD counting, and continues counting.
	 * Modes 2 and 3 are periodic; the Counter reloads
	 * itself with the initial count and continues counting
	 * from there.
	 */
<yellow>	remaining = hrtimer_get_remaining(&ps->timer);</yellow>
	elapsed = ps-&gt;period - ktime_to_ns(remaining);

	return elapsed;
}

<yellow>static s64 kpit_elapsed(struct kvm_pit *pit, struct kvm_kpit_channel_state *c,</yellow>
			int channel)
{
	if (channel == 0)
<yellow>		return __kpit_elapsed(pit);</yellow>

<yellow>	return ktime_to_ns(ktime_sub(ktime_get(), c->count_load_time));</yellow>
}

static int pit_get_count(struct kvm_pit *pit, int channel)
{
<yellow>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</yellow>
	s64 d, t;
	int counter;

<yellow>	t = kpit_elapsed(pit, c, channel);</yellow>
<yellow>	d = mul_u64_u32_div(t, KVM_PIT_FREQ, NSEC_PER_SEC);</yellow>

	switch (c-&gt;mode) {
	case 0:
	case 1:
	case 4:
	case 5:
<yellow>		counter = (c->count - d) & 0xffff;</yellow>
		break;
	case 3:
		/* XXX: may be incorrect for odd counts */
<yellow>		counter = c->count - (mod_64((2 * d), c->count));</yellow>
		break;
	default:
<yellow>		counter = c->count - mod_64(d, c->count);</yellow>
		break;
	}
	return counter;
<yellow>}</yellow>

static int pit_get_out(struct kvm_pit *pit, int channel)
{
<yellow>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</yellow>
	s64 d, t;
	int out;

<yellow>	t = kpit_elapsed(pit, c, channel);</yellow>
<yellow>	d = mul_u64_u32_div(t, KVM_PIT_FREQ, NSEC_PER_SEC);</yellow>

	switch (c-&gt;mode) {
	default:
	case 0:
<yellow>		out = (d >= c->count);</yellow>
		break;
	case 1:
<yellow>		out = (d < c->count);</yellow>
		break;
	case 2:
<yellow>		out = ((mod_64(d, c->count) == 0) && (d != 0));</yellow>
		break;
	case 3:
<yellow>		out = (mod_64(d, c->count) < ((c->count + 1) >> 1));</yellow>
		break;
	case 4:
	case 5:
<yellow>		out = (d == c->count);</yellow>
		break;
	}

	return out;
<yellow>}</yellow>

<yellow>static void pit_latch_count(struct kvm_pit *pit, int channel)</yellow>
{
<yellow>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</yellow>

	if (!c-&gt;count_latched) {
<yellow>		c->latched_count = pit_get_count(pit, channel);</yellow>
		c-&gt;count_latched = c-&gt;rw_mode;
	}
}

static void pit_latch_status(struct kvm_pit *pit, int channel)
{
<yellow>	struct kvm_kpit_channel_state *c = &pit->pit_state.channels[channel];</yellow>

	if (!c-&gt;status_latched) {
		/* TODO: Return NULL COUNT (bit 6). */
<yellow>		c->status = ((pit_get_out(pit, channel) << 7) |</yellow>
				(c-&gt;rw_mode &lt;&lt; 4) |
				(c-&gt;mode &lt;&lt; 1) |
				c-&gt;bcd);
		c-&gt;status_latched = 1;
	}
}

static inline struct kvm_pit *pit_state_to_pit(struct kvm_kpit_state *ps)
{
	return container_of(ps, struct kvm_pit, pit_state);
}

static void kvm_pit_ack_irq(struct kvm_irq_ack_notifier *kian)
{
	struct kvm_kpit_state *ps = container_of(kian, struct kvm_kpit_state,
						 irq_ack_notifier);
	struct kvm_pit *pit = pit_state_to_pit(ps);

<yellow>	atomic_set(&ps->irq_ack, 1);</yellow>
	/* irq_ack should be set before pending is read.  Order accesses with
	 * inc(pending) in pit_timer_fn and xchg(irq_ack, 0) in pit_do_work.
	 */
	smp_mb();
<yellow>	if (atomic_dec_if_positive(&ps->pending) > 0)</yellow>
<yellow>		kthread_queue_work(pit->worker, &pit->expired);</yellow>
<yellow>}</yellow>

void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)
{
<blue>	struct kvm_pit *pit = vcpu->kvm->arch.vpit;</blue>
	struct hrtimer *timer;

	/* Somewhat arbitrarily make vcpu0 the owner of the PIT. */
<blue>	if (vcpu->vcpu_id || !pit)</blue>
		return;

	timer = &amp;pit-&gt;pit_state.timer;
<blue>	mutex_lock(&pit->pit_state.lock);</blue>
	if (hrtimer_cancel(timer))
<yellow>		hrtimer_start_expires(timer, HRTIMER_MODE_ABS);</yellow>
<blue>	mutex_unlock(&pit->pit_state.lock);</blue>
<blue>}</blue>

static void destroy_pit_timer(struct kvm_pit *pit)
{
<blue>	hrtimer_cancel(&pit->pit_state.timer);</blue>
	kthread_flush_work(&amp;pit-&gt;expired);
}

static void pit_do_work(struct kthread_work *work)
<yellow>{</yellow>
	struct kvm_pit *pit = container_of(work, struct kvm_pit, expired);
<yellow>	struct kvm *kvm = pit->kvm;</yellow>
	struct kvm_vcpu *vcpu;
	unsigned long i;
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;

<yellow>	if (atomic_read(&ps->reinject) && !atomic_xchg(&ps->irq_ack, 0))</yellow>
<yellow>		return;</yellow>

<yellow>	kvm_set_irq(kvm, pit->irq_source_id, 0, 1, false);</yellow>
	kvm_set_irq(kvm, pit-&gt;irq_source_id, 0, 0, false);

	/*
	 * Provides NMI watchdog support via Virtual Wire mode.
	 * The route is: PIT -&gt; LVT0 in NMI mode.
	 *
	 * Note: Our Virtual Wire implementation does not follow
	 * the MP specification.  We propagate a PIT interrupt to all
	 * VCPUs and only when LVT0 is in NMI mode.  The interrupt can
	 * also be simultaneously delivered through PIC and IOAPIC.
	 */
	if (atomic_read(&amp;kvm-&gt;arch.vapics_in_nmi_mode) &gt; 0)
<yellow>		kvm_for_each_vcpu(i, vcpu, kvm)</yellow>
<yellow>			kvm_apic_nmi_wd_deliver(vcpu);</yellow>
}

static enum hrtimer_restart pit_timer_fn(struct hrtimer *data)
{
	struct kvm_kpit_state *ps = container_of(data, struct kvm_kpit_state, timer);
	struct kvm_pit *pt = pit_state_to_pit(ps);

<yellow>	if (atomic_read(&ps->reinject))</yellow>
<yellow>		atomic_inc(&ps->pending);</yellow>

<yellow>	kthread_queue_work(pt->worker, &pt->expired);</yellow>

<yellow>	if (ps->is_periodic) {</yellow>
<yellow>		hrtimer_add_expires_ns(&ps->timer, ps->period);</yellow>
		return HRTIMER_RESTART;
	} else
		return HRTIMER_NORESTART;
<yellow>}</yellow>

static inline void kvm_pit_reset_reinject(struct kvm_pit *pit)
{
<blue>	atomic_set(&pit->pit_state.pending, 0);</blue>
	atomic_set(&amp;pit-&gt;pit_state.irq_ack, 1);
}

void kvm_pit_set_reinject(struct kvm_pit *pit, bool reinject)
{
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;
<blue>	struct kvm *kvm = pit->kvm;</blue>

	if (atomic_read(&amp;ps-&gt;reinject) == reinject)
		return;

	/*
	 * AMD SVM AVIC accelerates EOI write and does not trap.
	 * This cause in-kernel PIT re-inject mode to fail
	 * since it checks ps-&gt;irq_ack before kvm_set_irq()
	 * and relies on the ack notifier to timely queue
	 * the pt-&gt;worker work iterm and reinject the missed tick.
	 * So, deactivate APICv when PIT is in reinject mode.
	 */
	if (reinject) {
		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
		/* The initial state is preserved while ps-&gt;reinject == 0. */
<blue>		kvm_pit_reset_reinject(pit);</blue>
<blue>		kvm_register_irq_ack_notifier(kvm, &ps->irq_ack_notifier);</blue>
		kvm_register_irq_mask_notifier(kvm, 0, &amp;pit-&gt;mask_notifier);
	} else {
<blue>		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);</blue>
		kvm_unregister_irq_ack_notifier(kvm, &amp;ps-&gt;irq_ack_notifier);
		kvm_unregister_irq_mask_notifier(kvm, 0, &amp;pit-&gt;mask_notifier);
	}

<blue>	atomic_set(&ps->reinject, reinject);</blue>
<blue>}</blue>

static void create_pit_timer(struct kvm_pit *pit, u32 val, int is_period)
{
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;
<blue>	struct kvm *kvm = pit->kvm;</blue>
	s64 interval;

<blue>	if (!ioapic_in_kernel(kvm) ||</blue>
<blue>	    ps->flags & KVM_PIT_FLAGS_HPET_LEGACY)</blue>
		return;

<blue>	interval = mul_u64_u32_div(val, NSEC_PER_SEC, KVM_PIT_FREQ);</blue>

<yellow>	pr_debug("create pit timer, interval is %llu nsec\n", interval);</yellow>

	/* TODO The new value only affected after the retriggered */
<blue>	hrtimer_cancel(&ps->timer);</blue>
	kthread_flush_work(&amp;pit-&gt;expired);
	ps-&gt;period = interval;
	ps-&gt;is_periodic = is_period;

	kvm_pit_reset_reinject(pit);

	/*
	 * Do not allow the guest to program periodic timers with small
	 * interval, since the hrtimers are not throttled by the host
	 * scheduler.
	 */
<blue>	if (ps->is_periodic) {</blue>
<blue>		s64 min_period = min_timer_period_us * 1000LL;</blue>

		if (ps-&gt;period &lt; min_period) {
<blue>			pr_info_ratelimited(</blue>
			    &quot;kvm: requested %lld ns &quot;
			    &quot;i8254 timer period limited to %lld ns\n&quot;,
			    ps-&gt;period, min_period);
<blue>			ps->period = min_period;</blue>
		}
	}

<blue>	hrtimer_start(&ps->timer, ktime_add_ns(ktime_get(), interval),</blue>
		      HRTIMER_MODE_ABS);
}

static void pit_load_count(struct kvm_pit *pit, int channel, u32 val)
{
	struct kvm_kpit_state *ps = &amp;pit-&gt;pit_state;

<blue>	pr_debug("load_count val is %u, channel is %d\n", val, channel);</blue>

	/*
	 * The largest possible initial count is 0; this is equivalent
	 * to 216 for binary counting and 104 for BCD counting.
	 */
<blue>	if (val == 0)</blue>
		val = 0x10000;

<blue>	ps->channels[channel].count = val;</blue>

	if (channel != 0) {
<blue>		ps->channels[channel].count_load_time = ktime_get();</blue>
		return;
	}

	/* Two types of timer
	 * mode 1 is one shot, mode 2 is period, otherwise del timer */
<blue>	switch (ps->channels[0].mode) {</blue>
	case 0:
	case 1:
        /* FIXME: enhance mode 4 precision */
	case 4:
<blue>		create_pit_timer(pit, val, 0);</blue>
		break;
	case 2:
	case 3:
<blue>		create_pit_timer(pit, val, 1);</blue>
		break;
	default:
<blue>		destroy_pit_timer(pit);</blue>
	}
<blue>}</blue>

void kvm_pit_load_count(struct kvm_pit *pit, int channel, u32 val,
		int hpet_legacy_start)
{
	u8 saved_mode;

<blue>	WARN_ON_ONCE(!mutex_is_locked(&pit->pit_state.lock));</blue>

<blue>	if (hpet_legacy_start) {</blue>
		/* save existing mode for later reenablement */
<blue>		WARN_ON(channel != 0);</blue>
<blue>		saved_mode = pit->pit_state.channels[0].mode;</blue>
		pit-&gt;pit_state.channels[0].mode = 0xff; /* disable timer */
		pit_load_count(pit, channel, val);
		pit-&gt;pit_state.channels[0].mode = saved_mode;
	} else {
<blue>		pit_load_count(pit, channel, val);</blue>
	}
<blue>}</blue>

static inline struct kvm_pit *dev_to_pit(struct kvm_io_device *dev)
{
	return container_of(dev, struct kvm_pit, dev);
}

static inline struct kvm_pit *speaker_to_pit(struct kvm_io_device *dev)
{
<yellow>	return container_of(dev, struct kvm_pit, speaker_dev);</yellow>
}

static inline int pit_in_range(gpa_t addr)
{
	return ((addr &gt;= KVM_PIT_BASE_ADDRESS) &amp;&amp;
		(addr &lt; KVM_PIT_BASE_ADDRESS + KVM_PIT_MEM_LENGTH));
}

static int pit_ioport_write(struct kvm_vcpu *vcpu,
				struct kvm_io_device *this,
			    gpa_t addr, int len, const void *data)
{
	struct kvm_pit *pit = dev_to_pit(this);
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
	int channel, access;
	struct kvm_kpit_channel_state *s;
<blue>	u32 val = *(u32 *) data;</blue>
	if (!pit_in_range(addr))
		return -EOPNOTSUPP;

<blue>	val  &= 0xff;</blue>
	addr &amp;= KVM_PIT_CHANNEL_MASK;

	mutex_lock(&amp;pit_state-&gt;lock);

	if (val != 0)
<blue>		pr_debug("write addr is 0x%x, len is %d, val is 0x%x\n",</blue>
			 (unsigned int)addr, len, val);

<blue>	if (addr == 3) {</blue>
<yellow>		channel = val >> 6;</yellow>
		if (channel == 3) {
			/* Read-Back Command. */
<yellow>			for (channel = 0; channel < 3; channel++) {</yellow>
<yellow>				if (val & (2 << channel)) {</yellow>
<yellow>					if (!(val & 0x20))</yellow>
<yellow>						pit_latch_count(pit, channel);</yellow>
<yellow>					if (!(val & 0x10))</yellow>
<yellow>						pit_latch_status(pit, channel);</yellow>
				}
			}
		} else {
			/* Select Counter &lt;channel&gt;. */
<yellow>			s = &pit_state->channels[channel];</yellow>
<yellow>			access = (val >> 4) & KVM_PIT_CHANNEL_MASK;</yellow>
<yellow>			if (access == 0) {</yellow>
<yellow>				pit_latch_count(pit, channel);</yellow>
			} else {
<yellow>				s->rw_mode = access;</yellow>
				s-&gt;read_state = access;
				s-&gt;write_state = access;
<yellow>				s->mode = (val >> 1) & 7;</yellow>
				if (s-&gt;mode &gt; 5)
<yellow>					s->mode -= 4;</yellow>
<yellow>				s->bcd = val & 1;</yellow>
			}
		}
	} else {
		/* Write Count. */
		s = &amp;pit_state-&gt;channels[addr];
<blue>		switch (s->write_state) {</blue>
		default:
		case RW_STATE_LSB:
<blue>			pit_load_count(pit, addr, val);</blue>
			break;
		case RW_STATE_MSB:
<yellow>			pit_load_count(pit, addr, val << 8);</yellow>
			break;
		case RW_STATE_WORD0:
<yellow>			s->write_latch = val;</yellow>
			s-&gt;write_state = RW_STATE_WORD1;
			break;
		case RW_STATE_WORD1:
<yellow>			pit_load_count(pit, addr, s->write_latch | (val << 8));</yellow>
			s-&gt;write_state = RW_STATE_WORD0;
			break;
		}
	}

<blue>	mutex_unlock(&pit_state->lock);</blue>
	return 0;
<blue>}</blue>

static int pit_ioport_read(struct kvm_vcpu *vcpu,
			   struct kvm_io_device *this,
			   gpa_t addr, int len, void *data)
<yellow>{</yellow>
	struct kvm_pit *pit = dev_to_pit(this);
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
	int ret, count;
	struct kvm_kpit_channel_state *s;
<yellow>	if (!pit_in_range(addr))</yellow>
		return -EOPNOTSUPP;

<yellow>	addr &= KVM_PIT_CHANNEL_MASK;</yellow>
	if (addr == 3)
		return 0;

	s = &amp;pit_state-&gt;channels[addr];

<yellow>	mutex_lock(&pit_state->lock);</yellow>

	if (s-&gt;status_latched) {
<yellow>		s->status_latched = 0;</yellow>
		ret = s-&gt;status;
<yellow>	} else if (s->count_latched) {</yellow>
		switch (s-&gt;count_latched) {
		default:
		case RW_STATE_LSB:
<yellow>			ret = s->latched_count & 0xff;</yellow>
			s-&gt;count_latched = 0;
			break;
		case RW_STATE_MSB:
<yellow>			ret = s->latched_count >> 8;</yellow>
			s-&gt;count_latched = 0;
			break;
		case RW_STATE_WORD0:
			ret = s-&gt;latched_count &amp; 0xff;
<yellow>			s->count_latched = RW_STATE_MSB;</yellow>
			break;
		}
	} else {
<yellow>		switch (s->read_state) {</yellow>
		default:
		case RW_STATE_LSB:
<yellow>			count = pit_get_count(pit, addr);</yellow>
			ret = count &amp; 0xff;
			break;
		case RW_STATE_MSB:
<yellow>			count = pit_get_count(pit, addr);</yellow>
			ret = (count &gt;&gt; 8) &amp; 0xff;
			break;
		case RW_STATE_WORD0:
<yellow>			count = pit_get_count(pit, addr);</yellow>
			ret = count &amp; 0xff;
			s-&gt;read_state = RW_STATE_WORD1;
			break;
		case RW_STATE_WORD1:
<yellow>			count = pit_get_count(pit, addr);</yellow>
			ret = (count &gt;&gt; 8) &amp; 0xff;
			s-&gt;read_state = RW_STATE_WORD0;
			break;
		}
	}

<yellow>	if (len > sizeof(ret))</yellow>
		len = sizeof(ret);
<yellow>	memcpy(data, (char *)&ret, len);</yellow>

	mutex_unlock(&amp;pit_state-&gt;lock);
	return 0;
}

static int speaker_ioport_write(struct kvm_vcpu *vcpu,
				struct kvm_io_device *this,
				gpa_t addr, int len, const void *data)
{
	struct kvm_pit *pit = speaker_to_pit(this);
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
<yellow>	u32 val = *(u32 *) data;</yellow>
	if (addr != KVM_SPEAKER_BASE_ADDRESS)
		return -EOPNOTSUPP;

<yellow>	mutex_lock(&pit_state->lock);</yellow>
	if (val &amp; (1 &lt;&lt; 1))
<yellow>		pit_state->flags |= KVM_PIT_FLAGS_SPEAKER_DATA_ON;</yellow>
	else
<yellow>		pit_state->flags &= ~KVM_PIT_FLAGS_SPEAKER_DATA_ON;</yellow>
<yellow>	pit_set_gate(pit, 2, val & 1);</yellow>
	mutex_unlock(&amp;pit_state-&gt;lock);
	return 0;
<yellow>}</yellow>

static int speaker_ioport_read(struct kvm_vcpu *vcpu,
				   struct kvm_io_device *this,
				   gpa_t addr, int len, void *data)
<yellow>{</yellow>
<yellow>	struct kvm_pit *pit = speaker_to_pit(this);</yellow>
	struct kvm_kpit_state *pit_state = &amp;pit-&gt;pit_state;
	unsigned int refresh_clock;
	int ret;
	if (addr != KVM_SPEAKER_BASE_ADDRESS)
		return -EOPNOTSUPP;

	/* Refresh clock toggles at about 15us. We approximate as 2^14ns. */
	refresh_clock = ((unsigned int)ktime_to_ns(ktime_get()) &gt;&gt; 14) &amp; 1;

<yellow>	mutex_lock(&pit_state->lock);</yellow>
	ret = (!!(pit_state-&gt;flags &amp; KVM_PIT_FLAGS_SPEAKER_DATA_ON) &lt;&lt; 1) |
		pit_get_gate(pit, 2) | (pit_get_out(pit, 2) &lt;&lt; 5) |
		(refresh_clock &lt;&lt; 4);
	if (len &gt; sizeof(ret))
		len = sizeof(ret);
<yellow>	memcpy(data, (char *)&ret, len);</yellow>
	mutex_unlock(&amp;pit_state-&gt;lock);
	return 0;
}

static void kvm_pit_reset(struct kvm_pit *pit)
{
	int i;
	struct kvm_kpit_channel_state *c;

	pit-&gt;pit_state.flags = 0;
	for (i = 0; i &lt; 3; i++) {
<blue>		c = &pit->pit_state.channels[i];</blue>
		c-&gt;mode = 0xff;
		c-&gt;gate = (i != 2);
		pit_load_count(pit, i, 0);
	}

<blue>	kvm_pit_reset_reinject(pit);</blue>
}

static void pit_mask_notifer(struct kvm_irq_mask_notifier *kimn, bool mask)
{
	struct kvm_pit *pit = container_of(kimn, struct kvm_pit, mask_notifier);

<yellow>	if (!mask)</yellow>
<yellow>		kvm_pit_reset_reinject(pit);</yellow>
<yellow>}</yellow>

static const struct kvm_io_device_ops pit_dev_ops = {
	.read     = pit_ioport_read,
	.write    = pit_ioport_write,
};

static const struct kvm_io_device_ops speaker_dev_ops = {
	.read     = speaker_ioport_read,
	.write    = speaker_ioport_write,
};

struct kvm_pit *kvm_create_pit(struct kvm *kvm, u32 flags)
{
	struct kvm_pit *pit;
	struct kvm_kpit_state *pit_state;
	struct pid *pid;
	pid_t pid_nr;
	int ret;

<blue>	pit = kzalloc(sizeof(struct kvm_pit), GFP_KERNEL_ACCOUNT);</blue>
	if (!pit)
		return NULL;

<blue>	pit->irq_source_id = kvm_request_irq_source_id(kvm);</blue>
	if (pit-&gt;irq_source_id &lt; 0)
		goto fail_request;

<blue>	mutex_init(&pit->pit_state.lock);</blue>

<blue>	pid = get_pid(task_tgid(current));</blue>
<blue>	pid_nr = pid_vnr(pid);</blue>
	put_pid(pid);

	pit-&gt;worker = kthread_create_worker(0, &quot;kvm-pit/%d&quot;, pid_nr);
	if (IS_ERR(pit-&gt;worker))
		goto fail_kthread;

<blue>	kthread_init_work(&pit->expired, pit_do_work);</blue>

	pit-&gt;kvm = kvm;

	pit_state = &amp;pit-&gt;pit_state;
	hrtimer_init(&amp;pit_state-&gt;timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
	pit_state-&gt;timer.function = pit_timer_fn;

	pit_state-&gt;irq_ack_notifier.gsi = 0;
	pit_state-&gt;irq_ack_notifier.irq_acked = kvm_pit_ack_irq;
	pit-&gt;mask_notifier.func = pit_mask_notifer;

<blue>	kvm_pit_reset(pit);</blue>

	kvm_pit_set_reinject(pit, true);

	mutex_lock(&amp;kvm-&gt;slots_lock);
	kvm_iodevice_init(&amp;pit-&gt;dev, &amp;pit_dev_ops);
	ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
				      KVM_PIT_MEM_LENGTH, &amp;pit-&gt;dev);
	if (ret &lt; 0)
		goto fail_register_pit;

<blue>	if (flags & KVM_PIT_SPEAKER_DUMMY) {</blue>
<blue>		kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);</blue>
		ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
					      KVM_SPEAKER_BASE_ADDRESS, 4,
					      &amp;pit-&gt;speaker_dev);
		if (ret &lt; 0)
			goto fail_register_speaker;
	}
<blue>	mutex_unlock(&kvm->slots_lock);</blue>

	return pit;

fail_register_speaker:
<yellow>	kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &pit->dev);</yellow>
fail_register_pit:
<yellow>	mutex_unlock(&kvm->slots_lock);</yellow>
	kvm_pit_set_reinject(pit, false);
	kthread_destroy_worker(pit-&gt;worker);
fail_kthread:
<yellow>	kvm_free_irq_source_id(kvm, pit->irq_source_id);</yellow>
fail_request:
<yellow>	kfree(pit);</yellow>
	return NULL;
<blue>}</blue>

void kvm_free_pit(struct kvm *kvm)
{
<yellow>	struct kvm_pit *pit = kvm->arch.vpit;</yellow>

	if (pit) {
<yellow>		mutex_lock(&kvm->slots_lock);</yellow>
		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &amp;pit-&gt;dev);
		kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &amp;pit-&gt;speaker_dev);
		mutex_unlock(&amp;kvm-&gt;slots_lock);
		kvm_pit_set_reinject(pit, false);
		hrtimer_cancel(&amp;pit-&gt;pit_state.timer);
		kthread_destroy_worker(pit-&gt;worker);
		kvm_free_irq_source_id(kvm, pit-&gt;irq_source_id);
		kfree(pit);
	}
<yellow>}</yellow>


</code></pre></td></tr></table>
</body>
</html>
