<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, 'Courier New';
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>831<br>832<br>833<br>834<br>835<br>836<br>837<br>838<br>839<br>840<br>841<br>842<br>843<br>844<br>845<br>846<br>847<br>848<br>849<br>850<br>851<br>852<br>853<br>854<br>855<br>856<br>857<br>858<br>859<br>860<br>861<br>862<br>863<br>864<br>865<br>866<br>867<br>868<br>869<br>870<br>871<br>872<br>873<br>874<br>875<br>876<br>877<br>878<br>879<br>880<br>881<br>882<br>883<br>884<br>885<br>886<br>887<br>888<br>889<br>890<br>891<br>892<br>893<br>894<br>895<br>896<br>897<br>898<br>899<br>900<br>901<br>902<br>903<br>904<br>905<br>906<br>907<br>908<br>909<br>910<br>911<br>912<br>913<br>914<br>915<br>916<br>917<br>918<br>919<br>920<br>921<br>922<br>923<br>924<br>925<br>926<br>927<br>928<br>929<br>930<br>931<br>932<br>933<br>934<br>935<br>936<br>937<br>938<br>939<br>940<br>941<br>942<br>943<br>944<br>945<br>946<br>947<br>948<br>949<br>950<br>951<br>952<br>953<br>954<br>955<br>956<br>957<br>958<br>959<br>960<br>961<br>962<br>963<br>964<br>965<br>966<br>967<br>968<br>969<br>970<br>971<br>972<br>973<br>974<br>975<br>976<br>977<br>978<br>979<br>980<br>981<br>982<br>983<br>984<br>985<br>986<br>987<br>988<br>989<br>990<br>991<br>992<br>993<br>994<br>995<br>996<br>997<br>998<br>999<br>1000<br>1001<br>1002<br>1003<br>1004<br>1005<br>1006<br>1007<br>1008<br>1009<br>1010<br>1011<br>1012<br>1013<br>1014<br>1015<br>1016<br>1017<br>1018<br>1019<br>1020<br>1021<br>1022<br>1023<br>1024<br>1025<br>1026<br>1027<br>1028<br>1029<br>1030<br>1031<br>1032<br>1033<br>1034<br>1035<br>1036<br>1037<br>1038<br>1039<br>1040<br>1041<br>1042<br>1043<br>1044<br>1045<br>1046<br>1047<br>1048<br>1049<br>1050<br>1051<br>1052<br>1053<br>1054<br>1055<br>1056<br>1057<br>1058<br>1059<br>1060<br>1061<br>1062<br>1063<br>1064<br>1065<br>1066<br>1067<br>1068<br>1069<br>1070<br>1071<br>1072<br>1073<br>1074<br>1075<br>1076<br>1077<br>1078<br>1079<br>1080<br>1081<br>1082<br>1083<br>1084<br>1085<br>1086<br>1087<br>1088<br>1089<br>1090<br>1091<br>1092<br>1093<br>1094<br>1095<br>1096<br>1097<br>1098<br>1099<br>1100<br>1101<br>1102<br>1103<br>1104<br>1105<br>1106<br>1107<br>1108<br>1109<br>1110<br>1111<br>1112<br>1113<br>1114<br>1115<br>1116<br>1117<br>1118<br>1119<br>1120<br>1121<br>1122<br>1123<br>1124<br>1125<br>1126<br>1127<br>1128<br>1129<br>1130<br>1131<br>1132<br>1133<br>1134<br>1135<br>1136<br>1137<br>1138<br>1139<br>1140<br>1141<br>1142<br>1143<br>1144<br>1145<br>1146<br>1147<br>1148<br>1149<br>1150<br>1151<br>1152<br>1153<br>1154<br>1155<br>1156<br>1157<br>1158<br>1159<br>1160<br>1161<br>1162<br>1163<br>1164<br>1165<br>1166<br>1167<br>1168<br>1169<br>1170<br>1171<br>1172<br>1173<br>1174<br>1175<br>1176<br>1177<br>1178<br>1179<br>1180<br>1181<br>1182<br>1183<br>1184<br>1185<br>1186<br>1187<br>1188<br>1189<br>1190<br>1191<br>1192<br>1193<br>1194<br>1195<br>1196<br>1197<br>1198<br>1199<br>1200<br>1201<br>1202<br>1203<br>1204<br>1205<br>1206<br>1207<br>1208<br>1209<br>1210<br>1211<br>1212<br>1213<br>1214<br>1215<br>1216<br>1217<br>1218<br>1219<br>1220<br>1221<br>1222<br>1223<br>1224<br>1225<br>1226<br>1227<br>1228<br>1229<br>1230<br>1231<br>1232<br>1233<br>1234<br>1235<br>1236<br>1237<br>1238<br>1239<br>1240<br>1241<br>1242<br>1243<br>1244<br>1245<br>1246<br>1247<br>1248<br>1249<br>1250<br>1251<br>1252<br>1253<br>1254<br>1255<br>1256<br>1257<br>1258<br>1259<br>1260<br>1261<br>1262<br>1263<br>1264<br>1265<br>1266<br>1267<br>1268<br>1269<br>1270<br>1271<br>1272<br>1273<br>1274<br>1275<br>1276<br>1277<br>1278<br>1279<br>1280<br>1281<br>1282<br>1283<br>1284<br>1285<br>1286<br>1287<br>1288<br>1289<br>1290<br>1291<br>1292<br>1293<br>1294<br>1295<br>1296<br>1297<br>1298<br>1299<br>1300<br>1301<br>1302<br>1303<br>1304<br>1305<br>1306<br>1307<br>1308<br>1309<br>1310<br>1311<br>1312<br>1313<br>1314<br>1315<br>1316<br>1317<br>1318<br>1319<br>1320<br>1321<br>1322<br>1323<br>1324<br>1325<br>1326<br>1327<br>1328<br>1329<br>1330<br>1331<br>1332<br>1333<br>1334<br>1335<br>1336<br>1337<br>1338<br>1339<br>1340<br>1341<br>1342<br>1343<br>1344<br>1345<br>1346<br>1347<br>1348<br>1349<br>1350<br>1351<br>1352<br>1353<br>1354<br>1355<br>1356<br>1357<br>1358<br>1359<br>1360<br>1361<br>1362<br>1363<br>1364<br>1365<br>1366<br>1367<br>1368<br>1369<br>1370<br>1371<br>1372<br>1373<br>1374<br>1375<br>1376<br>1377<br>1378<br>1379<br>1380<br>1381<br>1382<br>1383<br>1384<br>1385<br>1386<br>1387<br>1388<br>1389<br>1390<br>1391<br>1392<br>1393<br>1394<br>1395<br>1396<br>1397<br>1398<br>1399<br>1400<br>1401<br>1402<br>1403<br>1404<br>1405<br>1406<br>1407<br>1408<br>1409<br>1410<br>1411<br>1412<br>1413<br>1414<br>1415<br>1416<br>1417<br>1418<br>1419<br>1420<br>1421<br>1422<br>1423<br>1424<br>1425<br>1426<br>1427<br>1428<br>1429<br>1430<br>1431<br>1432<br>1433<br>1434<br>1435<br>1436<br>1437<br>1438<br>1439<br>1440<br>1441<br>1442<br>1443<br>1444<br>1445<br>1446<br>1447<br>1448<br>1449<br>1450<br>1451<br>1452<br>1453<br>1454<br>1455<br>1456<br>1457<br>1458<br>1459<br>1460<br>1461<br>1462<br>1463<br>1464<br>1465<br>1466<br>1467<br>1468<br>1469<br>1470<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">/* SPDX-License-Identifier: GPL-2.0 */
#ifndef _ASM_X86_PGTABLE_H
#define _ASM_X86_PGTABLE_H

#include &lt;linux/mem_encrypt.h&gt;
#include &lt;asm/page.h&gt;
#include &lt;asm/pgtable_types.h&gt;

/*
 * Macro to mark a page protection value as UC-
 */
#define pgprot_noncached(prot)						\
	((boot_cpu_data.x86 &gt; 3)					\
	 ? (__pgprot(pgprot_val(prot) |					\
		     cachemode2protval(_PAGE_CACHE_MODE_UC_MINUS)))	\
	 : (prot))

#ifndef __ASSEMBLY__
#include &lt;linux/spinlock.h&gt;
#include &lt;asm/x86_init.h&gt;
#include &lt;asm/pkru.h&gt;
#include &lt;asm/fpu/api.h&gt;
#include &lt;asm/coco.h&gt;
#include &lt;asm-generic/pgtable_uffd.h&gt;
#include &lt;linux/page_table_check.h&gt;

extern pgd_t early_top_pgt[PTRS_PER_PGD];
bool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);

void ptdump_walk_pgd_level(struct seq_file *m, struct mm_struct *mm);
void ptdump_walk_pgd_level_debugfs(struct seq_file *m, struct mm_struct *mm,
				   bool user);
void ptdump_walk_pgd_level_checkwx(void);
void ptdump_walk_user_pgd_level_checkwx(void);

/*
 * Macros to add or remove encryption attribute
 */
#define pgprot_encrypted(prot)	__pgprot(cc_mkenc(pgprot_val(prot)))
#define pgprot_decrypted(prot)	__pgprot(cc_mkdec(pgprot_val(prot)))

#ifdef CONFIG_DEBUG_WX
#define debug_checkwx()		ptdump_walk_pgd_level_checkwx()
#define debug_checkwx_user()	ptdump_walk_user_pgd_level_checkwx()
#else
#define debug_checkwx()		do { } while (0)
#define debug_checkwx_user()	do { } while (0)
#endif

/*
 * ZERO_PAGE is a global shared page that is always zero: used
 * for zero-mapped memory areas etc..
 */
extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]
	__visible;
#define ZERO_PAGE(vaddr) ((void)(vaddr),virt_to_page(empty_zero_page))

extern spinlock_t pgd_lock;
extern struct list_head pgd_list;

extern struct mm_struct *pgd_page_get_mm(struct page *page);

extern pmdval_t early_pmd_flags;

#ifdef CONFIG_PARAVIRT_XXL
#include &lt;asm/paravirt.h&gt;
#else  /* !CONFIG_PARAVIRT_XXL */
#define set_pte(ptep, pte)		native_set_pte(ptep, pte)

#define set_pte_atomic(ptep, pte)					\
	native_set_pte_atomic(ptep, pte)

#define set_pmd(pmdp, pmd)		native_set_pmd(pmdp, pmd)

#ifndef __PAGETABLE_P4D_FOLDED
#define set_pgd(pgdp, pgd)		native_set_pgd(pgdp, pgd)
#define pgd_clear(pgd)			(pgtable_l5_enabled() ? native_pgd_clear(pgd) : 0)
#endif

#ifndef set_p4d
# define set_p4d(p4dp, p4d)		native_set_p4d(p4dp, p4d)
#endif

#ifndef __PAGETABLE_PUD_FOLDED
#define p4d_clear(p4d)			native_p4d_clear(p4d)
#endif

#ifndef set_pud
# define set_pud(pudp, pud)		native_set_pud(pudp, pud)
#endif

#ifndef __PAGETABLE_PUD_FOLDED
#define pud_clear(pud)			native_pud_clear(pud)
#endif

#define pte_clear(mm, addr, ptep)	native_pte_clear(mm, addr, ptep)
#define pmd_clear(pmd)			native_pmd_clear(pmd)

#define pgd_val(x)	native_pgd_val(x)
#define __pgd(x)	native_make_pgd(x)

#ifndef __PAGETABLE_P4D_FOLDED
#define p4d_val(x)	native_p4d_val(x)
#define __p4d(x)	native_make_p4d(x)
#endif

#ifndef __PAGETABLE_PUD_FOLDED
#define pud_val(x)	native_pud_val(x)
#define __pud(x)	native_make_pud(x)
#endif

#ifndef __PAGETABLE_PMD_FOLDED
#define pmd_val(x)	native_pmd_val(x)
#define __pmd(x)	native_make_pmd(x)
#endif

#define pte_val(x)	native_pte_val(x)
#define __pte(x)	native_make_pte(x)

#define arch_end_context_switch(prev)	do {} while(0)
#endif	/* CONFIG_PARAVIRT_XXL */

/*
 * The following only work if pte_present() is true.
 * Undefined behaviour if not..
 */
static inline int pte_dirty(pte_t pte)
{
<blue>	return pte_flags(pte) & _PAGE_DIRTY;</blue>
}

static inline int pte_young(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_ACCESSED;</yellow>
}

static inline int pmd_dirty(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_DIRTY;</yellow>
}

#define pmd_young pmd_young
static inline int pmd_young(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_ACCESSED;</yellow>
}

static inline int pud_dirty(pud_t pud)
{
	return pud_flags(pud) &amp; _PAGE_DIRTY;
}

static inline int pud_young(pud_t pud)
{
<yellow>	return pud_flags(pud) & _PAGE_ACCESSED;</yellow>
}

static inline int pte_write(pte_t pte)
{
<blue>	return pte_flags(pte) & _PAGE_RW;</blue>
}

static inline int pte_huge(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_PSE;</yellow>
}

static inline int pte_global(pte_t pte)
{
	return pte_flags(pte) &amp; _PAGE_GLOBAL;
}

static inline int pte_exec(pte_t pte)
{
	return !(pte_flags(pte) &amp; _PAGE_NX);
}

static inline int pte_special(pte_t pte)
{
<blue>	return pte_flags(pte) & _PAGE_SPECIAL;</blue>
}

/* Entries that were set to PROT_NONE are inverted */

static inline u64 protnone_mask(u64 val);

static inline unsigned long pte_pfn(pte_t pte)
{
<blue>	phys_addr_t pfn = pte_val(pte);</blue>
<blue>	pfn ^= protnone_mask(pfn);</blue>
<blue>	return (pfn & PTE_PFN_MASK) >> PAGE_SHIFT;</blue>
<yellow>}</yellow>

static inline unsigned long pmd_pfn(pmd_t pmd)
{
<blue>	phys_addr_t pfn = pmd_val(pmd);</blue>
<blue>	pfn ^= protnone_mask(pfn);</blue>
<blue>	return (pfn & pmd_pfn_mask(pmd)) >> PAGE_SHIFT;</blue>
}

static inline unsigned long pud_pfn(pud_t pud)
{
<yellow>	phys_addr_t pfn = pud_val(pud);</yellow>
<yellow>	pfn ^= protnone_mask(pfn);</yellow>
<yellow>	return (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;</yellow>
}

static inline unsigned long p4d_pfn(p4d_t p4d)
{
<yellow>	return (p4d_val(p4d) & p4d_pfn_mask(p4d)) >> PAGE_SHIFT;</yellow>
}

static inline unsigned long pgd_pfn(pgd_t pgd)
{
<yellow>	return (pgd_val(pgd) & PTE_PFN_MASK) >> PAGE_SHIFT;</yellow>
}

#define p4d_leaf	p4d_large
static inline int p4d_large(p4d_t p4d)
{
	/* No 512 GiB pages yet */
	return 0;
}

#define pte_page(pte)	pfn_to_page(pte_pfn(pte))

#define pmd_leaf	pmd_large
static inline int pmd_large(pmd_t pte)
{
<blue>	return pmd_flags(pte) & _PAGE_PSE;</blue>
}

#ifdef CONFIG_TRANSPARENT_HUGEPAGE
/* NOTE: when predicate huge page, consider also pmd_devmap, or use pmd_large */
static inline int pmd_trans_huge(pmd_t pmd)
{
<blue>	return (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;</blue>
}

#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
static inline int pud_trans_huge(pud_t pud)
{
<blue>	return (pud_val(pud) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;</blue>
}
#endif

#define has_transparent_hugepage has_transparent_hugepage
static inline int has_transparent_hugepage(void)
{
<yellow>	return boot_cpu_has(X86_FEATURE_PSE);</yellow>
}

#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
static inline int pmd_devmap(pmd_t pmd)
{
<blue>	return !!(pmd_val(pmd) & _PAGE_DEVMAP);</blue>
}

#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
static inline int pud_devmap(pud_t pud)
{
<blue>	return !!(pud_val(pud) & _PAGE_DEVMAP);</blue>
}
#else
static inline int pud_devmap(pud_t pud)
{
	return 0;
}
#endif

static inline int pgd_devmap(pgd_t pgd)
{
	return 0;
}
#endif
#endif /* CONFIG_TRANSPARENT_HUGEPAGE */

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
<yellow>	pteval_t v = native_pte_val(pte);</yellow>

<blue>	return native_make_pte(v | set);</blue>
}

static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
{
<yellow>	pteval_t v = native_pte_val(pte);</yellow>

<yellow>	return native_make_pte(v & ~clear);</yellow>
}

#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
static inline int pte_uffd_wp(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_UFFD_WP;</yellow>
}

static inline pte_t pte_mkuffd_wp(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_UFFD_WP);</yellow>
}

static inline pte_t pte_clear_uffd_wp(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_UFFD_WP);</yellow>
}
#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */

static inline pte_t pte_mkclean(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_DIRTY);</yellow>
}

static inline pte_t pte_mkold(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_ACCESSED);</yellow>
}

static inline pte_t pte_wrprotect(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_RW);</yellow>
}

static inline pte_t pte_mkexec(pte_t pte)
{
	return pte_clear_flags(pte, _PAGE_NX);
}

static inline pte_t pte_mkdirty(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);</yellow>
}

static inline pte_t pte_mkyoung(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_ACCESSED);</yellow>
}

static inline pte_t pte_mkwrite(pte_t pte)
{
<blue>	return pte_set_flags(pte, _PAGE_RW);</blue>
}

static inline pte_t pte_mkhuge(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_PSE);</yellow>
}

static inline pte_t pte_clrhuge(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_PSE);</yellow>
}

static inline pte_t pte_mkglobal(pte_t pte)
{
	return pte_set_flags(pte, _PAGE_GLOBAL);
}

static inline pte_t pte_clrglobal(pte_t pte)
{
	return pte_clear_flags(pte, _PAGE_GLOBAL);
}

static inline pte_t pte_mkspecial(pte_t pte)
{
<blue>	return pte_set_flags(pte, _PAGE_SPECIAL);</blue>
}

static inline pte_t pte_mkdevmap(pte_t pte)
{
	return pte_set_flags(pte, _PAGE_SPECIAL|_PAGE_DEVMAP);
}

static inline pmd_t pmd_set_flags(pmd_t pmd, pmdval_t set)
{
<yellow>	pmdval_t v = native_pmd_val(pmd);</yellow>

<yellow>	return native_make_pmd(v | set);</yellow>
}

static inline pmd_t pmd_clear_flags(pmd_t pmd, pmdval_t clear)
{
	pmdval_t v = native_pmd_val(pmd);

<yellow>	return native_make_pmd(v & ~clear);</yellow>
}

#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
static inline int pmd_uffd_wp(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_UFFD_WP;</yellow>
}

static inline pmd_t pmd_mkuffd_wp(pmd_t pmd)
{
	return pmd_set_flags(pmd, _PAGE_UFFD_WP);
}

static inline pmd_t pmd_clear_uffd_wp(pmd_t pmd)
{
<yellow>	return pmd_clear_flags(pmd, _PAGE_UFFD_WP);</yellow>
}
#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */

static inline pmd_t pmd_mkold(pmd_t pmd)
{
<yellow>	return pmd_clear_flags(pmd, _PAGE_ACCESSED);</yellow>
}

static inline pmd_t pmd_mkclean(pmd_t pmd)
{
	return pmd_clear_flags(pmd, _PAGE_DIRTY);
}

static inline pmd_t pmd_wrprotect(pmd_t pmd)
{
<yellow>	return pmd_clear_flags(pmd, _PAGE_RW);</yellow>
}

static inline pmd_t pmd_mkdirty(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);</yellow>
}

static inline pmd_t pmd_mkdevmap(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_DEVMAP);</yellow>
}

static inline pmd_t pmd_mkhuge(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_PSE);</yellow>
}

static inline pmd_t pmd_mkyoung(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_ACCESSED);</yellow>
}

static inline pmd_t pmd_mkwrite(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_RW);</yellow>
}

static inline pud_t pud_set_flags(pud_t pud, pudval_t set)
{
<yellow>	pudval_t v = native_pud_val(pud);</yellow>

<yellow>	return native_make_pud(v | set);</yellow>
}

static inline pud_t pud_clear_flags(pud_t pud, pudval_t clear)
{
	pudval_t v = native_pud_val(pud);

	return native_make_pud(v &amp; ~clear);
}

static inline pud_t pud_mkold(pud_t pud)
{
	return pud_clear_flags(pud, _PAGE_ACCESSED);
}

static inline pud_t pud_mkclean(pud_t pud)
{
	return pud_clear_flags(pud, _PAGE_DIRTY);
}

static inline pud_t pud_wrprotect(pud_t pud)
{
	return pud_clear_flags(pud, _PAGE_RW);
}

static inline pud_t pud_mkdirty(pud_t pud)
{
<yellow>	return pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);</yellow>
}

static inline pud_t pud_mkdevmap(pud_t pud)
{
<yellow>	return pud_set_flags(pud, _PAGE_DEVMAP);</yellow>
}

static inline pud_t pud_mkhuge(pud_t pud)
{
<yellow>	return pud_set_flags(pud, _PAGE_PSE);</yellow>
}

static inline pud_t pud_mkyoung(pud_t pud)
{
<yellow>	return pud_set_flags(pud, _PAGE_ACCESSED);</yellow>
}

static inline pud_t pud_mkwrite(pud_t pud)
{
<yellow>	return pud_set_flags(pud, _PAGE_RW);</yellow>
}

#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
static inline int pte_soft_dirty(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_SOFT_DIRTY;</yellow>
}

static inline int pmd_soft_dirty(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_SOFT_DIRTY;</yellow>
}

static inline int pud_soft_dirty(pud_t pud)
{
	return pud_flags(pud) &amp; _PAGE_SOFT_DIRTY;
}

static inline pte_t pte_mksoft_dirty(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);</yellow>
}

static inline pmd_t pmd_mksoft_dirty(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);</yellow>
}

static inline pud_t pud_mksoft_dirty(pud_t pud)
{
	return pud_set_flags(pud, _PAGE_SOFT_DIRTY);
}

static inline pte_t pte_clear_soft_dirty(pte_t pte)
{
	return pte_clear_flags(pte, _PAGE_SOFT_DIRTY);
}

static inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)
{
<yellow>	return pmd_clear_flags(pmd, _PAGE_SOFT_DIRTY);</yellow>
}

static inline pud_t pud_clear_soft_dirty(pud_t pud)
{
	return pud_clear_flags(pud, _PAGE_SOFT_DIRTY);
}

#endif /* CONFIG_HAVE_ARCH_SOFT_DIRTY */

/*
 * Mask out unsupported bits in a present pgprot.  Non-present pgprots
 * can use those bits for other purposes, so leave them be.
 */
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
<yellow>	pgprotval_t protval = pgprot_val(pgprot);</yellow>

<blue>	if (protval & _PAGE_PRESENT)</blue>
<blue>		protval &= __supported_pte_mask;</blue>

	return protval;
}

static inline pgprotval_t check_pgprot(pgprot_t pgprot)
{
<blue>	pgprotval_t massaged_val = massage_pgprot(pgprot);</blue>

	/* mmdebug.h can not be included here because of dependencies */
#ifdef CONFIG_DEBUG_VM
	WARN_ONCE(pgprot_val(pgprot) != massaged_val,
		  &quot;attempted to set unsupported pgprot: %016llx &quot;
		  &quot;bits: %016llx supported: %016llx\n&quot;,
		  (u64)pgprot_val(pgprot),
		  (u64)pgprot_val(pgprot) ^ massaged_val,
		  (u64)__supported_pte_mask);
#endif

<yellow>	return massaged_val;</yellow>
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
<yellow>	phys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;</yellow>
<blue>	pfn ^= protnone_mask(pgprot_val(pgprot));</blue>
<yellow>	pfn &= PTE_PFN_MASK;</yellow>
<blue>	return __pte(pfn | check_pgprot(pgprot));</blue>
}

static inline pmd_t pfn_pmd(unsigned long page_nr, pgprot_t pgprot)
{
<yellow>	phys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;</yellow>
<yellow>	pfn ^= protnone_mask(pgprot_val(pgprot));</yellow>
<yellow>	pfn &= PHYSICAL_PMD_PAGE_MASK;</yellow>
<yellow>	return __pmd(pfn | check_pgprot(pgprot));</yellow>
}

static inline pud_t pfn_pud(unsigned long page_nr, pgprot_t pgprot)
{
<yellow>	phys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;</yellow>
<yellow>	pfn ^= protnone_mask(pgprot_val(pgprot));</yellow>
<yellow>	pfn &= PHYSICAL_PUD_PAGE_MASK;</yellow>
<yellow>	return __pud(pfn | check_pgprot(pgprot));</yellow>
}

static inline pmd_t pmd_mkinvalid(pmd_t pmd)
{
<yellow>	return pfn_pmd(pmd_pfn(pmd),</yellow>
<yellow>		      __pgprot(pmd_flags(pmd) & ~(_PAGE_PRESENT|_PAGE_PROTNONE)));</yellow>
}

static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);

static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
{
	pteval_t val = pte_val(pte), oldval = val;

	/*
	 * Chop off the NX bit (if present), and add the NX portion of
	 * the newprot (if present):
	 */
	val &amp;= _PAGE_CHG_MASK;
<yellow>	val |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;</yellow>
<yellow>	val = flip_protnone_guard(oldval, val, PTE_PFN_MASK);</yellow>
<yellow>	return __pte(val);</yellow>
}

static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
{
	pmdval_t val = pmd_val(pmd), oldval = val;

	val &amp;= _HPAGE_CHG_MASK;
<yellow>	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;</yellow>
<yellow>	val = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);</yellow>
<yellow>	return __pmd(val);</yellow>
}

/*
 * mprotect needs to preserve PAT and encryption bits when updating
 * vm_page_prot
 */
#define pgprot_modify pgprot_modify
static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)
{
	pgprotval_t preservebits = pgprot_val(oldprot) &amp; _PAGE_CHG_MASK;
	pgprotval_t addbits = pgprot_val(newprot) &amp; ~_PAGE_CHG_MASK;
	return __pgprot(preservebits | addbits);
}

#define pte_pgprot(x) __pgprot(pte_flags(x))
#define pmd_pgprot(x) __pgprot(pmd_flags(x))
#define pud_pgprot(x) __pgprot(pud_flags(x))
#define p4d_pgprot(x) __pgprot(p4d_flags(x))

#define canon_pgprot(p) __pgprot(massage_pgprot(p))

<yellow>static inline int is_new_memtype_allowed(u64 paddr, unsigned long size,</yellow>
					 enum page_cache_mode pcm,
					 enum page_cache_mode new_pcm)
{
	/*
	 * PAT type is always WB for untracked ranges, so no need to check.
	 */
<yellow>	if (x86_platform.is_untracked_pat_range(paddr, paddr + size))</yellow>
		return 1;

	/*
	 * Certain new memtypes are not allowed with certain
	 * requested memtype:
	 * - request is uncached, return cannot be write-back
	 * - request is write-combine, return cannot be write-back
	 * - request is write-through, return cannot be write-back
	 * - request is write-through, return cannot be write-combine
	 */
	if ((pcm == _PAGE_CACHE_MODE_UC_MINUS &amp;&amp;
<yellow>	     new_pcm == _PAGE_CACHE_MODE_WB) ||</yellow>
	    (pcm == _PAGE_CACHE_MODE_WC &amp;&amp;
<yellow>	     new_pcm == _PAGE_CACHE_MODE_WB) ||</yellow>
	    (pcm == _PAGE_CACHE_MODE_WT &amp;&amp;
<yellow>	     new_pcm == _PAGE_CACHE_MODE_WB) ||</yellow>
<yellow>	    (pcm == _PAGE_CACHE_MODE_WT &&</yellow>
	     new_pcm == _PAGE_CACHE_MODE_WC)) {
		return 0;
	}

	return 1;
}

pmd_t *populate_extra_pmd(unsigned long vaddr);
pte_t *populate_extra_pte(unsigned long vaddr);

#ifdef CONFIG_PAGE_TABLE_ISOLATION
pgd_t __pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd);

/*
 * Take a PGD location (pgdp) and a pgd value that needs to be set there.
 * Populates the user and returns the resulting PGD that must be set in
 * the kernel copy of the page tables.
 */
static inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)
{
	if (!static_cpu_has(X86_FEATURE_PTI))
		return pgd;
	return __pti_set_user_pgtbl(pgdp, pgd);
}
#else   /* CONFIG_PAGE_TABLE_ISOLATION */
static inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)
{
	return pgd;
}
#endif  /* CONFIG_PAGE_TABLE_ISOLATION */

#endif	/* __ASSEMBLY__ */


#ifdef CONFIG_X86_32
# include &lt;asm/pgtable_32.h&gt;
#else
# include &lt;asm/pgtable_64.h&gt;
#endif

#ifndef __ASSEMBLY__
#include &lt;linux/mm_types.h&gt;
#include &lt;linux/mmdebug.h&gt;
#include &lt;linux/log2.h&gt;
#include &lt;asm/fixmap.h&gt;

static inline int pte_none(pte_t pte)
{
<yellow>	return !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));</yellow>
}

#define __HAVE_ARCH_PTE_SAME
static inline int pte_same(pte_t a, pte_t b)
{
<blue>	return a.pte == b.pte;</blue>
}

static inline int pte_present(pte_t a)
{
<blue>	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);</blue>
}

#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
static inline int pte_devmap(pte_t a)
{
<blue>	return (pte_flags(a) & _PAGE_DEVMAP) == _PAGE_DEVMAP;</blue>
}
#endif

#define pte_accessible pte_accessible
static inline bool pte_accessible(struct mm_struct *mm, pte_t a)
{
	if (pte_flags(a) &amp; _PAGE_PRESENT)
		return true;

<yellow>	if ((pte_flags(a) & _PAGE_PROTNONE) &&</yellow>
<yellow>			atomic_read(&mm->tlb_flush_pending))</yellow>
		return true;

	return false;
}

static inline int pmd_present(pmd_t pmd)
{
	/*
	 * Checking for _PAGE_PSE is needed too because
	 * split_huge_page will temporarily clear the present bit (but
	 * the _PAGE_PSE flag will remain set at all times while the
	 * _PAGE_PRESENT bit is clear).
	 */
<blue>	return pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);</blue>
}

#ifdef CONFIG_NUMA_BALANCING
/*
 * These work without NUMA balancing but the kernel does not care. See the
 * comment in include/linux/pgtable.h
 */
static inline int pte_protnone(pte_t pte)
{
<blue>	return (pte_flags(pte) & (_PAGE_PROTNONE | _PAGE_PRESENT))</blue>
		== _PAGE_PROTNONE;
}

static inline int pmd_protnone(pmd_t pmd)
{
<yellow>	return (pmd_flags(pmd) & (_PAGE_PROTNONE | _PAGE_PRESENT))</yellow>
		== _PAGE_PROTNONE;
}
#endif /* CONFIG_NUMA_BALANCING */

static inline int pmd_none(pmd_t pmd)
{
	/* Only check low word on 32-bit platforms, since it might be
	   out of sync with upper half. */
<blue>	unsigned long val = native_pmd_val(pmd);</blue>
<yellow>	return (val & ~_PAGE_KNL_ERRATUM_MASK) == 0;</yellow>
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
<blue>	return (unsigned long)__va(pmd_val(pmd) & pmd_pfn_mask(pmd));</blue>
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h&#x27;s __section_mem_map_addr() definition:
 */
#define pmd_page(pmd)	pfn_to_page(pmd_pfn(pmd))

/*
 * Conversion functions: convert a page and protection to a page entry,
 * and a page entry and page directory to the page they refer to.
 *
 * (Currently stuck as a macro because of indirect forward reference
 * to linux/mm.h:page_to_nid())
 */
#define mk_pte(page, pgprot)   pfn_pte(page_to_pfn(page), (pgprot))

static inline int pmd_bad(pmd_t pmd)
{
<blue>	return (pmd_flags(pmd) & ~(_PAGE_USER | _PAGE_ACCESSED)) !=</blue>
<blue>	       (_KERNPG_TABLE & ~_PAGE_ACCESSED);</blue>
}

static inline unsigned long pages_to_mb(unsigned long npg)
{
	return npg &gt;&gt; (20 - PAGE_SHIFT);
}

#if CONFIG_PGTABLE_LEVELS &gt; 2
static inline int pud_none(pud_t pud)
{
<blue>	return (native_pud_val(pud) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;</blue>
}

static inline int pud_present(pud_t pud)
{
<blue>	return pud_flags(pud) & _PAGE_PRESENT;</blue>
}

static inline pmd_t *pud_pgtable(pud_t pud)
{
<blue>	return (pmd_t *)__va(pud_val(pud) & pud_pfn_mask(pud));</blue>
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h&#x27;s __section_mem_map_addr() definition:
 */
#define pud_page(pud)	pfn_to_page(pud_pfn(pud))

#define pud_leaf	pud_large
static inline int pud_large(pud_t pud)
{
<blue>	return (pud_val(pud) & (_PAGE_PSE | _PAGE_PRESENT)) ==</blue>
		(_PAGE_PSE | _PAGE_PRESENT);
}

static inline int pud_bad(pud_t pud)
{
<blue>	return (pud_flags(pud) & ~(_KERNPG_TABLE | _PAGE_USER)) != 0;</blue>
}
#else
#define pud_leaf	pud_large
static inline int pud_large(pud_t pud)
{
	return 0;
}
#endif	/* CONFIG_PGTABLE_LEVELS &gt; 2 */

#if CONFIG_PGTABLE_LEVELS &gt; 3
static inline int p4d_none(p4d_t p4d)
{
<blue>	return (native_p4d_val(p4d) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;</blue>
}

static inline int p4d_present(p4d_t p4d)
{
<yellow>	return p4d_flags(p4d) & _PAGE_PRESENT;</yellow>
}

static inline pud_t *p4d_pgtable(p4d_t p4d)
{
<blue>	return (pud_t *)__va(p4d_val(p4d) & p4d_pfn_mask(p4d));</blue>
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h&#x27;s __section_mem_map_addr() definition:
 */
#define p4d_page(p4d)	pfn_to_page(p4d_pfn(p4d))

static inline int p4d_bad(p4d_t p4d)
{
<blue>	unsigned long ignore_flags = _KERNPG_TABLE | _PAGE_USER;</blue>

	if (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))
		ignore_flags |= _PAGE_NX;

	return (p4d_flags(p4d) &amp; ~ignore_flags) != 0;
}
#endif  /* CONFIG_PGTABLE_LEVELS &gt; 3 */

static inline unsigned long p4d_index(unsigned long address)
{
<yellow>	return (address >> P4D_SHIFT) & (PTRS_PER_P4D - 1);</yellow>
}

#if CONFIG_PGTABLE_LEVELS &gt; 4
static inline int pgd_present(pgd_t pgd)
<yellow>{</yellow>
<yellow>	if (!pgtable_l5_enabled())</yellow>
		return 1;
<yellow>	return pgd_flags(pgd) & _PAGE_PRESENT;</yellow>
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
<yellow>	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);</yellow>
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h&#x27;s __section_mem_map_addr() definition:
 */
#define pgd_page(pgd)	pfn_to_page(pgd_pfn(pgd))

/* to find an entry in a page-table-directory. */
<yellow>static inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)</yellow>
{
<blue>	if (!pgtable_l5_enabled())</blue>
		return (p4d_t *)pgd;
<yellow>	return (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);</yellow>
<blue>}</blue>

static inline int pgd_bad(pgd_t pgd)
{
	unsigned long ignore_flags = _PAGE_USER;

<blue>	if (!pgtable_l5_enabled())</blue>
		return 0;

	if (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))
		ignore_flags |= _PAGE_NX;

<blue>	return (pgd_flags(pgd) & ~ignore_flags) != _KERNPG_TABLE;</blue>
}

static inline int pgd_none(pgd_t pgd)
<blue>{</blue>
<blue>	if (!pgtable_l5_enabled())</blue>
		return 0;
	/*
	 * There is no need to do a workaround for the KNL stray
	 * A/D bit erratum here.  PGDs only point to page tables
	 * except on 32-bit non-PAE which is not supported on
	 * KNL.
	 */
<yellow>	return !native_pgd_val(pgd);</yellow>
}
#endif	/* CONFIG_PGTABLE_LEVELS &gt; 4 */

#endif	/* __ASSEMBLY__ */

#define KERNEL_PGD_BOUNDARY	pgd_index(PAGE_OFFSET)
#define KERNEL_PGD_PTRS		(PTRS_PER_PGD - KERNEL_PGD_BOUNDARY)

#ifndef __ASSEMBLY__

extern int direct_gbpages;
void init_mem_mapping(void);
void early_alloc_pgt_buf(void);
extern void memblock_find_dma_reserve(void);
void __init poking_init(void);
unsigned long init_memory_mapping(unsigned long start,
				  unsigned long end, pgprot_t prot);

#ifdef CONFIG_X86_64
extern pgd_t trampoline_pgd_entry;
#endif

/* local pte updates need not use xchg for locking */
static inline pte_t native_local_ptep_get_and_clear(pte_t *ptep)
{
<yellow>	pte_t res = *ptep;</yellow>

	/* Pure native function needs no input for mm, addr */
	native_pte_clear(NULL, 0, ptep);
	return res;
}

static inline pmd_t native_local_pmdp_get_and_clear(pmd_t *pmdp)
{
	pmd_t res = *pmdp;

	native_pmd_clear(pmdp);
	return res;
}

static inline pud_t native_local_pudp_get_and_clear(pud_t *pudp)
{
	pud_t res = *pudp;

	native_pud_clear(pudp);
	return res;
}

static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
			      pte_t *ptep, pte_t pte)
{
	page_table_check_pte_set(mm, addr, ptep, pte);
<yellow>	set_pte(ptep, pte);</yellow>
}

static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
			      pmd_t *pmdp, pmd_t pmd)
{
	page_table_check_pmd_set(mm, addr, pmdp, pmd);
<yellow>	set_pmd(pmdp, pmd);</yellow>
}

static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
			      pud_t *pudp, pud_t pud)
{
	page_table_check_pud_set(mm, addr, pudp, pud);
<yellow>	native_set_pud(pudp, pud);</yellow>
}

/*
 * We only update the dirty/accessed state if we set
 * the dirty bit by hand in the kernel, since the hardware
 * will do the accessed bit for us, and we don&#x27;t want to
 * race with other CPU&#x27;s that might be updating the dirty
 * bit at the same time.
 */
struct vm_area_struct;

#define  __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS
extern int ptep_set_access_flags(struct vm_area_struct *vma,
				 unsigned long address, pte_t *ptep,
				 pte_t entry, int dirty);

#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
extern int ptep_test_and_clear_young(struct vm_area_struct *vma,
				     unsigned long addr, pte_t *ptep);

#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH
extern int ptep_clear_flush_young(struct vm_area_struct *vma,
				  unsigned long address, pte_t *ptep);

#define __HAVE_ARCH_PTEP_GET_AND_CLEAR
static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
				       pte_t *ptep)
{
<blue>	pte_t pte = native_ptep_get_and_clear(ptep);</blue>
	page_table_check_pte_clear(mm, addr, pte);
	return pte;
}

#define __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL
static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
					    unsigned long addr, pte_t *ptep,
					    int full)
{
	pte_t pte;
	if (full) {
		/*
		 * Full address destruction in progress; paravirt does not
		 * care about updates and native needs no locking
		 */
<yellow>		pte = native_local_ptep_get_and_clear(ptep);</yellow>
		page_table_check_pte_clear(mm, addr, pte);
	} else {
<yellow>		pte = ptep_get_and_clear(mm, addr, ptep);</yellow>
	}
	return pte;
}

#define __HAVE_ARCH_PTEP_SET_WRPROTECT
static inline void ptep_set_wrprotect(struct mm_struct *mm,
				      unsigned long addr, pte_t *ptep)
{
<yellow>	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);</yellow>
}

#define flush_tlb_fix_spurious_fault(vma, address) do { } while (0)

#define mk_pmd(page, pgprot)   pfn_pmd(page_to_pfn(page), (pgprot))

#define  __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS
extern int pmdp_set_access_flags(struct vm_area_struct *vma,
				 unsigned long address, pmd_t *pmdp,
				 pmd_t entry, int dirty);
extern int pudp_set_access_flags(struct vm_area_struct *vma,
				 unsigned long address, pud_t *pudp,
				 pud_t entry, int dirty);

#define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG
extern int pmdp_test_and_clear_young(struct vm_area_struct *vma,
				     unsigned long addr, pmd_t *pmdp);
extern int pudp_test_and_clear_young(struct vm_area_struct *vma,
				     unsigned long addr, pud_t *pudp);

#define __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH
extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
				  unsigned long address, pmd_t *pmdp);


#define pmd_write pmd_write
static inline int pmd_write(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_RW;</yellow>
}

#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
				       pmd_t *pmdp)
{
<yellow>	pmd_t pmd = native_pmdp_get_and_clear(pmdp);</yellow>

	page_table_check_pmd_clear(mm, addr, pmd);

	return pmd;
}

#define __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR
static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
					unsigned long addr, pud_t *pudp)
{
<yellow>	pud_t pud = native_pudp_get_and_clear(pudp);</yellow>

	page_table_check_pud_clear(mm, addr, pud);

	return pud;
}

#define __HAVE_ARCH_PMDP_SET_WRPROTECT
static inline void pmdp_set_wrprotect(struct mm_struct *mm,
				      unsigned long addr, pmd_t *pmdp)
{
	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
}

#define pud_write pud_write
static inline int pud_write(pud_t pud)
{
<yellow>	return pud_flags(pud) & _PAGE_RW;</yellow>
}

#ifndef pmdp_establish
#define pmdp_establish pmdp_establish
static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
		unsigned long address, pmd_t *pmdp, pmd_t pmd)
{
	page_table_check_pmd_set(vma-&gt;vm_mm, address, pmdp, pmd);
	if (IS_ENABLED(CONFIG_SMP)) {
		return xchg(pmdp, pmd);
	} else {
		pmd_t old = *pmdp;
		WRITE_ONCE(*pmdp, pmd);
		return old;
	}
}
#endif

#define __HAVE_ARCH_PMDP_INVALIDATE_AD
extern pmd_t pmdp_invalidate_ad(struct vm_area_struct *vma,
				unsigned long address, pmd_t *pmdp);

/*
 * Page table pages are page-aligned.  The lower half of the top
 * level is used for userspace and the top half for the kernel.
 *
 * Returns true for parts of the PGD that map userspace and
 * false for the parts that map the kernel.
 */
static inline bool pgdp_maps_userspace(void *__ptr)
{
	unsigned long ptr = (unsigned long)__ptr;

	return (((ptr &amp; ~PAGE_MASK) / sizeof(pgd_t)) &lt; PGD_KERNEL_START);
}

#define pgd_leaf	pgd_large
static inline int pgd_large(pgd_t pgd) { return 0; }

#ifdef CONFIG_PAGE_TABLE_ISOLATION
/*
 * All top-level PAGE_TABLE_ISOLATION page tables are order-1 pages
 * (8k-aligned and 8k in size).  The kernel one is at the beginning 4k and
 * the user one is in the last 4k.  To switch between them, you
 * just need to flip the 12th bit in their addresses.
 */
#define PTI_PGTABLE_SWITCH_BIT	PAGE_SHIFT

/*
 * This generates better code than the inline assembly in
 * __set_bit().
 */
static inline void *ptr_set_bit(void *ptr, int bit)
{
	unsigned long __ptr = (unsigned long)ptr;

	__ptr |= BIT(bit);
<yellow>	return (void *)__ptr;</yellow>
}
static inline void *ptr_clear_bit(void *ptr, int bit)
{
	unsigned long __ptr = (unsigned long)ptr;

	__ptr &amp;= ~BIT(bit);
	return (void *)__ptr;
}

static inline pgd_t *kernel_to_user_pgdp(pgd_t *pgdp)
{
<yellow>	return ptr_set_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);</yellow>
}

static inline pgd_t *user_to_kernel_pgdp(pgd_t *pgdp)
{
	return ptr_clear_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);
}

static inline p4d_t *kernel_to_user_p4dp(p4d_t *p4dp)
{
	return ptr_set_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);
}

static inline p4d_t *user_to_kernel_p4dp(p4d_t *p4dp)
{
	return ptr_clear_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);
}
#endif /* CONFIG_PAGE_TABLE_ISOLATION */

/*
 * clone_pgd_range(pgd_t *dst, pgd_t *src, int count);
 *
 *  dst - pointer to pgd range anywhere on a pgd page
 *  src - &quot;&quot;
 *  count - the number of pgds to copy.
 *
 * dst and src can be on the same page, but the range must not overlap,
 * and must not cross a page boundary.
 */
static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
{
	memcpy(dst, src, count * sizeof(pgd_t));
#ifdef CONFIG_PAGE_TABLE_ISOLATION
	if (!static_cpu_has(X86_FEATURE_PTI))
		return;
	/* Clone the user space pgd as well */
<yellow>	memcpy(kernel_to_user_pgdp(dst), kernel_to_user_pgdp(src),</yellow>
	       count * sizeof(pgd_t));
#endif
}

#define PTE_SHIFT ilog2(PTRS_PER_PTE)
static inline int page_level_shift(enum pg_level level)
{
<yellow>	return (PAGE_SHIFT - PTE_SHIFT) + level * PTE_SHIFT;</yellow>
}
static inline unsigned long page_level_size(enum pg_level level)
{
<yellow>	return 1UL << page_level_shift(level);</yellow>
}
static inline unsigned long page_level_mask(enum pg_level level)
{
<yellow>	return ~(page_level_size(level) - 1);</yellow>
}

/*
 * The x86 doesn&#x27;t have any external MMU info: the kernel page
 * tables contain all the necessary information.
 */
static inline void update_mmu_cache(struct vm_area_struct *vma,
		unsigned long addr, pte_t *ptep)
{
}
static inline void update_mmu_cache_pmd(struct vm_area_struct *vma,
		unsigned long addr, pmd_t *pmd)
{
}
static inline void update_mmu_cache_pud(struct vm_area_struct *vma,
		unsigned long addr, pud_t *pud)
{
}
#ifdef _PAGE_SWP_EXCLUSIVE
#define __HAVE_ARCH_PTE_SWP_EXCLUSIVE
static inline pte_t pte_swp_mkexclusive(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_SWP_EXCLUSIVE);</yellow>
}

static inline int pte_swp_exclusive(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_SWP_EXCLUSIVE;</yellow>
}

static inline pte_t pte_swp_clear_exclusive(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_SWP_EXCLUSIVE);</yellow>
}
#endif /* _PAGE_SWP_EXCLUSIVE */

#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);</yellow>
}

static inline int pte_swp_soft_dirty(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;</yellow>
}

static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);</yellow>
}

#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
static inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_SWP_SOFT_DIRTY);</yellow>
}

static inline int pmd_swp_soft_dirty(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_SWP_SOFT_DIRTY;</yellow>
}

static inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)
{
<yellow>	return pmd_clear_flags(pmd, _PAGE_SWP_SOFT_DIRTY);</yellow>
}
#endif
#endif

#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
static inline pte_t pte_swp_mkuffd_wp(pte_t pte)
{
<yellow>	return pte_set_flags(pte, _PAGE_SWP_UFFD_WP);</yellow>
}

static inline int pte_swp_uffd_wp(pte_t pte)
{
<yellow>	return pte_flags(pte) & _PAGE_SWP_UFFD_WP;</yellow>
}

static inline pte_t pte_swp_clear_uffd_wp(pte_t pte)
{
<yellow>	return pte_clear_flags(pte, _PAGE_SWP_UFFD_WP);</yellow>
}

static inline pmd_t pmd_swp_mkuffd_wp(pmd_t pmd)
{
<yellow>	return pmd_set_flags(pmd, _PAGE_SWP_UFFD_WP);</yellow>
}

static inline int pmd_swp_uffd_wp(pmd_t pmd)
{
<yellow>	return pmd_flags(pmd) & _PAGE_SWP_UFFD_WP;</yellow>
}

static inline pmd_t pmd_swp_clear_uffd_wp(pmd_t pmd)
{
<yellow>	return pmd_clear_flags(pmd, _PAGE_SWP_UFFD_WP);</yellow>
}
#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */

static inline u16 pte_flags_pkey(unsigned long pte_flags)
{
#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
	/* ifdef to avoid doing 59-bit shift on 32-bit values */
<blue>	return (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;</blue>
#else
	return 0;
#endif
}

static inline bool __pkru_allows_pkey(u16 pkey, bool write)
{
<blue>	u32 pkru = read_pkru();</blue>

<yellow>	if (!__pkru_allows_read(pkru, pkey))</yellow>
		return false;
<blue>	if (write && !__pkru_allows_write(pkru, pkey))</blue>
		return false;

	return true;
}

/*
 * &#x27;pteval&#x27; can come from a PTE, PMD or PUD.  We only check
 * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the
 * same value on all 3 types.
 */
<blue>static inline bool __pte_access_permitted(unsigned long pteval, bool write)</blue>
{
	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;

	if (write)
		need_pte_bits |= _PAGE_RW;

	if ((pteval &amp; need_pte_bits) != need_pte_bits)
		return 0;

<blue>	return __pkru_allows_pkey(pte_flags_pkey(pteval), write);</blue>
}

#define pte_access_permitted pte_access_permitted
static inline bool pte_access_permitted(pte_t pte, bool write)
{
<blue>	return __pte_access_permitted(pte_val(pte), write);</blue>
}

#define pmd_access_permitted pmd_access_permitted
static inline bool pmd_access_permitted(pmd_t pmd, bool write)
{
<yellow>	return __pte_access_permitted(pmd_val(pmd), write);</yellow>
}

#define pud_access_permitted pud_access_permitted
static inline bool pud_access_permitted(pud_t pud, bool write)
{
<yellow>	return __pte_access_permitted(pud_val(pud), write);</yellow>
}

#define __HAVE_ARCH_PFN_MODIFY_ALLOWED 1
extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);

static inline bool arch_has_pfn_modify_check(void)
{
<yellow>	return boot_cpu_has_bug(X86_BUG_L1TF);</yellow>
}

#define arch_has_hw_pte_young arch_has_hw_pte_young
static inline bool arch_has_hw_pte_young(void)
{
	return true;
}

#ifdef CONFIG_XEN_PV
#define arch_has_hw_nonleaf_pmd_young arch_has_hw_nonleaf_pmd_young
static inline bool arch_has_hw_nonleaf_pmd_young(void)
{
	return !cpu_feature_enabled(X86_FEATURE_XENPV);
}
#endif

#ifdef CONFIG_PAGE_TABLE_CHECK
static inline bool pte_user_accessible_page(pte_t pte)
{
	return (pte_val(pte) &amp; _PAGE_PRESENT) &amp;&amp; (pte_val(pte) &amp; _PAGE_USER);
}

static inline bool pmd_user_accessible_page(pmd_t pmd)
{
	return pmd_leaf(pmd) &amp;&amp; (pmd_val(pmd) &amp; _PAGE_PRESENT) &amp;&amp; (pmd_val(pmd) &amp; _PAGE_USER);
}

static inline bool pud_user_accessible_page(pud_t pud)
{
	return pud_leaf(pud) &amp;&amp; (pud_val(pud) &amp; _PAGE_PRESENT) &amp;&amp; (pud_val(pud) &amp; _PAGE_USER);
}
#endif

#endif	/* __ASSEMBLY__ */

#endif /* _ASM_X86_PGTABLE_H */


</code></pre></td></tr></table>
</body>
</html>
