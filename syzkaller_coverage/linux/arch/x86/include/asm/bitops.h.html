<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.<br>2.<br>3.<br>4.<br>5.<br>6.<br>7.<br>8.<br>9.<br>10.<br>11.<br>12.<br>13.<br>14.<br>15.<br>16.<br>17.<br>18.<br>19.<br>20.<br>21.<br>22.<br>23.<br>24.<br>25.<br>26.<br>27.<br>28.<br>29.<br>30.<br>31.<br>32.<br>33.<br>34.<br>35.<br>36.<br>37.<br>38.<br>39.<br>40.<br>41.<br>42.<br>43.<br>44.<br>45.<br>46.<br>47.<br>48.<br>49.<br>50.<br>51.<br>52.<br>53.<br>54.<br>55.<br>56.<br>57.<br>58.<br>59.<br>60.<br>61.<br>62.<br>63.<br>64.<br>65.<br>66.<br>67.<br>68.<br>69.<br>70.<br>71.<br>72.<br>73.<br>74.<br>75.<br>76.<br>77.<br>78.<br>79.<br>80.<br>81.<br>82.<br>83.<br>84.<br>85.<br>86.<br>87.<br>88.<br>89.<br>90.<br>91.<br>92.<br>93.<br>94.<br>95.<br>96.<br>97.<br>98.<br>99.<br>100.<br>101.<br>102.<br>103.<br>104.<br>105.<br>106.<br>107.<br>108.<br>109.<br>110.<br>111.<br>112.<br>113.<br>114.<br>115.<br>116.<br>117.<br>118.<br>119.<br>120.<br>121.<br>122.<br>123.<br>124.<br>125.<br>126.<br>127.<br>128.<br>129.<br>130.<br>131.<br>132.<br>133.<br>134.<br>135.<br>136.<br>137.<br>138.<br>139.<br>140.<br>141.<br>142.<br>143.<br>144.<br>145.<br>146.<br>147.<br>148.<br>149.<br>150.<br>151.<br>152.<br>153.<br>154.<br>155.<br>156.<br>157.<br>158.<br>159.<br>160.<br>161.<br>162.<br>163.<br>164.<br>165.<br>166.<br>167.<br>168.<br>169.<br>170.<br>171.<br>172.<br>173.<br>174.<br>175.<br>176.<br>177.<br>178.<br>179.<br>180.<br>181.<br>182.<br>183.<br>184.<br>185.<br>186.<br>187.<br>188.<br>189.<br>190.<br>191.<br>192.<br>193.<br>194.<br>195.<br>196.<br>197.<br>198.<br>199.<br>200.<br>201.<br>202.<br>203.<br>204.<br>205.<br>206.<br>207.<br>208.<br>209.<br>210.<br>211.<br>212.<br>213.<br>214.<br>215.<br>216.<br>217.<br>218.<br>219.<br>220.<br>221.<br>222.<br>223.<br>224.<br>225.<br>226.<br>227.<br>228.<br>229.<br>230.<br>231.<br>232.<br>233.<br>234.<br>235.<br>236.<br>237.<br>238.<br>239.<br>240.<br>241.<br>242.<br>243.<br>244.<br>245.<br>246.<br>247.<br>248.<br>249.<br>250.<br>251.<br>252.<br>253.<br>254.<br>255.<br>256.<br>257.<br>258.<br>259.<br>260.<br>261.<br>262.<br>263.<br>264.<br>265.<br>266.<br>267.<br>268.<br>269.<br>270.<br>271.<br>272.<br>273.<br>274.<br>275.<br>276.<br>277.<br>278.<br>279.<br>280.<br>281.<br>282.<br>283.<br>284.<br>285.<br>286.<br>287.<br>288.<br>289.<br>290.<br>291.<br>292.<br>293.<br>294.<br>295.<br>296.<br>297.<br>298.<br>299.<br>300.<br>301.<br>302.<br>303.<br>304.<br>305.<br>306.<br>307.<br>308.<br>309.<br>310.<br>311.<br>312.<br>313.<br>314.<br>315.<br>316.<br>317.<br>318.<br>319.<br>320.<br>321.<br>322.<br>323.<br>324.<br>325.<br>326.<br>327.<br>328.<br>329.<br>330.<br>331.<br>332.<br>333.<br>334.<br>335.<br>336.<br>337.<br>338.<br>339.<br>340.<br>341.<br>342.<br>343.<br>344.<br>345.<br>346.<br>347.<br>348.<br>349.<br>350.<br>351.<br>352.<br>353.<br>354.<br>355.<br>356.<br>357.<br>358.<br>359.<br>360.<br>361.<br>362.<br>363.<br>364.<br>365.<br>366.<br>367.<br>368.<br>369.<br>370.<br>371.<br>372.<br>373.<br>374.<br>375.<br>376.<br>377.<br>378.<br>379.<br>380.<br>381.<br>382.<br>383.<br>384.<br>385.<br>386.<br>387.<br>388.<br>389.<br>390.<br>391.<br>392.<br>393.<br>394.<br>395.<br>396.<br>397.<br>398.<br>399.<br>400.<br>401.<br>402.<br>403.<br>404.<br>405.<br>406.<br>407.<br>408.<br>409.<br>410.<br>411.<br>412.<br>413.<br>414.<br>415.<br>416.<br>417.<br>418.<br>419.<br>420.<br>421.<br>422.<br>423.<br>424.<br>425.<br>426.<br>427.<br>428.<br>429.<br>430.<br>431.<br>432.<br>433.<br>434.<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">/* SPDX-License-Identifier: GPL-2.0 */
#ifndef _ASM_X86_BITOPS_H
#define _ASM_X86_BITOPS_H

/*
 * Copyright 1992, Linus Torvalds.
 *
 * Note: inlines with more than a single statement should be marked
 * __always_inline to avoid problems with older gcc&#x27;s inlining heuristics.
 */

#ifndef _LINUX_BITOPS_H
#error only &lt;linux/bitops.h&gt; can be included directly
#endif

#include &lt;linux/compiler.h&gt;
#include &lt;asm/alternative.h&gt;
#include &lt;asm/rmwcc.h&gt;
#include &lt;asm/barrier.h&gt;

#if BITS_PER_LONG == 32
# define _BITOPS_LONG_SHIFT 5
#elif BITS_PER_LONG == 64
# define _BITOPS_LONG_SHIFT 6
#else
# error &quot;Unexpected BITS_PER_LONG&quot;
#endif

#define BIT_64(n)			(U64_C(1) &lt;&lt; (n))

/*
 * These have to be done with inline assembly: that way the bit-setting
 * is guaranteed to be atomic. All bit operations return 0 if the bit
 * was cleared before the operation and != 0 if it was not.
 *
 * bit 0 is the LSB of addr; bit 32 is the LSB of (addr+1).
 */

#define RLONG_ADDR(x)			 &quot;m&quot; (*(volatile long *) (x))
#define WBYTE_ADDR(x)			&quot;+m&quot; (*(volatile char *) (x))

#define ADDR				RLONG_ADDR(addr)

/*
 * We do the locked ops that don&#x27;t return the old value as
 * a mask operation on a byte.
 */
#define CONST_MASK_ADDR(nr, addr)	WBYTE_ADDR((void *)(addr) + ((nr)&gt;&gt;3))
#define CONST_MASK(nr)			(1 &lt;&lt; ((nr) &amp; 7))

static __always_inline void
arch_set_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
<yellow>		asm volatile(LOCK_PREFIX "orb %b1,%0"</yellow>
<yellow>			: CONST_MASK_ADDR(nr, addr)</yellow>
			: &quot;iq&quot; (CONST_MASK(nr))
			: &quot;memory&quot;);
	} else {
<yellow>		asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"</yellow>
			: : RLONG_ADDR(addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);
	}
}

static __always_inline void
arch___set_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(__ASM_SIZE(bts) &quot; %1,%0&quot; : : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
}

static __always_inline void
arch_clear_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
<yellow>		asm volatile(LOCK_PREFIX "andb %b1,%0"</yellow>
<yellow>			: CONST_MASK_ADDR(nr, addr)</yellow>
			: &quot;iq&quot; (~CONST_MASK(nr)));
	} else {
<blue>		asm volatile(LOCK_PREFIX __ASM_SIZE(btr) " %1,%0"</blue>
			: : RLONG_ADDR(addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);
	}
}

static __always_inline void
arch_clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	barrier();
<yellow>	arch_clear_bit(nr, addr);</yellow>
}

static __always_inline void
arch___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(__ASM_SIZE(btr) &quot; %1,%0&quot; : : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
}

static __always_inline bool
arch_clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
{
	bool negative;
	asm volatile(LOCK_PREFIX &quot;andb %2,%1&quot;
		CC_SET(s)
		: CC_OUT(s) (negative), WBYTE_ADDR(addr)
		: &quot;ir&quot; ((char) ~(1 &lt;&lt; nr)) : &quot;memory&quot;);
	return negative;
}
#define arch_clear_bit_unlock_is_negative_byte                                 \
	arch_clear_bit_unlock_is_negative_byte

static __always_inline void
arch___clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	arch___clear_bit(nr, addr);
}

static __always_inline void
arch___change_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(__ASM_SIZE(btc) &quot; %1,%0&quot; : : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
}

static __always_inline void
arch_change_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
		asm volatile(LOCK_PREFIX &quot;xorb %b1,%0&quot;
			: CONST_MASK_ADDR(nr, addr)
			: &quot;iq&quot; (CONST_MASK(nr)));
	} else {
		asm volatile(LOCK_PREFIX __ASM_SIZE(btc) &quot; %1,%0&quot;
			: : RLONG_ADDR(addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);
	}
}

static __always_inline bool
arch_test_and_set_bit(long nr, volatile unsigned long *addr)
{
<yellow>	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(bts), *addr, c, "Ir", nr);</yellow>
}

static __always_inline bool
arch_test_and_set_bit_lock(long nr, volatile unsigned long *addr)
{
	return arch_test_and_set_bit(nr, addr);
}

static __always_inline bool
arch___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm(__ASM_SIZE(bts) &quot; %2,%1&quot;
	    CC_SET(c)
	    : CC_OUT(c) (oldbit)
	    : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
	return oldbit;
}

static __always_inline bool
arch_test_and_clear_bit(long nr, volatile unsigned long *addr)
{
	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btr), *addr, c, &quot;Ir&quot;, nr);
}

/*
 * Note: the operation is performed atomically with respect to
 * the local CPU, but not other CPUs. Portable code should not
 * rely on this behaviour.
 * KVM relies on this behaviour on x86 for modifying memory that is also
 * accessed from a hypervisor on the same CPU if running in a VM: don&#x27;t change
 * this without also updating arch/x86/kernel/kvm.c
 */
static __always_inline bool
arch___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(__ASM_SIZE(btr) &quot; %2,%1&quot;
		     CC_SET(c)
		     : CC_OUT(c) (oldbit)
		     : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);
	return oldbit;
}

static __always_inline bool
arch___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(__ASM_SIZE(btc) &quot; %2,%1&quot;
		     CC_SET(c)
		     : CC_OUT(c) (oldbit)
		     : ADDR, &quot;Ir&quot; (nr) : &quot;memory&quot;);

	return oldbit;
}

static __always_inline bool
arch_test_and_change_bit(long nr, volatile unsigned long *addr)
{
	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc), *addr, c, &quot;Ir&quot;, nr);
}

static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL &lt;&lt; (nr &amp; (BITS_PER_LONG-1))) &amp;
<blue>		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;</blue>
}

static __always_inline bool constant_test_bit_acquire(long nr, const volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(&quot;testb %2,%1&quot;
		     CC_SET(nz)
		     : CC_OUT(nz) (oldbit)
		     : &quot;m&quot; (((unsigned char *)addr)[nr &gt;&gt; 3]),
		       &quot;i&quot; (1 &lt;&lt; (nr &amp; 7))
		     :&quot;memory&quot;);

	return oldbit;
}

static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
{
	bool oldbit;

<blue>	asm volatile(__ASM_SIZE(bt) " %2,%1"</blue>
		     CC_SET(c)
		     : CC_OUT(c) (oldbit)
		     : &quot;m&quot; (*(unsigned long *)addr), &quot;Ir&quot; (nr) : &quot;memory&quot;);

	return oldbit;
}

static __always_inline bool
arch_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
<blue>	return __builtin_constant_p(nr) ? constant_test_bit(nr, addr) :</blue>
<blue>					  variable_test_bit(nr, addr);</blue>
}

static __always_inline bool
arch_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
	return __builtin_constant_p(nr) ? constant_test_bit_acquire(nr, addr) :
					  variable_test_bit(nr, addr);
}

static __always_inline unsigned long variable__ffs(unsigned long word)
{
<blue>	asm("rep; bsf %1,%0"</blue>
		: &quot;=r&quot; (word)
		: &quot;rm&quot; (word));
	return word;
}

/**
 * __ffs - find first set bit in word
 * @word: The word to search
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
#define __ffs(word)				\
	(__builtin_constant_p(word) ?		\
	 (unsigned long)__builtin_ctzl(word) :	\
	 variable__ffs(word))

static __always_inline unsigned long variable_ffz(unsigned long word)
{
<yellow>	asm("rep; bsf %1,%0"</yellow>
		: &quot;=r&quot; (word)
<blue>		: "r" (~word));</blue>
	return word;
}

/**
 * ffz - find first zero bit in word
 * @word: The word to search
 *
 * Undefined if no zero exists, so code should check against ~0UL first.
 */
#define ffz(word)				\
	(__builtin_constant_p(word) ?		\
	 (unsigned long)__builtin_ctzl(~word) :	\
	 variable_ffz(word))

/*
 * __fls: find last set bit in word
 * @word: The word to search
 *
 * Undefined if no set bit exists, so code should check against 0 first.
 */
static __always_inline unsigned long __fls(unsigned long word)
{
<yellow>	asm("bsr %1,%0"</yellow>
	    : &quot;=r&quot; (word)
	    : &quot;rm&quot; (word));
<yellow>	return word;</yellow>
}

#undef ADDR

#ifdef __KERNEL__
static __always_inline int variable_ffs(int x)
{
	int r;

#ifdef CONFIG_X86_64
	/*
	 * AMD64 says BSFL won&#x27;t clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
<yellow>	asm("bsfl %1,%0"</yellow>
	    : &quot;=r&quot; (r)
	    : &quot;rm&quot; (x), &quot;0&quot; (-1));
#elif defined(CONFIG_X86_CMOV)
	asm(&quot;bsfl %1,%0\n\t&quot;
	    &quot;cmovzl %2,%0&quot;
	    : &quot;=&amp;r&quot; (r) : &quot;rm&quot; (x), &quot;r&quot; (-1));
#else
	asm(&quot;bsfl %1,%0\n\t&quot;
	    &quot;jnz 1f\n\t&quot;
	    &quot;movl $-1,%0\n&quot;
	    &quot;1:&quot; : &quot;=r&quot; (r) : &quot;rm&quot; (x));
#endif
	return r + 1;
}

/**
 * ffs - find first set bit in word
 * @x: the word to search
 *
 * This is defined the same way as the libc and compiler builtin ffs
 * routines, therefore differs in spirit from the other bitops.
 *
 * ffs(value) returns 0 if value is 0 or the position of the first
 * set bit if value is nonzero. The first (least significant) bit
 * is at position 1.
 */
#define ffs(x) (__builtin_constant_p(x) ? __builtin_ffs(x) : variable_ffs(x))

/**
 * fls - find last set bit in word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffs, but returns the position of the most significant set bit.
 *
 * fls(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 32.
 */
static __always_inline int fls(unsigned int x)
{
	int r;

#ifdef CONFIG_X86_64
	/*
	 * AMD64 says BSRL won&#x27;t clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
<blue>	asm("bsrl %1,%0"</blue>
	    : &quot;=r&quot; (r)
	    : &quot;rm&quot; (x), &quot;0&quot; (-1));
#elif defined(CONFIG_X86_CMOV)
	asm(&quot;bsrl %1,%0\n\t&quot;
	    &quot;cmovzl %2,%0&quot;
	    : &quot;=&amp;r&quot; (r) : &quot;rm&quot; (x), &quot;rm&quot; (-1));
#else
	asm(&quot;bsrl %1,%0\n\t&quot;
	    &quot;jnz 1f\n\t&quot;
	    &quot;movl $-1,%0\n&quot;
	    &quot;1:&quot; : &quot;=r&quot; (r) : &quot;rm&quot; (x));
#endif
<yellow>	return r + 1;</yellow>
}

/**
 * fls64 - find last set bit in a 64-bit word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffsll, but returns the position of the most significant set bit.
 *
 * fls64(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 64.
 */
#ifdef CONFIG_X86_64
static __always_inline int fls64(__u64 x)
{
	int bitpos = -1;
	/*
	 * AMD64 says BSRQ won&#x27;t clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before.
	 */
<yellow>	asm("bsrq %1,%q0"</yellow>
	    : &quot;+r&quot; (bitpos)
	    : &quot;rm&quot; (x));
<yellow>	return bitpos + 1;</yellow>
}
#else
#include &lt;asm-generic/bitops/fls64.h&gt;
#endif

#include &lt;asm-generic/bitops/sched.h&gt;

#include &lt;asm/arch_hweight.h&gt;

#include &lt;asm-generic/bitops/const_hweight.h&gt;

#include &lt;asm-generic/bitops/instrumented-atomic.h&gt;
#include &lt;asm-generic/bitops/instrumented-non-atomic.h&gt;
#include &lt;asm-generic/bitops/instrumented-lock.h&gt;

#include &lt;asm-generic/bitops/le.h&gt;

#include &lt;asm-generic/bitops/ext2-atomic-setbit.h&gt;

#endif /* __KERNEL__ */
#endif /* _ASM_X86_BITOPS_H */


</code></pre></td></tr></table>
</body>
</html>
