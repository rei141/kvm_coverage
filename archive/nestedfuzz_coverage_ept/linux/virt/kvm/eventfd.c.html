<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.<br>2.<br>3.<br>4.<br>5.<br>6.<br>7.<br>8.<br>9.<br>10.<br>11.<br>12.<br>13.<br>14.<br>15.<br>16.<br>17.<br>18.<br>19.<br>20.<br>21.<br>22.<br>23.<br>24.<br>25.<br>26.<br>27.<br>28.<br>29.<br>30.<br>31.<br>32.<br>33.<br>34.<br>35.<br>36.<br>37.<br>38.<br>39.<br>40.<br>41.<br>42.<br>43.<br>44.<br>45.<br>46.<br>47.<br>48.<br>49.<br>50.<br>51.<br>52.<br>53.<br>54.<br>55.<br>56.<br>57.<br>58.<br>59.<br>60.<br>61.<br>62.<br>63.<br>64.<br>65.<br>66.<br>67.<br>68.<br>69.<br>70.<br>71.<br>72.<br>73.<br>74.<br>75.<br>76.<br>77.<br>78.<br>79.<br>80.<br>81.<br>82.<br>83.<br>84.<br>85.<br>86.<br>87.<br>88.<br>89.<br>90.<br>91.<br>92.<br>93.<br>94.<br>95.<br>96.<br>97.<br>98.<br>99.<br>100.<br>101.<br>102.<br>103.<br>104.<br>105.<br>106.<br>107.<br>108.<br>109.<br>110.<br>111.<br>112.<br>113.<br>114.<br>115.<br>116.<br>117.<br>118.<br>119.<br>120.<br>121.<br>122.<br>123.<br>124.<br>125.<br>126.<br>127.<br>128.<br>129.<br>130.<br>131.<br>132.<br>133.<br>134.<br>135.<br>136.<br>137.<br>138.<br>139.<br>140.<br>141.<br>142.<br>143.<br>144.<br>145.<br>146.<br>147.<br>148.<br>149.<br>150.<br>151.<br>152.<br>153.<br>154.<br>155.<br>156.<br>157.<br>158.<br>159.<br>160.<br>161.<br>162.<br>163.<br>164.<br>165.<br>166.<br>167.<br>168.<br>169.<br>170.<br>171.<br>172.<br>173.<br>174.<br>175.<br>176.<br>177.<br>178.<br>179.<br>180.<br>181.<br>182.<br>183.<br>184.<br>185.<br>186.<br>187.<br>188.<br>189.<br>190.<br>191.<br>192.<br>193.<br>194.<br>195.<br>196.<br>197.<br>198.<br>199.<br>200.<br>201.<br>202.<br>203.<br>204.<br>205.<br>206.<br>207.<br>208.<br>209.<br>210.<br>211.<br>212.<br>213.<br>214.<br>215.<br>216.<br>217.<br>218.<br>219.<br>220.<br>221.<br>222.<br>223.<br>224.<br>225.<br>226.<br>227.<br>228.<br>229.<br>230.<br>231.<br>232.<br>233.<br>234.<br>235.<br>236.<br>237.<br>238.<br>239.<br>240.<br>241.<br>242.<br>243.<br>244.<br>245.<br>246.<br>247.<br>248.<br>249.<br>250.<br>251.<br>252.<br>253.<br>254.<br>255.<br>256.<br>257.<br>258.<br>259.<br>260.<br>261.<br>262.<br>263.<br>264.<br>265.<br>266.<br>267.<br>268.<br>269.<br>270.<br>271.<br>272.<br>273.<br>274.<br>275.<br>276.<br>277.<br>278.<br>279.<br>280.<br>281.<br>282.<br>283.<br>284.<br>285.<br>286.<br>287.<br>288.<br>289.<br>290.<br>291.<br>292.<br>293.<br>294.<br>295.<br>296.<br>297.<br>298.<br>299.<br>300.<br>301.<br>302.<br>303.<br>304.<br>305.<br>306.<br>307.<br>308.<br>309.<br>310.<br>311.<br>312.<br>313.<br>314.<br>315.<br>316.<br>317.<br>318.<br>319.<br>320.<br>321.<br>322.<br>323.<br>324.<br>325.<br>326.<br>327.<br>328.<br>329.<br>330.<br>331.<br>332.<br>333.<br>334.<br>335.<br>336.<br>337.<br>338.<br>339.<br>340.<br>341.<br>342.<br>343.<br>344.<br>345.<br>346.<br>347.<br>348.<br>349.<br>350.<br>351.<br>352.<br>353.<br>354.<br>355.<br>356.<br>357.<br>358.<br>359.<br>360.<br>361.<br>362.<br>363.<br>364.<br>365.<br>366.<br>367.<br>368.<br>369.<br>370.<br>371.<br>372.<br>373.<br>374.<br>375.<br>376.<br>377.<br>378.<br>379.<br>380.<br>381.<br>382.<br>383.<br>384.<br>385.<br>386.<br>387.<br>388.<br>389.<br>390.<br>391.<br>392.<br>393.<br>394.<br>395.<br>396.<br>397.<br>398.<br>399.<br>400.<br>401.<br>402.<br>403.<br>404.<br>405.<br>406.<br>407.<br>408.<br>409.<br>410.<br>411.<br>412.<br>413.<br>414.<br>415.<br>416.<br>417.<br>418.<br>419.<br>420.<br>421.<br>422.<br>423.<br>424.<br>425.<br>426.<br>427.<br>428.<br>429.<br>430.<br>431.<br>432.<br>433.<br>434.<br>435.<br>436.<br>437.<br>438.<br>439.<br>440.<br>441.<br>442.<br>443.<br>444.<br>445.<br>446.<br>447.<br>448.<br>449.<br>450.<br>451.<br>452.<br>453.<br>454.<br>455.<br>456.<br>457.<br>458.<br>459.<br>460.<br>461.<br>462.<br>463.<br>464.<br>465.<br>466.<br>467.<br>468.<br>469.<br>470.<br>471.<br>472.<br>473.<br>474.<br>475.<br>476.<br>477.<br>478.<br>479.<br>480.<br>481.<br>482.<br>483.<br>484.<br>485.<br>486.<br>487.<br>488.<br>489.<br>490.<br>491.<br>492.<br>493.<br>494.<br>495.<br>496.<br>497.<br>498.<br>499.<br>500.<br>501.<br>502.<br>503.<br>504.<br>505.<br>506.<br>507.<br>508.<br>509.<br>510.<br>511.<br>512.<br>513.<br>514.<br>515.<br>516.<br>517.<br>518.<br>519.<br>520.<br>521.<br>522.<br>523.<br>524.<br>525.<br>526.<br>527.<br>528.<br>529.<br>530.<br>531.<br>532.<br>533.<br>534.<br>535.<br>536.<br>537.<br>538.<br>539.<br>540.<br>541.<br>542.<br>543.<br>544.<br>545.<br>546.<br>547.<br>548.<br>549.<br>550.<br>551.<br>552.<br>553.<br>554.<br>555.<br>556.<br>557.<br>558.<br>559.<br>560.<br>561.<br>562.<br>563.<br>564.<br>565.<br>566.<br>567.<br>568.<br>569.<br>570.<br>571.<br>572.<br>573.<br>574.<br>575.<br>576.<br>577.<br>578.<br>579.<br>580.<br>581.<br>582.<br>583.<br>584.<br>585.<br>586.<br>587.<br>588.<br>589.<br>590.<br>591.<br>592.<br>593.<br>594.<br>595.<br>596.<br>597.<br>598.<br>599.<br>600.<br>601.<br>602.<br>603.<br>604.<br>605.<br>606.<br>607.<br>608.<br>609.<br>610.<br>611.<br>612.<br>613.<br>614.<br>615.<br>616.<br>617.<br>618.<br>619.<br>620.<br>621.<br>622.<br>623.<br>624.<br>625.<br>626.<br>627.<br>628.<br>629.<br>630.<br>631.<br>632.<br>633.<br>634.<br>635.<br>636.<br>637.<br>638.<br>639.<br>640.<br>641.<br>642.<br>643.<br>644.<br>645.<br>646.<br>647.<br>648.<br>649.<br>650.<br>651.<br>652.<br>653.<br>654.<br>655.<br>656.<br>657.<br>658.<br>659.<br>660.<br>661.<br>662.<br>663.<br>664.<br>665.<br>666.<br>667.<br>668.<br>669.<br>670.<br>671.<br>672.<br>673.<br>674.<br>675.<br>676.<br>677.<br>678.<br>679.<br>680.<br>681.<br>682.<br>683.<br>684.<br>685.<br>686.<br>687.<br>688.<br>689.<br>690.<br>691.<br>692.<br>693.<br>694.<br>695.<br>696.<br>697.<br>698.<br>699.<br>700.<br>701.<br>702.<br>703.<br>704.<br>705.<br>706.<br>707.<br>708.<br>709.<br>710.<br>711.<br>712.<br>713.<br>714.<br>715.<br>716.<br>717.<br>718.<br>719.<br>720.<br>721.<br>722.<br>723.<br>724.<br>725.<br>726.<br>727.<br>728.<br>729.<br>730.<br>731.<br>732.<br>733.<br>734.<br>735.<br>736.<br>737.<br>738.<br>739.<br>740.<br>741.<br>742.<br>743.<br>744.<br>745.<br>746.<br>747.<br>748.<br>749.<br>750.<br>751.<br>752.<br>753.<br>754.<br>755.<br>756.<br>757.<br>758.<br>759.<br>760.<br>761.<br>762.<br>763.<br>764.<br>765.<br>766.<br>767.<br>768.<br>769.<br>770.<br>771.<br>772.<br>773.<br>774.<br>775.<br>776.<br>777.<br>778.<br>779.<br>780.<br>781.<br>782.<br>783.<br>784.<br>785.<br>786.<br>787.<br>788.<br>789.<br>790.<br>791.<br>792.<br>793.<br>794.<br>795.<br>796.<br>797.<br>798.<br>799.<br>800.<br>801.<br>802.<br>803.<br>804.<br>805.<br>806.<br>807.<br>808.<br>809.<br>810.<br>811.<br>812.<br>813.<br>814.<br>815.<br>816.<br>817.<br>818.<br>819.<br>820.<br>821.<br>822.<br>823.<br>824.<br>825.<br>826.<br>827.<br>828.<br>829.<br>830.<br>831.<br>832.<br>833.<br>834.<br>835.<br>836.<br>837.<br>838.<br>839.<br>840.<br>841.<br>842.<br>843.<br>844.<br>845.<br>846.<br>847.<br>848.<br>849.<br>850.<br>851.<br>852.<br>853.<br>854.<br>855.<br>856.<br>857.<br>858.<br>859.<br>860.<br>861.<br>862.<br>863.<br>864.<br>865.<br>866.<br>867.<br>868.<br>869.<br>870.<br>871.<br>872.<br>873.<br>874.<br>875.<br>876.<br>877.<br>878.<br>879.<br>880.<br>881.<br>882.<br>883.<br>884.<br>885.<br>886.<br>887.<br>888.<br>889.<br>890.<br>891.<br>892.<br>893.<br>894.<br>895.<br>896.<br>897.<br>898.<br>899.<br>900.<br>901.<br>902.<br>903.<br>904.<br>905.<br>906.<br>907.<br>908.<br>909.<br>910.<br>911.<br>912.<br>913.<br>914.<br>915.<br>916.<br>917.<br>918.<br>919.<br>920.<br>921.<br>922.<br>923.<br>924.<br>925.<br>926.<br>927.<br>928.<br>929.<br>930.<br>931.<br>932.<br>933.<br>934.<br>935.<br>936.<br>937.<br>938.<br>939.<br>940.<br>941.<br>942.<br>943.<br>944.<br>945.<br>946.<br>947.<br>948.<br>949.<br>950.<br>951.<br>952.<br>953.<br>954.<br>955.<br>956.<br>957.<br>958.<br>959.<br>960.<br>961.<br>962.<br>963.<br>964.<br>965.<br>966.<br>967.<br>968.<br>969.<br>970.<br>971.<br>972.<br>973.<br>974.<br>975.<br>976.<br>977.<br>978.<br>979.<br>980.<br>981.<br>982.<br>983.<br>984.<br></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * kvm eventfd support - use eventfd objects to signal various KVM events
 *
 * Copyright 2009 Novell.  All Rights Reserved.
 * Copyright 2010 Red Hat, Inc. and/or its affiliates.
 *
 * Author:
 *	Gregory Haskins &lt;ghaskins@novell.com&gt;
 */

#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/kvm.h&gt;
#include &lt;linux/kvm_irqfd.h&gt;
#include &lt;linux/workqueue.h&gt;
#include &lt;linux/syscalls.h&gt;
#include &lt;linux/wait.h&gt;
#include &lt;linux/poll.h&gt;
#include &lt;linux/file.h&gt;
#include &lt;linux/list.h&gt;
#include &lt;linux/eventfd.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/srcu.h&gt;
#include &lt;linux/slab.h&gt;
#include &lt;linux/seqlock.h&gt;
#include &lt;linux/irqbypass.h&gt;
#include &lt;trace/events/kvm.h&gt;

#include &lt;kvm/iodev.h&gt;

#ifdef CONFIG_HAVE_KVM_IRQFD

static struct workqueue_struct *irqfd_cleanup_wq;

bool __attribute__((weak))
kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
{
	return true;
<yellow>}</yellow>

static void
irqfd_inject(struct work_struct *work)
{
	struct kvm_kernel_irqfd *irqfd =
		container_of(work, struct kvm_kernel_irqfd, inject);
<yellow>	struct kvm *kvm = irqfd->kvm;</yellow>

	if (!irqfd-&gt;resampler) {
<yellow>		kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,</yellow>
				false);
		kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd-&gt;gsi, 0,
				false);
	} else
<yellow>		kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,</yellow>
			    irqfd-&gt;gsi, 1, false);
<yellow>}</yellow>

/*
 * Since resampler irqfds share an IRQ source ID, we de-assert once
 * then notify all of the resampler irqfds using this GSI.  We can&#x27;t
 * do multiple de-asserts or we risk racing with incoming re-asserts.
 */
static void
irqfd_resampler_ack(struct kvm_irq_ack_notifier *kian)
{
	struct kvm_kernel_irqfd_resampler *resampler;
	struct kvm *kvm;
	struct kvm_kernel_irqfd *irqfd;
	int idx;

	resampler = container_of(kian,
			struct kvm_kernel_irqfd_resampler, notifier);
<yellow>	kvm = resampler->kvm;</yellow>

	kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
		    resampler-&gt;notifier.gsi, 0, false);

	idx = srcu_read_lock(&amp;kvm-&gt;irq_srcu);

	list_for_each_entry_srcu(irqfd, &amp;resampler-&gt;list, resampler_link,
	    srcu_read_lock_held(&amp;kvm-&gt;irq_srcu))
<yellow>		eventfd_signal(irqfd->resamplefd, 1);</yellow>

<yellow>	srcu_read_unlock(&kvm->irq_srcu, idx);</yellow>
}

static void
irqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)
{
<yellow>	struct kvm_kernel_irqfd_resampler *resampler = irqfd->resampler;</yellow>
	struct kvm *kvm = resampler-&gt;kvm;

	mutex_lock(&amp;kvm-&gt;irqfds.resampler_lock);

	list_del_rcu(&amp;irqfd-&gt;resampler_link);
	synchronize_srcu(&amp;kvm-&gt;irq_srcu);

	if (list_empty(&amp;resampler-&gt;list)) {
<yellow>		list_del(&resampler->link);</yellow>
<yellow>		kvm_unregister_irq_ack_notifier(kvm, &resampler->notifier);</yellow>
		kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
			    resampler-&gt;notifier.gsi, 0, false);
		kfree(resampler);
	}

<yellow>	mutex_unlock(&kvm->irqfds.resampler_lock);</yellow>
}

/*
 * Race-free decouple logic (ordering is critical)
 */
static void
irqfd_shutdown(struct work_struct *work)
{
	struct kvm_kernel_irqfd *irqfd =
		container_of(work, struct kvm_kernel_irqfd, shutdown);
<yellow>	struct kvm *kvm = irqfd->kvm;</yellow>
	u64 cnt;

	/* Make sure irqfd has been initialized in assign path. */
	synchronize_srcu(&amp;kvm-&gt;irq_srcu);

	/*
	 * Synchronize with the wait-queue and unhook ourselves to prevent
	 * further events.
	 */
	eventfd_ctx_remove_wait_queue(irqfd-&gt;eventfd, &amp;irqfd-&gt;wait, &amp;cnt);

	/*
	 * We know no new events will be scheduled at this point, so block
	 * until all previously outstanding events have completed
	 */
	flush_work(&amp;irqfd-&gt;inject);

	if (irqfd-&gt;resampler) {
<yellow>		irqfd_resampler_shutdown(irqfd);</yellow>
		eventfd_ctx_put(irqfd-&gt;resamplefd);
	}

	/*
	 * It is now safe to release the object&#x27;s resources
	 */
#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
<yellow>	irq_bypass_unregister_consumer(&irqfd->consumer);</yellow>
#endif
	eventfd_ctx_put(irqfd-&gt;eventfd);
	kfree(irqfd);
}


/* assumes kvm-&gt;irqfds.lock is held */
static bool
irqfd_is_active(struct kvm_kernel_irqfd *irqfd)
{
<yellow>	return list_empty(&irqfd->list) ? false : true;</yellow>
}

/*
 * Mark the irqfd as inactive and schedule it for removal
 *
 * assumes kvm-&gt;irqfds.lock is held
 */
static void
irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
{
<yellow>	BUG_ON(!irqfd_is_active(irqfd));</yellow>

<yellow>	list_del_init(&irqfd->list);</yellow>

	queue_work(irqfd_cleanup_wq, &amp;irqfd-&gt;shutdown);
}

int __attribute__((weak)) kvm_arch_set_irq_inatomic(
				struct kvm_kernel_irq_routing_entry *irq,
				struct kvm *kvm, int irq_source_id,
				int level,
				bool line_status)
{
	return -EWOULDBLOCK;
<yellow>}</yellow>

/*
 * Called with wqh-&gt;lock held and interrupts disabled
 */
static int
irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
<yellow>{</yellow>
	struct kvm_kernel_irqfd *irqfd =
		container_of(wait, struct kvm_kernel_irqfd, wait);
	__poll_t flags = key_to_poll(key);
	struct kvm_kernel_irq_routing_entry irq;
<yellow>	struct kvm *kvm = irqfd->kvm;</yellow>
	unsigned seq;
	int idx;
	int ret = 0;

	if (flags &amp; EPOLLIN) {
		u64 cnt;
<yellow>		eventfd_ctx_do_read(irqfd->eventfd, &cnt);</yellow>

		idx = srcu_read_lock(&amp;kvm-&gt;irq_srcu);
		do {
<yellow>			seq = read_seqcount_begin(&irqfd->irq_entry_sc);</yellow>
<yellow>			irq = irqfd->irq_entry;</yellow>
		} while (read_seqcount_retry(&amp;irqfd-&gt;irq_entry_sc, seq));
		/* An event has been signaled, inject an interrupt */
<yellow>		if (kvm_arch_set_irq_inatomic(&irq, kvm,</yellow>
					      KVM_USERSPACE_IRQ_SOURCE_ID, 1,
					      false) == -EWOULDBLOCK)
<yellow>			schedule_work(&irqfd->inject);</yellow>
<yellow>		srcu_read_unlock(&kvm->irq_srcu, idx);</yellow>
		ret = 1;
	}

<yellow>	if (flags & EPOLLHUP) {</yellow>
		/* The eventfd is closing, detach from KVM */
		unsigned long iflags;

<yellow>		spin_lock_irqsave(&kvm->irqfds.lock, iflags);</yellow>

		/*
		 * We must check if someone deactivated the irqfd before
		 * we could acquire the irqfds.lock since the item is
		 * deactivated from the KVM side before it is unhooked from
		 * the wait-queue.  If it is already deactivated, we can
		 * simply return knowing the other side will cleanup for us.
		 * We cannot race against the irqfd going away since the
		 * other side is required to acquire wqh-&gt;lock, which we hold
		 */
		if (irqfd_is_active(irqfd))
<yellow>			irqfd_deactivate(irqfd);</yellow>

<yellow>		spin_unlock_irqrestore(&kvm->irqfds.lock, iflags);</yellow>
	}

	return ret;
}

static void
irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,
			poll_table *pt)
{
	struct kvm_kernel_irqfd *irqfd =
		container_of(pt, struct kvm_kernel_irqfd, pt);
<yellow>	add_wait_queue_priority(wqh, &irqfd->wait);</yellow>
}

/* Must be called under irqfds.lock */
static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
{
	struct kvm_kernel_irq_routing_entry *e;
	struct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];
	int n_entries;

<yellow>	n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);</yellow>

	write_seqcount_begin(&amp;irqfd-&gt;irq_entry_sc);

	e = entries;
	if (n_entries == 1)
<yellow>		irqfd->irq_entry = *e;</yellow>
	else
<yellow>		irqfd->irq_entry.type = 0;</yellow>

<yellow>	write_seqcount_end(&irqfd->irq_entry_sc);</yellow>
}

#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
void __attribute__((weak)) kvm_arch_irq_bypass_stop(
				struct irq_bypass_consumer *cons)
{
<yellow>}</yellow>

void __attribute__((weak)) kvm_arch_irq_bypass_start(
				struct irq_bypass_consumer *cons)
{
}

int  __attribute__((weak)) kvm_arch_update_irqfd_routing(
				struct kvm *kvm, unsigned int host_irq,
				uint32_t guest_irq, bool set)
{
	return 0;
<yellow>}</yellow>

bool __attribute__((weak)) kvm_arch_irqfd_route_changed(
				struct kvm_kernel_irq_routing_entry *old,
				struct kvm_kernel_irq_routing_entry *new)
{
	return true;
}
#endif

static int
kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
{
	struct kvm_kernel_irqfd *irqfd, *tmp;
	struct fd f;
	struct eventfd_ctx *eventfd = NULL, *resamplefd = NULL;
	int ret;
	__poll_t events;
	int idx;

	if (!kvm_arch_intc_initialized(kvm))
		return -EAGAIN;

<yellow>	if (!kvm_arch_irqfd_allowed(kvm, args))</yellow>
		return -EINVAL;

<yellow>	irqfd = kzalloc(sizeof(*irqfd), GFP_KERNEL_ACCOUNT);</yellow>
	if (!irqfd)
		return -ENOMEM;

<yellow>	irqfd->kvm = kvm;</yellow>
	irqfd-&gt;gsi = args-&gt;gsi;
	INIT_LIST_HEAD(&amp;irqfd-&gt;list);
	INIT_WORK(&amp;irqfd-&gt;inject, irqfd_inject);
	INIT_WORK(&amp;irqfd-&gt;shutdown, irqfd_shutdown);
	seqcount_spinlock_init(&amp;irqfd-&gt;irq_entry_sc, &amp;kvm-&gt;irqfds.lock);

	f = fdget(args-&gt;fd);
	if (!f.file) {
		ret = -EBADF;
		goto out;
	}

<yellow>	eventfd = eventfd_ctx_fileget(f.file);</yellow>
	if (IS_ERR(eventfd)) {
		ret = PTR_ERR(eventfd);
		goto fail;
	}

<yellow>	irqfd->eventfd = eventfd;</yellow>

	if (args-&gt;flags &amp; KVM_IRQFD_FLAG_RESAMPLE) {
		struct kvm_kernel_irqfd_resampler *resampler;

<yellow>		resamplefd = eventfd_ctx_fdget(args->resamplefd);</yellow>
		if (IS_ERR(resamplefd)) {
<yellow>			ret = PTR_ERR(resamplefd);</yellow>
			goto fail;
		}

<yellow>		irqfd->resamplefd = resamplefd;</yellow>
		INIT_LIST_HEAD(&amp;irqfd-&gt;resampler_link);

		mutex_lock(&amp;kvm-&gt;irqfds.resampler_lock);

<yellow>		list_for_each_entry(resampler,</yellow>
				    &amp;kvm-&gt;irqfds.resampler_list, link) {
<yellow>			if (resampler->notifier.gsi == irqfd->gsi) {</yellow>
<yellow>				irqfd->resampler = resampler;</yellow>
				break;
			}
		}

<yellow>		if (!irqfd->resampler) {</yellow>
<yellow>			resampler = kzalloc(sizeof(*resampler),</yellow>
					    GFP_KERNEL_ACCOUNT);
			if (!resampler) {
				ret = -ENOMEM;
<yellow>				mutex_unlock(&kvm->irqfds.resampler_lock);</yellow>
				goto fail;
			}

<yellow>			resampler->kvm = kvm;</yellow>
			INIT_LIST_HEAD(&amp;resampler-&gt;list);
			resampler-&gt;notifier.gsi = irqfd-&gt;gsi;
			resampler-&gt;notifier.irq_acked = irqfd_resampler_ack;
			INIT_LIST_HEAD(&amp;resampler-&gt;link);

			list_add(&amp;resampler-&gt;link, &amp;kvm-&gt;irqfds.resampler_list);
<yellow>			kvm_register_irq_ack_notifier(kvm,</yellow>
						      &amp;resampler-&gt;notifier);
			irqfd-&gt;resampler = resampler;
		}

<yellow>		list_add_rcu(&irqfd->resampler_link, &irqfd->resampler->list);</yellow>
		synchronize_srcu(&amp;kvm-&gt;irq_srcu);

		mutex_unlock(&amp;kvm-&gt;irqfds.resampler_lock);
	}

	/*
	 * Install our own custom wake-up handling so we are notified via
	 * a callback whenever someone signals the underlying eventfd
	 */
<yellow>	init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);</yellow>
	init_poll_funcptr(&amp;irqfd-&gt;pt, irqfd_ptable_queue_proc);

	spin_lock_irq(&amp;kvm-&gt;irqfds.lock);

	ret = 0;
<yellow>	list_for_each_entry(tmp, &kvm->irqfds.items, list) {</yellow>
<yellow>		if (irqfd->eventfd != tmp->eventfd)</yellow>
			continue;
		/* This fd is used for another irq already. */
		ret = -EBUSY;
<yellow>		spin_unlock_irq(&kvm->irqfds.lock);</yellow>
		goto fail;
	}

<yellow>	idx = srcu_read_lock(&kvm->irq_srcu);</yellow>
	irqfd_update(kvm, irqfd);

	list_add_tail(&amp;irqfd-&gt;list, &amp;kvm-&gt;irqfds.items);

	spin_unlock_irq(&amp;kvm-&gt;irqfds.lock);

	/*
	 * Check if there was an event already pending on the eventfd
	 * before we registered, and trigger it as if we didn&#x27;t miss it.
	 */
<yellow>	events = vfs_poll(f.file, &irqfd->pt);</yellow>

	if (events &amp; EPOLLIN)
<yellow>		schedule_work(&irqfd->inject);</yellow>

#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
<yellow>	if (kvm_arch_has_irq_bypass()) {</yellow>
<yellow>		irqfd->consumer.token = (void *)irqfd->eventfd;</yellow>
		irqfd-&gt;consumer.add_producer = kvm_arch_irq_bypass_add_producer;
		irqfd-&gt;consumer.del_producer = kvm_arch_irq_bypass_del_producer;
		irqfd-&gt;consumer.stop = kvm_arch_irq_bypass_stop;
		irqfd-&gt;consumer.start = kvm_arch_irq_bypass_start;
		ret = irq_bypass_register_consumer(&amp;irqfd-&gt;consumer);
		if (ret)
			pr_info(&quot;irq bypass consumer (token %p) registration fails: %d\n&quot;,
				irqfd-&gt;consumer.token, ret);
	}
#endif

<yellow>	srcu_read_unlock(&kvm->irq_srcu, idx);</yellow>

	/*
	 * do not drop the file until the irqfd is fully initialized, otherwise
	 * we might race against the EPOLLHUP
	 */
<yellow>	fdput(f);</yellow>
	return 0;

fail:
<yellow>	if (irqfd->resampler)</yellow>
<yellow>		irqfd_resampler_shutdown(irqfd);</yellow>

<yellow>	if (resamplefd && !IS_ERR(resamplefd))</yellow>
<yellow>		eventfd_ctx_put(resamplefd);</yellow>

<yellow>	if (eventfd && !IS_ERR(eventfd))</yellow>
<yellow>		eventfd_ctx_put(eventfd);</yellow>

<yellow>	fdput(f);</yellow>

out:
<yellow>	kfree(irqfd);</yellow>
	return ret;
}

bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)
{
	struct kvm_irq_ack_notifier *kian;
	int gsi, idx;

<blue>	idx = srcu_read_lock(&kvm->irq_srcu);</blue>
	gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
	if (gsi != -1)
<blue>		hlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,</blue>
					  link, srcu_read_lock_held(&amp;kvm-&gt;irq_srcu))
<blue>			if (kian->gsi == gsi) {</blue>
<blue>				srcu_read_unlock(&kvm->irq_srcu, idx);</blue>
				return true;
			}

<blue>	srcu_read_unlock(&kvm->irq_srcu, idx);</blue>

	return false;
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_irq_has_notifier);

void kvm_notify_acked_gsi(struct kvm *kvm, int gsi)
{
	struct kvm_irq_ack_notifier *kian;

<blue>	hlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,</blue>
				  link, srcu_read_lock_held(&amp;kvm-&gt;irq_srcu))
<blue>		if (kian->gsi == gsi)</blue>
<blue>			kian->irq_acked(kian);</blue>
<yellow>}</yellow>

void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)
{
	int gsi, idx;

<blue>	trace_kvm_ack_irq(irqchip, pin);</blue>

<blue>	idx = srcu_read_lock(&kvm->irq_srcu);</blue>
	gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
	if (gsi != -1)
<blue>		kvm_notify_acked_gsi(kvm, gsi);</blue>
<blue>	srcu_read_unlock(&kvm->irq_srcu, idx);</blue>
}

void kvm_register_irq_ack_notifier(struct kvm *kvm,
				   struct kvm_irq_ack_notifier *kian)
{
<yellow>	mutex_lock(&kvm->irq_lock);</yellow>
<yellow>	hlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);</yellow>
<yellow>	mutex_unlock(&kvm->irq_lock);</yellow>
	kvm_arch_post_irq_ack_notifier_list_update(kvm);
}

void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
				    struct kvm_irq_ack_notifier *kian)
{
<yellow>	mutex_lock(&kvm->irq_lock);</yellow>
<yellow>	hlist_del_init_rcu(&kian->link);</yellow>
<yellow>	mutex_unlock(&kvm->irq_lock);</yellow>
	synchronize_srcu(&amp;kvm-&gt;irq_srcu);
	kvm_arch_post_irq_ack_notifier_list_update(kvm);
}
#endif

void
kvm_eventfd_init(struct kvm *kvm)
{
#ifdef CONFIG_HAVE_KVM_IRQFD
<yellow>	spin_lock_init(&kvm->irqfds.lock);</yellow>
	INIT_LIST_HEAD(&amp;kvm-&gt;irqfds.items);
	INIT_LIST_HEAD(&amp;kvm-&gt;irqfds.resampler_list);
	mutex_init(&amp;kvm-&gt;irqfds.resampler_lock);
#endif
	INIT_LIST_HEAD(&amp;kvm-&gt;ioeventfds);
}

#ifdef CONFIG_HAVE_KVM_IRQFD
/*
 * shutdown any irqfd&#x27;s that match fd+gsi
 */
static int
kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
{
	struct kvm_kernel_irqfd *irqfd, *tmp;
	struct eventfd_ctx *eventfd;

<yellow>	eventfd = eventfd_ctx_fdget(args->fd);</yellow>
	if (IS_ERR(eventfd))
<yellow>		return PTR_ERR(eventfd);</yellow>

<yellow>	spin_lock_irq(&kvm->irqfds.lock);</yellow>

<yellow>	list_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {</yellow>
<yellow>		if (irqfd->eventfd == eventfd && irqfd->gsi == args->gsi) {</yellow>
			/*
			 * This clearing of irq_entry.type is needed for when
			 * another thread calls kvm_irq_routing_update before
			 * we flush workqueue below (we synchronize with
			 * kvm_irq_routing_update using irqfds.lock).
			 */
<yellow>			write_seqcount_begin(&irqfd->irq_entry_sc);</yellow>
			irqfd-&gt;irq_entry.type = 0;
			write_seqcount_end(&amp;irqfd-&gt;irq_entry_sc);
<yellow>			irqfd_deactivate(irqfd);</yellow>
		}
	}

<yellow>	spin_unlock_irq(&kvm->irqfds.lock);</yellow>
	eventfd_ctx_put(eventfd);

	/*
	 * Block until we know all outstanding shutdown jobs have completed
	 * so that we guarantee there will not be any more interrupts on this
	 * gsi once this deassign function returns.
	 */
	flush_workqueue(irqfd_cleanup_wq);

	return 0;
}

int
kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
{
<yellow>	if (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))</yellow>
		return -EINVAL;

<yellow>	if (args->flags & KVM_IRQFD_FLAG_DEASSIGN)</yellow>
<yellow>		return kvm_irqfd_deassign(kvm, args);</yellow>

<yellow>	return kvm_irqfd_assign(kvm, args);</yellow>
<yellow>}</yellow>

/*
 * This function is called as the kvm VM fd is being released. Shutdown all
 * irqfds that still remain open
 */
void
kvm_irqfd_release(struct kvm *kvm)
{
	struct kvm_kernel_irqfd *irqfd, *tmp;

<yellow>	spin_lock_irq(&kvm->irqfds.lock);</yellow>

	list_for_each_entry_safe(irqfd, tmp, &amp;kvm-&gt;irqfds.items, list)
<yellow>		irqfd_deactivate(irqfd);</yellow>

<yellow>	spin_unlock_irq(&kvm->irqfds.lock);</yellow>

	/*
	 * Block until we know all outstanding shutdown jobs have completed
	 * since we do not take a kvm* reference.
	 */
	flush_workqueue(irqfd_cleanup_wq);

}

/*
 * Take note of a change in irq routing.
 * Caller must invoke synchronize_srcu(&amp;kvm-&gt;irq_srcu) afterwards.
 */
void kvm_irq_routing_update(struct kvm *kvm)
{
	struct kvm_kernel_irqfd *irqfd;

<yellow>	spin_lock_irq(&kvm->irqfds.lock);</yellow>

<yellow>	list_for_each_entry(irqfd, &kvm->irqfds.items, list) {</yellow>
#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
		/* Under irqfds.lock, so can read irq_entry safely */
<yellow>		struct kvm_kernel_irq_routing_entry old = irqfd->irq_entry;</yellow>
#endif

		irqfd_update(kvm, irqfd);

#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
		if (irqfd-&gt;producer &amp;&amp;
<yellow>		    kvm_arch_irqfd_route_changed(&old, &irqfd->irq_entry)) {</yellow>
			int ret = kvm_arch_update_irqfd_routing(
					irqfd-&gt;kvm, irqfd-&gt;producer-&gt;irq,
<yellow>					irqfd->gsi, 1);</yellow>
<yellow>			WARN_ON(ret);</yellow>
		}
#endif
	}

<yellow>	spin_unlock_irq(&kvm->irqfds.lock);</yellow>
}

/*
 * create a host-wide workqueue for issuing deferred shutdown requests
 * aggregated from all vm* instances. We need our own isolated
 * queue to ease flushing work items when a VM exits.
 */
int kvm_irqfd_init(void)
{
<yellow>	irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);</yellow>
	if (!irqfd_cleanup_wq)
<yellow>		return -ENOMEM;</yellow>

	return 0;
}

void kvm_irqfd_exit(void)
{
<yellow>	destroy_workqueue(irqfd_cleanup_wq);</yellow>
}
#endif

/*
 * --------------------------------------------------------------------
 * ioeventfd: translate a PIO/MMIO memory write to an eventfd signal.
 *
 * userspace can register a PIO/MMIO address with an eventfd for receiving
 * notification when the memory has been touched.
 * --------------------------------------------------------------------
 */

struct _ioeventfd {
	struct list_head     list;
	u64                  addr;
	int                  length;
	struct eventfd_ctx  *eventfd;
	u64                  datamatch;
	struct kvm_io_device dev;
	u8                   bus_idx;
	bool                 wildcard;
};

static inline struct _ioeventfd *
to_ioeventfd(struct kvm_io_device *dev)
{
	return container_of(dev, struct _ioeventfd, dev);
}

static void
ioeventfd_release(struct _ioeventfd *p)
{
<yellow>	eventfd_ctx_put(p->eventfd);</yellow>
	list_del(&amp;p-&gt;list);
	kfree(p);
}

static bool
ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
{
	u64 _val;

<yellow>	if (addr != p->addr)</yellow>
		/* address must be precise for a hit */
		return false;

<yellow>	if (!p->length)</yellow>
		/* length = 0 means only look at the address, so always a hit */
		return true;

<yellow>	if (len != p->length)</yellow>
		/* address-range must be precise for a hit */
		return false;

<yellow>	if (p->wildcard)</yellow>
		/* all else equal, wildcard is always a hit */
		return true;

	/* otherwise, we have to actually compare the data */

<yellow>	BUG_ON(!IS_ALIGNED((unsigned long)val, len));</yellow>

<yellow>	switch (len) {</yellow>
	case 1:
<yellow>		_val = *(u8 *)val;</yellow>
		break;
	case 2:
<yellow>		_val = *(u16 *)val;</yellow>
		break;
	case 4:
<yellow>		_val = *(u32 *)val;</yellow>
		break;
	case 8:
<yellow>		_val = *(u64 *)val;</yellow>
		break;
	default:
		return false;
	}

<yellow>	return _val == p->datamatch;</yellow>
}

/* MMIO/PIO writes trigger an event if the addr/val match */
static int
ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
		int len, const void *val)
{
	struct _ioeventfd *p = to_ioeventfd(this);

<yellow>	if (!ioeventfd_in_range(p, addr, len, val))</yellow>
		return -EOPNOTSUPP;

<yellow>	eventfd_signal(p->eventfd, 1);</yellow>
	return 0;
<yellow>}</yellow>

/*
 * This function is called as KVM is completely shutting down.  We do not
 * need to worry about locking just nuke anything we have as quickly as possible
 */
static void
ioeventfd_destructor(struct kvm_io_device *this)
{
	struct _ioeventfd *p = to_ioeventfd(this);

<yellow>	ioeventfd_release(p);</yellow>
}

static const struct kvm_io_device_ops ioeventfd_ops = {
	.write      = ioeventfd_write,
	.destructor = ioeventfd_destructor,
};

/* assumes kvm-&gt;slots_lock held */
static bool
ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
{
	struct _ioeventfd *_p;

<yellow>	list_for_each_entry(_p, &kvm->ioeventfds, list)</yellow>
<yellow>		if (_p->bus_idx == p->bus_idx &&</yellow>
<yellow>		    _p->addr == p->addr &&</yellow>
<yellow>		    (!_p->length || !p->length ||</yellow>
<yellow>		     (_p->length == p->length &&</yellow>
<yellow>		      (_p->wildcard || p->wildcard ||</yellow>
<yellow>		       _p->datamatch == p->datamatch))))</yellow>
			return true;

	return false;
}

static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
{
<yellow>	if (flags & KVM_IOEVENTFD_FLAG_PIO)</yellow>
		return KVM_PIO_BUS;
<yellow>	if (flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY)</yellow>
		return KVM_VIRTIO_CCW_NOTIFY_BUS;
	return KVM_MMIO_BUS;
}

static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
				enum kvm_bus bus_idx,
				struct kvm_ioeventfd *args)
{

	struct eventfd_ctx *eventfd;
	struct _ioeventfd *p;
	int ret;

<yellow>	eventfd = eventfd_ctx_fdget(args->fd);</yellow>
	if (IS_ERR(eventfd))
<yellow>		return PTR_ERR(eventfd);</yellow>

<yellow>	p = kzalloc(sizeof(*p), GFP_KERNEL_ACCOUNT);</yellow>
	if (!p) {
		ret = -ENOMEM;
		goto fail;
	}

<yellow>	INIT_LIST_HEAD(&p->list);</yellow>
	p-&gt;addr    = args-&gt;addr;
	p-&gt;bus_idx = bus_idx;
	p-&gt;length  = args-&gt;len;
	p-&gt;eventfd = eventfd;

	/* The datamatch feature is optional, otherwise this is a wildcard */
	if (args-&gt;flags &amp; KVM_IOEVENTFD_FLAG_DATAMATCH)
<yellow>		p->datamatch = args->datamatch;</yellow>
	else
<yellow>		p->wildcard = true;</yellow>

<yellow>	mutex_lock(&kvm->slots_lock);</yellow>

	/* Verify that there isn&#x27;t a match already */
<yellow>	if (ioeventfd_check_collision(kvm, p)) {</yellow>
		ret = -EEXIST;
		goto unlock_fail;
	}

<yellow>	kvm_iodevice_init(&p->dev, &ioeventfd_ops);</yellow>

<yellow>	ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,</yellow>
				      &amp;p-&gt;dev);
	if (ret &lt; 0)
		goto unlock_fail;

<yellow>	kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;</yellow>
	list_add_tail(&amp;p-&gt;list, &amp;kvm-&gt;ioeventfds);

	mutex_unlock(&amp;kvm-&gt;slots_lock);

	return 0;

unlock_fail:
<yellow>	mutex_unlock(&kvm->slots_lock);</yellow>

fail:
<yellow>	kfree(p);</yellow>
	eventfd_ctx_put(eventfd);

	return ret;
<yellow>}</yellow>

static int
kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,
			   struct kvm_ioeventfd *args)
{
	struct _ioeventfd        *p, *tmp;
	struct eventfd_ctx       *eventfd;
	struct kvm_io_bus	 *bus;
	int                       ret = -ENOENT;
	bool                      wildcard;

<yellow>	eventfd = eventfd_ctx_fdget(args->fd);</yellow>
	if (IS_ERR(eventfd))
<yellow>		return PTR_ERR(eventfd);</yellow>

<yellow>	wildcard = !(args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH);</yellow>

	mutex_lock(&amp;kvm-&gt;slots_lock);

<yellow>	list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {</yellow>

<yellow>		if (p->bus_idx != bus_idx ||</yellow>
<yellow>		    p->eventfd != eventfd  ||</yellow>
<yellow>		    p->addr != args->addr  ||</yellow>
<yellow>		    p->length != args->len ||</yellow>
<yellow>		    p->wildcard != wildcard)</yellow>
			continue;

<yellow>		if (!p->wildcard && p->datamatch != args->datamatch)</yellow>
			continue;

<yellow>		kvm_io_bus_unregister_dev(kvm, bus_idx, &p->dev);</yellow>
		bus = kvm_get_bus(kvm, bus_idx);
		if (bus)
<yellow>			bus->ioeventfd_count--;</yellow>
<yellow>		ioeventfd_release(p);</yellow>
		ret = 0;
		break;
	}

<yellow>	mutex_unlock(&kvm->slots_lock);</yellow>

	eventfd_ctx_put(eventfd);

	return ret;
<yellow>}</yellow>

static int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
{
<yellow>	enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);</yellow>
<yellow>	int ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);</yellow>

	if (!args-&gt;len &amp;&amp; bus_idx == KVM_MMIO_BUS)
<yellow>		kvm_deassign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);</yellow>

	return ret;
}

static int
kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
{
	enum kvm_bus              bus_idx;
	int ret;

<yellow>	bus_idx = ioeventfd_bus_from_flags(args->flags);</yellow>
	/* must be natural-word sized, or 0 to ignore length */
<yellow>	switch (args->len) {</yellow>
	case 0:
	case 1:
	case 2:
	case 4:
	case 8:
		break;
	default:
		return -EINVAL;
	}

	/* check for range overflow */
<yellow>	if (args->addr + args->len < args->addr)</yellow>
		return -EINVAL;

	/* check for extra flags that we don&#x27;t understand */
<yellow>	if (args->flags & ~KVM_IOEVENTFD_VALID_FLAG_MASK)</yellow>
		return -EINVAL;

	/* ioeventfd with no length can&#x27;t be combined with DATAMATCH */
<yellow>	if (!args->len && (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH))</yellow>
		return -EINVAL;

<yellow>	ret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);</yellow>
	if (ret)
		goto fail;

	/* When length is ignored, MMIO is also put on a separate bus, for
	 * faster lookups.
	 */
<yellow>	if (!args->len && bus_idx == KVM_MMIO_BUS) {</yellow>
<yellow>		ret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);</yellow>
		if (ret &lt; 0)
			goto fast_fail;
	}

	return 0;

fast_fail:
<yellow>	kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);</yellow>
fail:
	return ret;
}

int
kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
{
<yellow>	if (args->flags & KVM_IOEVENTFD_FLAG_DEASSIGN)</yellow>
<yellow>		return kvm_deassign_ioeventfd(kvm, args);</yellow>

<yellow>	return kvm_assign_ioeventfd(kvm, args);</yellow>
<yellow>}</yellow>


</code></pre></td></tr></table>
</body>
</html>
