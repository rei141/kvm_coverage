<doctype html>
<html lang="ja">
<head>
<meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
</style>

</head>
<body>
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line>1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.
57.
58.
59.
60.
61.
62.
63.
64.
65.
66.
67.
68.
69.
70.
71.
72.
73.
74.
75.
76.
77.
78.
79.
80.
81.
82.
83.
84.
85.
86.
87.
88.
89.
90.
91.
92.
93.
94.
95.
96.
97.
98.
99.
100.
101.
102.
103.
</code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">/* SPDX-License-Identifier: GPL-2.0 */
#ifndef _ASM_X86_BARRIER_H
#define _ASM_X86_BARRIER_H

#include &lt;asm/alternative.h&gt;
#include &lt;asm/nops.h&gt;

/*
 * Force strict CPU ordering.
 * And yes, this might be required on UP too when we&#x27;re talking
 * to devices.
 */

#ifdef CONFIG_X86_32
#define mb() asm volatile(ALTERNATIVE(&quot;lock; addl $0,-4(%%esp)&quot;, &quot;mfence&quot;, \
				      X86_FEATURE_XMM2) ::: &quot;memory&quot;, &quot;cc&quot;)
#define rmb() asm volatile(ALTERNATIVE(&quot;lock; addl $0,-4(%%esp)&quot;, &quot;lfence&quot;, \
				       X86_FEATURE_XMM2) ::: &quot;memory&quot;, &quot;cc&quot;)
#define wmb() asm volatile(ALTERNATIVE(&quot;lock; addl $0,-4(%%esp)&quot;, &quot;sfence&quot;, \
				       X86_FEATURE_XMM2) ::: &quot;memory&quot;, &quot;cc&quot;)
#else
#define __mb()	asm volatile(&quot;mfence&quot;:::&quot;memory&quot;)
#define __rmb()	asm volatile(&quot;lfence&quot;:::&quot;memory&quot;)
#define __wmb()	asm volatile(&quot;sfence&quot; ::: &quot;memory&quot;)
#endif

/**
 * array_index_mask_nospec() - generate a mask that is ~0UL when the
 * 	bounds check succeeds and 0 otherwise
 * @index: array element index
 * @size: number of elements in array
 *
 * Returns:
 *     0 - (index &lt; size)
 */
static inline unsigned long array_index_mask_nospec(unsigned long index,
		unsigned long size)
{
	unsigned long mask;

<blue>	asm volatile ("cmp %1,%2; sbb %0,%0;"</blue>
			:&quot;=r&quot; (mask)
			:&quot;g&quot;(size),&quot;r&quot; (index)
			:&quot;cc&quot;);
	return mask;
}

/* Override the default implementation from linux/nospec.h. */
#define array_index_mask_nospec array_index_mask_nospec

/* Prevent speculative execution past this barrier. */
#define barrier_nospec() alternative(&quot;&quot;, &quot;lfence&quot;, X86_FEATURE_LFENCE_RDTSC)

#define __dma_rmb()	barrier()
#define __dma_wmb()	barrier()

#define __smp_mb()	asm volatile(&quot;lock; addl $0,-4(%%&quot; _ASM_SP &quot;)&quot; ::: &quot;memory&quot;, &quot;cc&quot;)

#define __smp_rmb()	dma_rmb()
#define __smp_wmb()	barrier()
#define __smp_store_mb(var, value) do { (void)xchg(&amp;var, value); } while (0)

#define __smp_store_release(p, v)					\
do {									\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	WRITE_ONCE(*p, v);						\
} while (0)

#define __smp_load_acquire(p)						\
({									\
	typeof(*p) ___p1 = READ_ONCE(*p);				\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	___p1;								\
})

/* Atomic operations are already serializing on x86 */
#define __smp_mb__before_atomic()	do { } while (0)
#define __smp_mb__after_atomic()	do { } while (0)

#include &lt;asm-generic/barrier.h&gt;

/*
 * Make previous memory operations globally visible before
 * a WRMSR.
 *
 * MFENCE makes writes visible, but only affects load/store
 * instructions.  WRMSR is unfortunately not a load/store
 * instruction and is unaffected by MFENCE.  The LFENCE ensures
 * that the WRMSR is not reordered.
 *
 * Most WRMSRs are full serializing instructions themselves and
 * do not require this barrier.  This is only required for the
 * IA32_TSC_DEADLINE and X2APIC MSRs.
 */
static inline void weak_wrmsr_fence(void)
{
	asm volatile(&quot;mfence; lfence&quot; : : : &quot;memory&quot;);
}

#endif /* _ASM_X86_BARRIER_H */


</code></pre></td></tr></table>
</body>
</html>
