<doctype html>
<html lang="ja">
<head><title>mtrr.c</title><meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
      .split {
         height: 100%;
         position: fixed;
         z-index: 1;
         top: 0;
         overflow-x: hidden;
      }

      .tree {
         left: 0;
         width: 20%;
      }

      .right {
         border-left: 2px solid #444;
         right: 0;
         width: 80%;
         /* font-family: 'Courier New', Courier, monospace;
				color: rgb(80, 80, 80); */
      }
</style>

</head>
<body>
   <div class="split tree">
      <ul id="file_list">
      </ul>
   </div>
   <div class="split right">
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line><script>for (let i = 1; i <= 722; i++){
         document.write(i+".\n");
   }
         </script></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * vMTRR implementation
 *
 * Copyright (C) 2006 Qumranet, Inc.
 * Copyright 2010 Red Hat, Inc. and/or its affiliates.
 * Copyright(C) 2015 Intel Corporation.
 *
 * Authors:
 *   Yaniv Kamay  &lt;yaniv@qumranet.com&gt;
 *   Avi Kivity   &lt;avi@qumranet.com&gt;
 *   Marcelo Tosatti &lt;mtosatti@redhat.com&gt;
 *   Paolo Bonzini &lt;pbonzini@redhat.com&gt;
 *   Xiao Guangrong &lt;guangrong.xiao@linux.intel.com&gt;
 */

#include &lt;linux/kvm_host.h&gt;
#include &lt;asm/mtrr.h&gt;

#include &quot;cpuid.h&quot;
#include &quot;mmu.h&quot;

#define IA32_MTRR_DEF_TYPE_E		(1ULL &lt;&lt; 11)
#define IA32_MTRR_DEF_TYPE_FE		(1ULL &lt;&lt; 10)
#define IA32_MTRR_DEF_TYPE_TYPE_MASK	(0xff)

static bool msr_mtrr_valid(unsigned msr)
{
<blue>	switch (msr) {</blue>
	case 0x200 ... 0x200 + 2 * KVM_NR_VAR_MTRR - 1:
	case MSR_MTRRfix64K_00000:
	case MSR_MTRRfix16K_80000:
	case MSR_MTRRfix16K_A0000:
	case MSR_MTRRfix4K_C0000:
	case MSR_MTRRfix4K_C8000:
	case MSR_MTRRfix4K_D0000:
	case MSR_MTRRfix4K_D8000:
	case MSR_MTRRfix4K_E0000:
	case MSR_MTRRfix4K_E8000:
	case MSR_MTRRfix4K_F0000:
	case MSR_MTRRfix4K_F8000:
	case MSR_MTRRdefType:
	case MSR_IA32_CR_PAT:
		return true;
	}
	return false;
}

static bool valid_mtrr_type(unsigned t)
{
<blue>	return t < 8 && (1 << t) & 0x73; /* 0, 1, 4, 5, 6 */</blue>
}

<blue>bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)</blue>
{
	int i;
	u64 mask;

<blue>	if (!msr_mtrr_valid(msr))</blue>
		return false;

<blue>	if (msr == MSR_IA32_CR_PAT) {</blue>
<yellow>		return kvm_pat_valid(data);</yellow>
<blue>	} else if (msr == MSR_MTRRdefType) {</blue>
<blue>		if (data & ~0xcff)</blue>
			return false;
<blue>		return valid_mtrr_type(data & 0xff);</blue>
<blue>	} else if (msr >= MSR_MTRRfix64K_00000 && msr <= MSR_MTRRfix4K_F8000) {</blue>
<blue>		for (i = 0; i < 8 ; i++)</blue>
<blue>			if (!valid_mtrr_type((data >> (i * 8)) & 0xff))</blue>
				return false;
		return true;
	}

	/* variable MTRRs */
<blue>	WARN_ON(!(msr >= 0x200 && msr < 0x200 + 2 * KVM_NR_VAR_MTRR));</blue>

	mask = kvm_vcpu_reserved_gpa_bits_raw(vcpu);
<blue>	if ((msr & 1) == 0) {</blue>
		/* MTRR base */
<blue>		if (!valid_mtrr_type(data & 0xff))</blue>
			return false;
<blue>		mask |= 0xf00;</blue>
	} else
		/* MTRR mask */
<blue>		mask |= 0x7ff;</blue>

<blue>	return (data & mask) == 0;</blue>
<blue>}</blue>
EXPORT_SYMBOL_GPL(kvm_mtrr_valid);

static bool mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
{
<yellow>	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E);</yellow>
}

static bool fixed_mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
{
<yellow>	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE);</yellow>
}

static u8 mtrr_default_type(struct kvm_mtrr *mtrr_state)
{
<yellow>	return mtrr_state->deftype & IA32_MTRR_DEF_TYPE_TYPE_MASK;</yellow>
}

static u8 mtrr_disabled_type(struct kvm_vcpu *vcpu)
{
	/*
	 * Intel SDM 11.11.2.2: all MTRRs are disabled when
	 * IA32_MTRR_DEF_TYPE.E bit is cleared, and the UC
	 * memory type is applied to all of physical memory.
	 *
	 * However, virtual machines can be run with CPUID such that
	 * there are no MTRRs.  In that case, the firmware will never
	 * enable MTRRs and it is obviously undesirable to run the
	 * guest entirely with UC memory and we use WB.
	 */
<yellow>	if (guest_cpuid_has(vcpu, X86_FEATURE_MTRR))</yellow>
		return MTRR_TYPE_UNCACHABLE;
	else
		return MTRR_TYPE_WRBACK;
}

/*
* Three terms are used in the following code:
* - segment, it indicates the address segments covered by fixed MTRRs.
* - unit, it corresponds to the MSR entry in the segment.
* - range, a range is covered in one memory cache type.
*/
struct fixed_mtrr_segment {
	u64 start;
	u64 end;

	int range_shift;

	/* the start position in kvm_mtrr.fixed_ranges[]. */
	int range_start;
};

static struct fixed_mtrr_segment fixed_seg_table[] = {
	/* MSR_MTRRfix64K_00000, 1 unit. 64K fixed mtrr. */
	{
		.start = 0x0,
		.end = 0x80000,
		.range_shift = 16, /* 64K */
		.range_start = 0,
	},

	/*
	 * MSR_MTRRfix16K_80000 ... MSR_MTRRfix16K_A0000, 2 units,
	 * 16K fixed mtrr.
	 */
	{
		.start = 0x80000,
		.end = 0xc0000,
		.range_shift = 14, /* 16K */
		.range_start = 8,
	},

	/*
	 * MSR_MTRRfix4K_C0000 ... MSR_MTRRfix4K_F8000, 8 units,
	 * 4K fixed mtrr.
	 */
	{
		.start = 0xc0000,
		.end = 0x100000,
		.range_shift = 12, /* 12K */
		.range_start = 24,
	}
};

/*
 * The size of unit is covered in one MSR, one MSR entry contains
 * 8 ranges so that unit size is always 8 * 2^range_shift.
 */
static u64 fixed_mtrr_seg_unit_size(int seg)
{
<yellow>	return 8 << fixed_seg_table[seg].range_shift;</yellow>
}

<blue>static bool fixed_msr_to_seg_unit(u32 msr, int *seg, int *unit)</blue>
{
<blue>	switch (msr) {</blue>
	case MSR_MTRRfix64K_00000:
		*seg = 0;
		*unit = 0;
		break;
	case MSR_MTRRfix16K_80000 ... MSR_MTRRfix16K_A0000:
		*seg = 1;
<blue>		*unit = array_index_nospec(</blue>
			msr - MSR_MTRRfix16K_80000,
			MSR_MTRRfix16K_A0000 - MSR_MTRRfix16K_80000 + 1);
		break;
	case MSR_MTRRfix4K_C0000 ... MSR_MTRRfix4K_F8000:
		*seg = 2;
<blue>		*unit = array_index_nospec(</blue>
			msr - MSR_MTRRfix4K_C0000,
			MSR_MTRRfix4K_F8000 - MSR_MTRRfix4K_C0000 + 1);
		break;
	default:
		return false;
	}

	return true;
}

static void fixed_mtrr_seg_unit_range(int seg, int unit, u64 *start, u64 *end)
{
<yellow>	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];</yellow>
<yellow>	u64 unit_size = fixed_mtrr_seg_unit_size(seg);</yellow>

	*start = mtrr_seg-&gt;start + unit * unit_size;
	*end = *start + unit_size;
<yellow>	WARN_ON(*end > mtrr_seg->end);</yellow>
}

static int fixed_mtrr_seg_unit_range_index(int seg, int unit)
{
	struct fixed_mtrr_segment *mtrr_seg = &amp;fixed_seg_table[seg];

<blue>	WARN_ON(mtrr_seg->start + unit * fixed_mtrr_seg_unit_size(seg)</blue>
		&gt; mtrr_seg-&gt;end);

	/* each unit has 8 ranges. */
<blue>	return mtrr_seg->range_start + 8 * unit;</blue>
}

static int fixed_mtrr_seg_end_range_index(int seg)
{
	struct fixed_mtrr_segment *mtrr_seg = &amp;fixed_seg_table[seg];
	int n;

	n = (mtrr_seg-&gt;end - mtrr_seg-&gt;start) &gt;&gt; mtrr_seg-&gt;range_shift;
<yellow>	return mtrr_seg->range_start + n - 1;</yellow>
}

static bool fixed_msr_to_range(u32 msr, u64 *start, u64 *end)
{
	int seg, unit;

<yellow>	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))</yellow>
		return false;

<yellow>	fixed_mtrr_seg_unit_range(seg, unit, start, end);</yellow>
	return true;
}

static int fixed_msr_to_range_index(u32 msr)
{
	int seg, unit;

<blue>	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))</blue>
		return -1;

<blue>	return fixed_mtrr_seg_unit_range_index(seg, unit);</blue>
<blue>}</blue>

static int fixed_mtrr_addr_to_seg(u64 addr)
{
	struct fixed_mtrr_segment *mtrr_seg;
	int seg, seg_num = ARRAY_SIZE(fixed_seg_table);

<yellow>	for (seg = 0; seg < seg_num; seg++) {</yellow>
<yellow>		mtrr_seg = &fixed_seg_table[seg];</yellow>
<yellow>		if (mtrr_seg->start <= addr && addr < mtrr_seg->end)</yellow>
			return seg;
	}

	return -1;
}

static int fixed_mtrr_addr_seg_to_range_index(u64 addr, int seg)
{
	struct fixed_mtrr_segment *mtrr_seg;
	int index;

	mtrr_seg = &amp;fixed_seg_table[seg];
	index = mtrr_seg-&gt;range_start;
	index += (addr - mtrr_seg-&gt;start) &gt;&gt; mtrr_seg-&gt;range_shift;
	return index;
}

static u64 fixed_mtrr_range_end_addr(int seg, int index)
{
	struct fixed_mtrr_segment *mtrr_seg = &amp;fixed_seg_table[seg];
	int pos = index - mtrr_seg-&gt;range_start;

<yellow>	return mtrr_seg->start + ((pos + 1) << mtrr_seg->range_shift);</yellow>
}

static void var_mtrr_range(struct kvm_mtrr_range *range, u64 *start, u64 *end)
{
	u64 mask;

<yellow>	*start = range->base & PAGE_MASK;</yellow>

<yellow>	mask = range->mask & PAGE_MASK;</yellow>

	/* This cannot overflow because writing to the reserved bits of
	 * variable MTRRs causes a #GP.
	 */
	*end = (*start | ~mask) + 1;
}

static void update_mtrr(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_mtrr *mtrr_state = &amp;vcpu-&gt;arch.mtrr_state;
	gfn_t start, end;
	int index;

<blue>	if (msr == MSR_IA32_CR_PAT || !tdp_enabled ||</blue>
<blue>	      !kvm_arch_has_noncoherent_dma(vcpu->kvm))</blue>
		return;

<yellow>	if (!mtrr_is_enabled(mtrr_state) && msr != MSR_MTRRdefType)</yellow>
		return;

	/* fixed MTRRs. */
<yellow>	if (fixed_msr_to_range(msr, &start, &end)) {</yellow>
<yellow>		if (!fixed_mtrr_is_enabled(mtrr_state))</yellow>
			return;
<yellow>	} else if (msr == MSR_MTRRdefType) {</yellow>
		start = 0x0;
		end = ~0ULL;
	} else {
		/* variable range MTRRs. */
<yellow>		index = (msr - 0x200) / 2;</yellow>
		var_mtrr_range(&amp;mtrr_state-&gt;var_ranges[index], &amp;start, &amp;end);
	}

<yellow>	kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));</yellow>
}

static bool var_mtrr_range_is_valid(struct kvm_mtrr_range *range)
{
<blue>	return (range->mask & (1 << 11)) != 0;</blue>
}

static void set_var_mtrr_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
{
	struct kvm_mtrr *mtrr_state = &amp;vcpu-&gt;arch.mtrr_state;
	struct kvm_mtrr_range *tmp, *cur;
	int index, is_mtrr_mask;

<blue>	index = (msr - 0x200) / 2;</blue>
	is_mtrr_mask = msr - 0x200 - 2 * index;
	cur = &amp;mtrr_state-&gt;var_ranges[index];

	/* remove the entry if it&#x27;s in the list. */
	if (var_mtrr_range_is_valid(cur))
<yellow>		list_del(&mtrr_state->var_ranges[index].node);</yellow>

	/*
	 * Set all illegal GPA bits in the mask, since those bits must
	 * implicitly be 0.  The bits are then cleared when reading them.
	 */
<blue>	if (!is_mtrr_mask)</blue>
<blue>		cur->base = data;</blue>
	else
<blue>		cur->mask = data | kvm_vcpu_reserved_gpa_bits_raw(vcpu);</blue>

	/* add it to the list if it&#x27;s enabled. */
<blue>	if (var_mtrr_range_is_valid(cur)) {</blue>
<blue>		list_for_each_entry(tmp, &mtrr_state->head, node)</blue>
<yellow>			if (cur->base >= tmp->base)</yellow>
				break;
<blue>		list_add_tail(&cur->node, &tmp->node);</blue>
	}
}

int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
{
	int index;

<blue>	if (!kvm_mtrr_valid(vcpu, msr, data))</blue>
		return 1;

<blue>	index = fixed_msr_to_range_index(msr);</blue>
	if (index &gt;= 0)
<blue>		*(u64 *)&vcpu->arch.mtrr_state.fixed_ranges[index] = data;</blue>
<blue>	else if (msr == MSR_MTRRdefType)</blue>
<blue>		vcpu->arch.mtrr_state.deftype = data;</blue>
<blue>	else if (msr == MSR_IA32_CR_PAT)</blue>
<yellow>		vcpu->arch.pat = data;</yellow>
	else
<blue>		set_var_mtrr_msr(vcpu, msr, data);</blue>

<blue>	update_mtrr(vcpu, msr);</blue>
	return 0;
<blue>}</blue>

int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
{
	int index;

	/* MSR_MTRRcap is a readonly MSR. */
<blue>	if (msr == MSR_MTRRcap) {</blue>
		/*
		 * SMRR = 0
		 * WC = 1
		 * FIX = 1
		 * VCNT = KVM_NR_VAR_MTRR
		 */
<blue>		*pdata = 0x500 | KVM_NR_VAR_MTRR;</blue>
		return 0;
	}

<blue>	if (!msr_mtrr_valid(msr))</blue>
		return 1;

<blue>	index = fixed_msr_to_range_index(msr);</blue>
	if (index &gt;= 0)
<blue>		*pdata = *(u64 *)&vcpu->arch.mtrr_state.fixed_ranges[index];</blue>
<blue>	else if (msr == MSR_MTRRdefType)</blue>
<blue>		*pdata = vcpu->arch.mtrr_state.deftype;</blue>
<blue>	else if (msr == MSR_IA32_CR_PAT)</blue>
<blue>		*pdata = vcpu->arch.pat;</blue>
	else {	/* Variable MTRRs */
		int is_mtrr_mask;

<blue>		index = (msr - 0x200) / 2;</blue>
		is_mtrr_mask = msr - 0x200 - 2 * index;
		if (!is_mtrr_mask)
<blue>			*pdata = vcpu->arch.mtrr_state.var_ranges[index].base;</blue>
		else
<blue>			*pdata = vcpu->arch.mtrr_state.var_ranges[index].mask;</blue>

		*pdata &amp;= ~kvm_vcpu_reserved_gpa_bits_raw(vcpu);
	}

	return 0;
<blue>}</blue>

void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu)
{
<blue>	INIT_LIST_HEAD(&vcpu->arch.mtrr_state.head);</blue>
}

struct mtrr_iter {
	/* input fields. */
	struct kvm_mtrr *mtrr_state;
	u64 start;
	u64 end;

	/* output fields. */
	int mem_type;
	/* mtrr is completely disabled? */
	bool mtrr_disabled;
	/* [start, end) is not fully covered in MTRRs? */
	bool partial_map;

	/* private fields. */
	union {
		/* used for fixed MTRRs. */
		struct {
			int index;
			int seg;
		};

		/* used for var MTRRs. */
		struct {
			struct kvm_mtrr_range *range;
			/* max address has been covered in var MTRRs. */
			u64 start_max;
		};
	};

	bool fixed;
};

static bool mtrr_lookup_fixed_start(struct mtrr_iter *iter)
{
	int seg, index;

	if (!fixed_mtrr_is_enabled(iter-&gt;mtrr_state))
		return false;

<yellow>	seg = fixed_mtrr_addr_to_seg(iter->start);</yellow>
	if (seg &lt; 0)
		return false;

<yellow>	iter->fixed = true;</yellow>
	index = fixed_mtrr_addr_seg_to_range_index(iter-&gt;start, seg);
<yellow>	iter->index = index;</yellow>
	iter-&gt;seg = seg;
	return true;
}

static bool match_var_range(struct mtrr_iter *iter,
			    struct kvm_mtrr_range *range)
{
	u64 start, end;

<yellow>	var_mtrr_range(range, &start, &end);</yellow>
<yellow>	if (!(start >= iter->end || end <= iter->start)) {</yellow>
		iter-&gt;range = range;

		/*
		 * the function is called when we do kvm_mtrr.head walking.
		 * Range has the minimum base address which interleaves
		 * [looker-&gt;start_max, looker-&gt;end).
		 */
<yellow>		iter->partial_map |= iter->start_max < start;</yellow>

		/* update the max address has been covered. */
		iter-&gt;start_max = max(iter-&gt;start_max, end);
		return true;
	}

	return false;
}

<yellow>static void __mtrr_lookup_var_next(struct mtrr_iter *iter)</yellow>
{
<yellow>	struct kvm_mtrr *mtrr_state = iter->mtrr_state;</yellow>

<yellow>	list_for_each_entry_continue(iter->range, &mtrr_state->head, node)</yellow>
<yellow>		if (match_var_range(iter, iter->range))</yellow>
			return;

	iter-&gt;range = NULL;
<yellow>	iter->partial_map |= iter->start_max < iter->end;</yellow>
<yellow>}</yellow>

static void mtrr_lookup_var_start(struct mtrr_iter *iter)
{
<yellow>	struct kvm_mtrr *mtrr_state = iter->mtrr_state;</yellow>

<yellow>	iter->fixed = false;</yellow>
	iter-&gt;start_max = iter-&gt;start;
	iter-&gt;range = NULL;
	iter-&gt;range = list_prepare_entry(iter-&gt;range, &amp;mtrr_state-&gt;head, node);

	__mtrr_lookup_var_next(iter);
}

static void mtrr_lookup_fixed_next(struct mtrr_iter *iter)
{
	/* terminate the lookup. */
<yellow>	if (fixed_mtrr_range_end_addr(iter->seg, iter->index) >= iter->end) {</yellow>
<yellow>		iter->fixed = false;</yellow>
		iter-&gt;range = NULL;
		return;
	}

<yellow>	iter->index++;</yellow>

	/* have looked up for all fixed MTRRs. */
	if (iter-&gt;index &gt;= ARRAY_SIZE(iter-&gt;mtrr_state-&gt;fixed_ranges))
<yellow>		return mtrr_lookup_var_start(iter);</yellow>

	/* switch to next segment. */
<yellow>	if (iter->index > fixed_mtrr_seg_end_range_index(iter->seg))</yellow>
<yellow>		iter->seg++;</yellow>
}

static void mtrr_lookup_var_next(struct mtrr_iter *iter)
{
<yellow>	__mtrr_lookup_var_next(iter);</yellow>
}

static void mtrr_lookup_start(struct mtrr_iter *iter)
{
<yellow>	if (!mtrr_is_enabled(iter->mtrr_state)) {</yellow>
<yellow>		iter->mtrr_disabled = true;</yellow>
		return;
	}

<yellow>	if (!mtrr_lookup_fixed_start(iter))</yellow>
<yellow>		mtrr_lookup_var_start(iter);</yellow>
<yellow>}</yellow>

static void mtrr_lookup_init(struct mtrr_iter *iter,
			     struct kvm_mtrr *mtrr_state, u64 start, u64 end)
{
	iter-&gt;mtrr_state = mtrr_state;
	iter-&gt;start = start;
	iter-&gt;end = end;
	iter-&gt;mtrr_disabled = false;
	iter-&gt;partial_map = false;
	iter-&gt;fixed = false;
	iter-&gt;range = NULL;

	mtrr_lookup_start(iter);
}

<yellow>static bool mtrr_lookup_okay(struct mtrr_iter *iter)</yellow>
{
<yellow>	if (iter->fixed) {</yellow>
<yellow>		iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];</yellow>
		return true;
	}

<yellow>	if (iter->range) {</yellow>
<yellow>		iter->mem_type = iter->range->base & 0xff;</yellow>
		return true;
	}

	return false;
}

static void mtrr_lookup_next(struct mtrr_iter *iter)
{
<yellow>	if (iter->fixed)</yellow>
<yellow>		mtrr_lookup_fixed_next(iter);</yellow>
	else
<yellow>		mtrr_lookup_var_next(iter);</yellow>
<yellow>}</yellow>

#define mtrr_for_each_mem_type(_iter_, _mtrr_, _gpa_start_, _gpa_end_) \
	for (mtrr_lookup_init(_iter_, _mtrr_, _gpa_start_, _gpa_end_); \
	     mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))

u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
<yellow>{</yellow>
	struct kvm_mtrr *mtrr_state = &amp;vcpu-&gt;arch.mtrr_state;
	struct mtrr_iter iter;
	u64 start, end;
	int type = -1;
	const int wt_wb_mask = (1 &lt;&lt; MTRR_TYPE_WRBACK)
			       | (1 &lt;&lt; MTRR_TYPE_WRTHROUGH);

<yellow>	start = gfn_to_gpa(gfn);</yellow>
	end = start + PAGE_SIZE;

<yellow>	mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {</yellow>
		int curr_type = iter.mem_type;

		/*
		 * Please refer to Intel SDM Volume 3: 11.11.4.1 MTRR
		 * Precedences.
		 */

<yellow>		if (type == -1) {</yellow>
			type = curr_type;
			continue;
		}

		/*
		 * If two or more variable memory ranges match and the
		 * memory types are identical, then that memory type is
		 * used.
		 */
<yellow>		if (type == curr_type)</yellow>
			continue;

		/*
		 * If two or more variable memory ranges match and one of
		 * the memory types is UC, the UC memory type used.
		 */
<yellow>		if (curr_type == MTRR_TYPE_UNCACHABLE)</yellow>
			return MTRR_TYPE_UNCACHABLE;

		/*
		 * If two or more variable memory ranges match and the
		 * memory types are WT and WB, the WT memory type is used.
		 */
<yellow>		if (((1 << type) & wt_wb_mask) &&</yellow>
		      ((1 &lt;&lt; curr_type) &amp; wt_wb_mask)) {
			type = MTRR_TYPE_WRTHROUGH;
			continue;
		}

		/*
		 * For overlaps not defined by the above rules, processor
		 * behavior is undefined.
		 */

		/* We use WB for this undefined behavior. :( */
		return MTRR_TYPE_WRBACK;
	}

<yellow>	if (iter.mtrr_disabled)</yellow>
<yellow>		return mtrr_disabled_type(vcpu);</yellow>

	/* not contained in any MTRRs. */
<yellow>	if (type == -1)</yellow>
<yellow>		return mtrr_default_type(mtrr_state);</yellow>

	/*
	 * We just check one page, partially covered by MTRRs is
	 * impossible.
	 */
<yellow>	WARN_ON(iter.partial_map);</yellow>

<yellow>	return type;</yellow>
}
EXPORT_SYMBOL_GPL(kvm_mtrr_get_guest_memory_type);

bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
					  int page_num)
<yellow>{</yellow>
<yellow>	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;</yellow>
	struct mtrr_iter iter;
	u64 start, end;
	int type = -1;

	start = gfn_to_gpa(gfn);
	end = gfn_to_gpa(gfn + page_num);
<yellow>	mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {</yellow>
<yellow>		if (type == -1) {</yellow>
<yellow>			type = iter.mem_type;</yellow>
			continue;
		}

<yellow>		if (type != iter.mem_type)</yellow>
			return false;
	}

<yellow>	if (iter.mtrr_disabled)</yellow>
		return true;

<yellow>	if (!iter.partial_map)</yellow>
		return true;

<yellow>	if (type == -1)</yellow>
		return true;

<yellow>	return type == mtrr_default_type(mtrr_state);</yellow>
}


</code></pre></td></tr></table>
</div><script>const fileList = document.getElementById('file_list')
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/mmu/mmu.c.html">mmu.c 58.8%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/nested.c.html">nested.c 80.9%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/vmx.c.html">vmx.c 58.2%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/x86.c.html">x86.c 50.7%</li>`
</script></body></html>