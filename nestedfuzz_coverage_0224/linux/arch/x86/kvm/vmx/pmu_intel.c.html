<doctype html>
<html lang="ja">
<head><title>pmu_intel.c</title><meta charset="utf-8">
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<style>
    code{
        font-family:Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
    }
    code_line{
        font-family: Monaco, Menlo, Consolas, 'Courier New', Courier, monospace, sans-serif;;
        font-size: 14px;
        line-height: 18px;
        overflow: auto;
        resize: horizontal;
        color:#303134;
    }
    blue{
        background-color:#BEEDE8;
    }
    yellow{
        background-color:#FFFF99;
    }
    red{
        background-color:#FF99AC;
    }
      .split {
         height: 100%;
         position: fixed;
         z-index: 1;
         top: 0;
         overflow-x: hidden;
      }

      .tree {
         left: 0;
         width: 20%;
      }

      .right {
         border-left: 2px solid #444;
         right: 0;
         width: 80%;
         /* font-family: 'Courier New', Courier, monospace;
				color: rgb(80, 80, 80); */
      }
</style>

</head>
<body>
   <div class="split tree">
      <ul id="file_list">
      </ul>
   </div>
   <div class="split right">
<table summary='blob content' class='blob' cellspacing="15">
<tr><td align="right"><pre><code_line><script>for (let i = 1; i <= 815; i++){
         document.write(i+".\n");
   }
         </script></code_line></pre></td>
<td class='lines'><pre><code class="prettyprint">// SPDX-License-Identifier: GPL-2.0-only
/*
 * KVM PMU support for Intel CPUs
 *
 * Copyright 2011 Red Hat, Inc. and/or its affiliates.
 *
 * Authors:
 *   Avi Kivity   &lt;avi@redhat.com&gt;
 *   Gleb Natapov &lt;gleb@redhat.com&gt;
 */
#include &lt;linux/types.h&gt;
#include &lt;linux/kvm_host.h&gt;
#include &lt;linux/perf_event.h&gt;
#include &lt;asm/perf_event.h&gt;
#include &quot;x86.h&quot;
#include &quot;cpuid.h&quot;
#include &quot;lapic.h&quot;
#include &quot;nested.h&quot;
#include &quot;pmu.h&quot;

#define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)

static struct kvm_event_hw_type_mapping intel_arch_events[] = {
	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
	[2] = { 0x3c, 0x01, PERF_COUNT_HW_BUS_CYCLES  },
	[3] = { 0x2e, 0x4f, PERF_COUNT_HW_CACHE_REFERENCES },
	[4] = { 0x2e, 0x41, PERF_COUNT_HW_CACHE_MISSES },
	[5] = { 0xc4, 0x00, PERF_COUNT_HW_BRANCH_INSTRUCTIONS },
	[6] = { 0xc5, 0x00, PERF_COUNT_HW_BRANCH_MISSES },
	/* The above index must match CPUID 0x0A.EBX bit vector */
	[7] = { 0x00, 0x03, PERF_COUNT_HW_REF_CPU_CYCLES },
};

/* mapping between fixed pmc index and intel_arch_events array */
static int fixed_pmc_events[] = {1, 0, 7};

static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
{
	struct kvm_pmc *pmc;
	u8 old_fixed_ctr_ctrl = pmu-&gt;fixed_ctr_ctrl;
	int i;

	pmu-&gt;fixed_ctr_ctrl = data;
<yellow>	for (i = 0; i < pmu->nr_arch_fixed_counters; i++) {</yellow>
<yellow>		u8 new_ctrl = fixed_ctrl_field(data, i);</yellow>
<yellow>		u8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);</yellow>

		if (old_ctrl == new_ctrl)
			continue;

<yellow>		pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);</yellow>

<yellow>		__set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);</yellow>
		reprogram_counter(pmc);
	}
}

<blue>static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)</blue>
{
<blue>	if (pmc_idx < INTEL_PMC_IDX_FIXED) {</blue>
<blue>		return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,</blue>
				  MSR_P6_EVNTSEL0);
	} else {
		u32 idx = pmc_idx - INTEL_PMC_IDX_FIXED;

<blue>		return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);</blue>
	}
<blue>}</blue>

static void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
{
	int bit;
	struct kvm_pmc *pmc;

<yellow>	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX) {</yellow>
<yellow>		pmc = intel_pmc_idx_to_pmc(pmu, bit);</yellow>
<yellow>		if (pmc)</yellow>
<yellow>			reprogram_counter(pmc);</yellow>
	}
<yellow>}</yellow>

static bool intel_hw_event_available(struct kvm_pmc *pmc)
{
<yellow>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</yellow>
	u8 event_select = pmc-&gt;eventsel &amp; ARCH_PERFMON_EVENTSEL_EVENT;
	u8 unit_mask = (pmc-&gt;eventsel &amp; ARCH_PERFMON_EVENTSEL_UMASK) &gt;&gt; 8;
	int i;

<yellow>	for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {</yellow>
<yellow>		if (intel_arch_events[i].eventsel != event_select ||</yellow>
<yellow>		    intel_arch_events[i].unit_mask != unit_mask)</yellow>
			continue;

		/* disable event that reported as not present by cpuid */
<yellow>		if ((i < 7) && !(pmu->available_event_types & (1 << i)))</yellow>
			return false;

		break;
	}

	return true;
<yellow>}</yellow>

/* check if a PMC is enabled by comparing it with globl_ctrl bits. */
static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
{
<blue>	struct kvm_pmu *pmu = pmc_to_pmu(pmc);</blue>

	if (!intel_pmu_has_perf_global_ctrl(pmu))
		return true;

<blue>	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);</blue>
<blue>}</blue>

static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	bool fixed = idx &amp; (1u &lt;&lt; 30);

	idx &amp;= ~(3u &lt;&lt; 30);

<yellow>	return fixed ? idx < pmu->nr_arch_fixed_counters</yellow>
<yellow>		     : idx < pmu->nr_arch_gp_counters;</yellow>
<yellow>}</yellow>

static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
					    unsigned int idx, u64 *mask)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
<blue>	bool fixed = idx & (1u << 30);</blue>
	struct kvm_pmc *counters;
	unsigned int num_counters;

	idx &amp;= ~(3u &lt;&lt; 30);
	if (fixed) {
<blue>		counters = pmu->fixed_counters;</blue>
		num_counters = pmu-&gt;nr_arch_fixed_counters;
	} else {
<blue>		counters = pmu->gp_counters;</blue>
		num_counters = pmu-&gt;nr_arch_gp_counters;
	}
<blue>	if (idx >= num_counters)</blue>
		return NULL;
<yellow>	*mask &= pmu->counter_bitmask[fixed ? KVM_PMC_FIXED : KVM_PMC_GP];</yellow>
	return &amp;counters[array_index_nospec(idx, num_counters)];
<blue>}</blue>

static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
{
<blue>	if (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))</blue>
		return 0;

<blue>	return vcpu->arch.perf_capabilities;</blue>
}

static inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)
{
<blue>	return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;</blue>
}

static inline struct kvm_pmc *get_fw_gp_pmc(struct kvm_pmu *pmu, u32 msr)
{
<blue>	if (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))</blue>
		return NULL;

<blue>	return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);</blue>
}

static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
{
	struct x86_pmu_lbr *records = vcpu_to_lbr_records(vcpu);
	bool ret = false;

<blue>	if (!intel_pmu_lbr_is_enabled(vcpu))</blue>
		return ret;

	ret = (index == MSR_LBR_SELECT) || (index == MSR_LBR_TOS) ||
<yellow>		(index >= records->from && index < records->from + records->nr) ||</yellow>
<yellow>		(index >= records->to && index < records->to + records->nr);</yellow>

<yellow>	if (!ret && records->info)</yellow>
<yellow>		ret = (index >= records->info && index < records->info + records->nr);</yellow>

	return ret;
<blue>}</blue>

static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	u64 perf_capabilities;
	int ret;

<blue>	switch (msr) {</blue>
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
	case MSR_CORE_PERF_GLOBAL_STATUS:
	case MSR_CORE_PERF_GLOBAL_CTRL:
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		return intel_pmu_has_perf_global_ctrl(pmu);</blue>
		break;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		ret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;</yellow>
		break;
	case MSR_IA32_DS_AREA:
<yellow>		ret = guest_cpuid_has(vcpu, X86_FEATURE_DS);</yellow>
		break;
	case MSR_PEBS_DATA_CFG:
<yellow>		perf_capabilities = vcpu_get_perf_capabilities(vcpu);</yellow>
		ret = (perf_capabilities &amp; PERF_CAP_PEBS_BASELINE) &amp;&amp;
<yellow>			((perf_capabilities & PERF_CAP_PEBS_FORMAT) > 3);</yellow>
		break;
	default:
<blue>		ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||</blue>
<blue>			get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||</blue>
<blue>			get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||</blue>
<blue>			intel_pmu_is_valid_lbr_msr(vcpu, msr);</blue>
		break;
	}

	return ret;
<blue>}</blue>

<blue>static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)</blue>
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;

<blue>	pmc = get_fixed_pmc(pmu, msr);</blue>
<blue>	pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);</blue>
<blue>	pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);</blue>

	return pmc;
<blue>}</blue>

static inline void intel_pmu_release_guest_lbr_event(struct kvm_vcpu *vcpu)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (lbr_desc->event) {</yellow>
<yellow>		perf_event_release_kernel(lbr_desc->event);</yellow>
		lbr_desc-&gt;event = NULL;
		vcpu_to_pmu(vcpu)-&gt;event_count--;
	}
}

int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)
<yellow>{</yellow>
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct perf_event *event;

	/*
	 * The perf_event_attr is constructed in the minimum efficient way:
	 * - set &#x27;pinned = true&#x27; to make it task pinned so that if another
	 *   cpu pinned event reclaims LBR, the event-&gt;oncpu will be set to -1;
	 * - set &#x27;.exclude_host = true&#x27; to record guest branches behavior;
	 *
	 * - set &#x27;.config = INTEL_FIXED_VLBR_EVENT&#x27; to indicates host perf
	 *   schedule the event without a real HW counter but a fake one;
	 *   check is_guest_lbr_event() and __intel_get_event_constraints();
	 *
	 * - set &#x27;sample_type = PERF_SAMPLE_BRANCH_STACK&#x27; and
	 *   &#x27;branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |
	 *   PERF_SAMPLE_BRANCH_USER&#x27; to configure it as a LBR callstack
	 *   event, which helps KVM to save/restore guest LBR records
	 *   during host context switches and reduces quite a lot overhead,
	 *   check branch_user_callstack() and intel_pmu_lbr_sched_task();
	 */
<yellow>	struct perf_event_attr attr = {</yellow>
		.type = PERF_TYPE_RAW,
		.size = sizeof(attr),
		.config = INTEL_FIXED_VLBR_EVENT,
		.sample_type = PERF_SAMPLE_BRANCH_STACK,
		.pinned = true,
		.exclude_host = true,
		.branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |
					PERF_SAMPLE_BRANCH_USER,
	};

	if (unlikely(lbr_desc-&gt;event)) {
<yellow>		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);</yellow>
		return 0;
	}

<yellow>	event = perf_event_create_kernel_counter(&attr, -1,</yellow>
						current, NULL, NULL);
	if (IS_ERR(event)) {
<yellow>		pr_debug_ratelimited("%s: failed %ld\n",</yellow>
					__func__, PTR_ERR(event));
<yellow>		return PTR_ERR(event);</yellow>
	}
<yellow>	lbr_desc->event = event;</yellow>
	pmu-&gt;event_count++;
	__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu-&gt;pmc_in_use);
	return 0;
}

/*
 * It&#x27;s safe to access LBR msrs from guest when they have not
 * been passthrough since the host would help restore or reset
 * the LBR msrs records when the guest LBR event is scheduled in.
 */
static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
				     struct msr_data *msr_info, bool read)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
<yellow>	u32 index = msr_info->index;</yellow>

	if (!intel_pmu_is_valid_lbr_msr(vcpu, index))
		return false;

<yellow>	if (!lbr_desc->event && intel_pmu_create_guest_lbr_event(vcpu) < 0)</yellow>
		goto dummy;

	/*
	 * Disable irq to ensure the LBR feature doesn&#x27;t get reclaimed by the
	 * host at the time the value is read from the msr, and this avoids the
	 * host LBR value to be leaked to the guest. If LBR has been reclaimed,
	 * return 0 on guest reads.
	 */
<yellow>	local_irq_disable();</yellow>
<yellow>	if (lbr_desc->event->state == PERF_EVENT_STATE_ACTIVE) {</yellow>
<yellow>		if (read)</yellow>
<yellow>			rdmsrl(index, msr_info->data);</yellow>
		else
<yellow>			wrmsrl(index, msr_info->data);</yellow>
<yellow>		__set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);</yellow>
		local_irq_enable();
		return true;
	}
<yellow>	clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);</yellow>
	local_irq_enable();

dummy:
<yellow>	if (read)</yellow>
<yellow>		msr_info->data = 0;</yellow>
	return true;
<yellow>}</yellow>

static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc;
<blue>	u32 msr = msr_info->index;</blue>

	switch (msr) {
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
<blue>		msr_info->data = pmu->fixed_ctr_ctrl;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_STATUS:
<blue>		msr_info->data = pmu->global_status;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		msr_info->data = pmu->global_ctrl;</blue>
		return 0;
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		msr_info->data = 0;</blue>
		return 0;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		msr_info->data = pmu->pebs_enable;</yellow>
		return 0;
	case MSR_IA32_DS_AREA:
<yellow>		msr_info->data = pmu->ds_area;</yellow>
		return 0;
	case MSR_PEBS_DATA_CFG:
<yellow>		msr_info->data = pmu->pebs_data_cfg;</yellow>
		return 0;
	default:
<blue>		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||</blue>
<blue>		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {</blue>
<blue>			u64 val = pmc_read_counter(pmc);</blue>
			msr_info-&gt;data =
				val &amp; pmu-&gt;counter_bitmask[KVM_PMC_GP];
			return 0;
<blue>		} else if ((pmc = get_fixed_pmc(pmu, msr))) {</blue>
<blue>			u64 val = pmc_read_counter(pmc);</blue>
			msr_info-&gt;data =
				val &amp; pmu-&gt;counter_bitmask[KVM_PMC_FIXED];
			return 0;
<blue>		} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {</blue>
<blue>			msr_info->data = pmc->eventsel;</blue>
			return 0;
<yellow>		} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, true))</yellow>
			return 0;
	}

	return 1;
}

static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
<blue>{</blue>
<blue>	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);</blue>
	struct kvm_pmc *pmc;
	u32 msr = msr_info-&gt;index;
	u64 data = msr_info-&gt;data;
	u64 reserved_bits, diff;

	switch (msr) {
	case MSR_CORE_PERF_FIXED_CTR_CTRL:
<blue>		if (pmu->fixed_ctr_ctrl == data)</blue>
			return 0;
<yellow>		if (!(data & pmu->fixed_ctr_ctrl_mask)) {</yellow>
<yellow>			reprogram_fixed_counters(pmu, data);</yellow>
			return 0;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_STATUS:
<blue>		if (msr_info->host_initiated) {</blue>
<blue>			pmu->global_status = data;</blue>
			return 0;
		}
		break; /* RO MSR */
	case MSR_CORE_PERF_GLOBAL_CTRL:
<blue>		if (pmu->global_ctrl == data)</blue>
			return 0;
<yellow>		if (kvm_valid_perf_global_ctrl(pmu, data)) {</yellow>
			diff = pmu-&gt;global_ctrl ^ data;
<yellow>			pmu->global_ctrl = data;</yellow>
			reprogram_counters(pmu, diff);
			return 0;
		}
		break;
	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
<blue>		if (!(data & pmu->global_ovf_ctrl_mask)) {</blue>
<blue>			if (!msr_info->host_initiated)</blue>
<yellow>				pmu->global_status &= ~data;</yellow>
			return 0;
		}
		break;
	case MSR_IA32_PEBS_ENABLE:
<yellow>		if (pmu->pebs_enable == data)</yellow>
			return 0;
<yellow>		if (!(data & pmu->pebs_enable_mask)) {</yellow>
			diff = pmu-&gt;pebs_enable ^ data;
<yellow>			pmu->pebs_enable = data;</yellow>
			reprogram_counters(pmu, diff);
			return 0;
		}
		break;
	case MSR_IA32_DS_AREA:
<yellow>		if (msr_info->host_initiated && data && !guest_cpuid_has(vcpu, X86_FEATURE_DS))</yellow>
			return 1;
<yellow>		if (is_noncanonical_address(data, vcpu))</yellow>
			return 1;
<yellow>		pmu->ds_area = data;</yellow>
		return 0;
	case MSR_PEBS_DATA_CFG:
<yellow>		if (pmu->pebs_data_cfg == data)</yellow>
			return 0;
<yellow>		if (!(data & pmu->pebs_data_cfg_mask)) {</yellow>
			pmu-&gt;pebs_data_cfg = data;
<yellow>			return 0;</yellow>
		}
		break;
	default:
<blue>		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||</blue>
<blue>		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {</blue>
<blue>			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&</blue>
<yellow>			    (data & ~pmu->counter_bitmask[KVM_PMC_GP]))</yellow>
				return 1;
<blue>			if (!msr_info->host_initiated &&</blue>
			    !(msr &amp; MSR_PMC_FULL_WIDTH_BIT))
<yellow>				data = (s64)(s32)data;</yellow>
<blue>			pmc->counter += data - pmc_read_counter(pmc);</blue>
<yellow>			pmc_update_sample_period(pmc);</yellow>
			return 0;
<blue>		} else if ((pmc = get_fixed_pmc(pmu, msr))) {</blue>
<blue>			pmc->counter += data - pmc_read_counter(pmc);</blue>
<yellow>			pmc_update_sample_period(pmc);</yellow>
			return 0;
<blue>		} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {</blue>
<blue>			if (data == pmc->eventsel)</blue>
				return 0;
<yellow>			reserved_bits = pmu->reserved_bits;</yellow>
			if ((pmc-&gt;idx == 2) &amp;&amp;
<yellow>			    (pmu->raw_event_mask & HSW_IN_TX_CHECKPOINTED))</yellow>
<yellow>				reserved_bits ^= HSW_IN_TX_CHECKPOINTED;</yellow>
<yellow>			if (!(data & reserved_bits)) {</yellow>
<yellow>				pmc->eventsel = data;</yellow>
				reprogram_counter(pmc);
				return 0;
			}
<yellow>		} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, false))</yellow>
			return 0;
	}

	return 1;
}

static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
{
	size_t size = ARRAY_SIZE(fixed_pmc_events);
	struct kvm_pmc *pmc;
	u32 event;
	int i;

	for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++) {
<blue>		pmc = &pmu->fixed_counters[i];</blue>
		event = fixed_pmc_events[array_index_nospec(i, size)];
		pmc-&gt;eventsel = (intel_arch_events[event].unit_mask &lt;&lt; 8) |
			intel_arch_events[event].eventsel;
	}
}

static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
	struct kvm_cpuid_entry2 *entry;
	union cpuid10_eax eax;
	union cpuid10_edx edx;
	u64 perf_capabilities;
	u64 counter_mask;
	int i;

<blue>	pmu->nr_arch_gp_counters = 0;</blue>
	pmu-&gt;nr_arch_fixed_counters = 0;
	pmu-&gt;counter_bitmask[KVM_PMC_GP] = 0;
	pmu-&gt;counter_bitmask[KVM_PMC_FIXED] = 0;
	pmu-&gt;version = 0;
	pmu-&gt;reserved_bits = 0xffffffff00200000ull;
	pmu-&gt;raw_event_mask = X86_RAW_EVENT_MASK;
	pmu-&gt;global_ctrl_mask = ~0ull;
	pmu-&gt;global_ovf_ctrl_mask = ~0ull;
	pmu-&gt;fixed_ctr_ctrl_mask = ~0ull;
	pmu-&gt;pebs_enable_mask = ~0ull;
	pmu-&gt;pebs_data_cfg_mask = ~0ull;

	entry = kvm_find_cpuid_entry(vcpu, 0xa);
<blue>	if (!entry || !vcpu->kvm->arch.enable_pmu)</blue>
		return;
<blue>	eax.full = entry->eax;</blue>
	edx.full = entry-&gt;edx;

	pmu-&gt;version = eax.split.version_id;
	if (!pmu-&gt;version)
		return;

<blue>	pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters,</blue>
					 kvm_pmu_cap.num_counters_gp);
	eax.split.bit_width = min_t(int, eax.split.bit_width,
				    kvm_pmu_cap.bit_width_gp);
<blue>	pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;</blue>
	eax.split.mask_length = min_t(int, eax.split.mask_length,
				      kvm_pmu_cap.events_mask_len);
<blue>	pmu->available_event_types = ~entry->ebx &</blue>
					((1ull &lt;&lt; eax.split.mask_length) - 1);

	if (pmu-&gt;version == 1) {
<yellow>		pmu->nr_arch_fixed_counters = 0;</yellow>
	} else {
		pmu-&gt;nr_arch_fixed_counters =
<blue>			min3(ARRAY_SIZE(fixed_pmc_events),</blue>
			     (size_t) edx.split.num_counters_fixed,
			     (size_t)kvm_pmu_cap.num_counters_fixed);
		edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed,
						  kvm_pmu_cap.bit_width_fixed);
		pmu-&gt;counter_bitmask[KVM_PMC_FIXED] =
<blue>			((u64)1 << edx.split.bit_width_fixed) - 1;</blue>
<blue>		setup_fixed_pmc_eventsel(pmu);</blue>
	}

	for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++)
<blue>		pmu->fixed_ctr_ctrl_mask &= ~(0xbull << (i * 4));</blue>
<blue>	counter_mask = ~(((1ull << pmu->nr_arch_gp_counters) - 1) |</blue>
<blue>		(((1ull << pmu->nr_arch_fixed_counters) - 1) << INTEL_PMC_IDX_FIXED));</blue>
	pmu-&gt;global_ctrl_mask = counter_mask;
	pmu-&gt;global_ovf_ctrl_mask = pmu-&gt;global_ctrl_mask
			&amp; ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF |
			    MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
	if (vmx_pt_mode_is_host_guest())
<yellow>		pmu->global_ovf_ctrl_mask &=</yellow>
				~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;

<blue>	entry = kvm_find_cpuid_entry_index(vcpu, 7, 0);</blue>
	if (entry &amp;&amp;
<blue>	    (boot_cpu_has(X86_FEATURE_HLE) || boot_cpu_has(X86_FEATURE_RTM)) &&</blue>
<yellow>	    (entry->ebx & (X86_FEATURE_HLE|X86_FEATURE_RTM))) {</yellow>
<yellow>		pmu->reserved_bits ^= HSW_IN_TX;</yellow>
		pmu-&gt;raw_event_mask |= (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED);
	}

<blue>	bitmap_set(pmu->all_valid_pmc_idx,</blue>
		0, pmu-&gt;nr_arch_gp_counters);
	bitmap_set(pmu-&gt;all_valid_pmc_idx,
		INTEL_PMC_MAX_GENERIC, pmu-&gt;nr_arch_fixed_counters);

<blue>	perf_capabilities = vcpu_get_perf_capabilities(vcpu);</blue>
<blue>	if (cpuid_model_is_consistent(vcpu) &&</blue>
<blue>	    (perf_capabilities & PMU_CAP_LBR_FMT))</blue>
<blue>		x86_perf_get_lbr(&lbr_desc->records);</blue>
	else
<blue>		lbr_desc->records.nr = 0;</blue>

	if (lbr_desc-&gt;records.nr)
<blue>		bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);</blue>

<blue>	if (perf_capabilities & PERF_CAP_PEBS_FORMAT) {</blue>
<yellow>		if (perf_capabilities & PERF_CAP_PEBS_BASELINE) {</yellow>
<yellow>			pmu->pebs_enable_mask = counter_mask;</yellow>
			pmu-&gt;reserved_bits &amp;= ~ICL_EVENTSEL_ADAPTIVE;
			for (i = 0; i &lt; pmu-&gt;nr_arch_fixed_counters; i++) {
<yellow>				pmu->fixed_ctr_ctrl_mask &=</yellow>
<yellow>					~(1ULL << (INTEL_PMC_IDX_FIXED + i * 4));</yellow>
			}
<yellow>			pmu->pebs_data_cfg_mask = ~0xff00000full;</yellow>
		} else {
			pmu-&gt;pebs_enable_mask =
<yellow>				~((1ull << pmu->nr_arch_gp_counters) - 1);</yellow>
		}
	}
<blue>}</blue>

static void intel_pmu_init(struct kvm_vcpu *vcpu)
{
	int i;
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

	for (i = 0; i &lt; KVM_INTEL_PMC_MAX_GENERIC; i++) {
<blue>		pmu->gp_counters[i].type = KVM_PMC_GP;</blue>
		pmu-&gt;gp_counters[i].vcpu = vcpu;
		pmu-&gt;gp_counters[i].idx = i;
		pmu-&gt;gp_counters[i].current_config = 0;
	}

	for (i = 0; i &lt; KVM_PMC_MAX_FIXED; i++) {
<blue>		pmu->fixed_counters[i].type = KVM_PMC_FIXED;</blue>
		pmu-&gt;fixed_counters[i].vcpu = vcpu;
		pmu-&gt;fixed_counters[i].idx = i + INTEL_PMC_IDX_FIXED;
		pmu-&gt;fixed_counters[i].current_config = 0;
	}

<blue>	vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();</blue>
	lbr_desc-&gt;records.nr = 0;
	lbr_desc-&gt;event = NULL;
	lbr_desc-&gt;msr_passthrough = false;
}

static void intel_pmu_reset(struct kvm_vcpu *vcpu)
<blue>{</blue>
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct kvm_pmc *pmc = NULL;
	int i;

	for (i = 0; i &lt; KVM_INTEL_PMC_MAX_GENERIC; i++) {
<blue>		pmc = &pmu->gp_counters[i];</blue>

<yellow>		pmc_stop_counter(pmc);</yellow>
<blue>		pmc->counter = pmc->eventsel = 0;</blue>
	}

	for (i = 0; i &lt; KVM_PMC_MAX_FIXED; i++) {
<blue>		pmc = &pmu->fixed_counters[i];</blue>

<yellow>		pmc_stop_counter(pmc);</yellow>
<blue>		pmc->counter = 0;</blue>
	}

<blue>	pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;</blue>

<yellow>	intel_pmu_release_guest_lbr_event(vcpu);</yellow>
}

/*
 * Emulate LBR_On_PMI behavior for 1 &lt; pmu.version &lt; 4.
 *
 * If Freeze_LBR_On_PMI = 1, the LBR is frozen on PMI and
 * the KVM emulates to clear the LBR bit (bit 0) in IA32_DEBUGCTL.
 *
 * Guest needs to re-enable LBR to resume branches recording.
 */
static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
{
<yellow>	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);</yellow>

<yellow>	if (data & DEBUGCTLMSR_FREEZE_LBRS_ON_PMI) {</yellow>
<yellow>		data &= ~DEBUGCTLMSR_LBR;</yellow>
<yellow>		vmcs_write64(GUEST_IA32_DEBUGCTL, data);</yellow>
	}
}

static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
{
<yellow>	u8 version = vcpu_to_pmu(vcpu)->version;</yellow>

	if (!intel_pmu_lbr_is_enabled(vcpu))
		return;

<yellow>	if (version > 1 && version < 4)</yellow>
<yellow>		intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);</yellow>
<yellow>}</yellow>

static void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)
{
	struct x86_pmu_lbr *lbr = vcpu_to_lbr_records(vcpu);
	int i;

<yellow>	for (i = 0; i < lbr->nr; i++) {</yellow>
<yellow>		vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);</yellow>
		vmx_set_intercept_for_msr(vcpu, lbr-&gt;to + i, MSR_TYPE_RW, set);
		if (lbr-&gt;info)
<yellow>			vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);</yellow>
	}

<yellow>	vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);</yellow>
	vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
<yellow>}</yellow>

<yellow>static inline void vmx_disable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)</yellow>
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (!lbr_desc->msr_passthrough)</yellow>
		return;

<yellow>	vmx_update_intercept_for_lbr_msrs(vcpu, true);</yellow>
	lbr_desc-&gt;msr_passthrough = false;
}

static inline void vmx_enable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)
{
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (lbr_desc->msr_passthrough)</yellow>
		return;

<yellow>	vmx_update_intercept_for_lbr_msrs(vcpu, false);</yellow>
	lbr_desc-&gt;msr_passthrough = true;
}

/*
 * Higher priority host perf events (e.g. cpu pinned) could reclaim the
 * pmu resources (e.g. LBR) that were assigned to the guest. This is
 * usually done via ipi calls (more details in perf_install_in_context).
 *
 * Before entering the non-root mode (with irq disabled here), double
 * confirm that the pmu features enabled to the guest are not reclaimed
 * by higher priority host events. Otherwise, disallow vcpu&#x27;s access to
 * the reclaimed features.
 */
void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
{
	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);

<yellow>	if (!lbr_desc->event) {</yellow>
<yellow>		vmx_disable_lbr_msrs_passthrough(vcpu);</yellow>
<yellow>		if (vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR)</yellow>
			goto warn;
<yellow>		if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))</yellow>
			goto warn;
		return;
	}

<yellow>	if (lbr_desc->event->state < PERF_EVENT_STATE_ACTIVE) {</yellow>
<yellow>		vmx_disable_lbr_msrs_passthrough(vcpu);</yellow>
<yellow>		__clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);</yellow>
		goto warn;
	} else
<yellow>		vmx_enable_lbr_msrs_passthrough(vcpu);</yellow>

	return;

warn:
<yellow>	pr_warn_ratelimited("kvm: vcpu-%d: fail to passthrough LBR.\n",</yellow>
		vcpu-&gt;vcpu_id);
<yellow>}</yellow>

static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
{
<yellow>	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))</yellow>
<yellow>		intel_pmu_release_guest_lbr_event(vcpu);</yellow>
<yellow>}</yellow>

void intel_pmu_cross_mapped_check(struct kvm_pmu *pmu)
{
	struct kvm_pmc *pmc = NULL;
	int bit, hw_idx;

<yellow>	for_each_set_bit(bit, (unsigned long *)&pmu->global_ctrl,</yellow>
			 X86_PMC_IDX_MAX) {
<yellow>		pmc = intel_pmc_idx_to_pmc(pmu, bit);</yellow>

<yellow>		if (!pmc || !pmc_speculative_in_use(pmc) ||</yellow>
<yellow>		    !intel_pmc_is_enabled(pmc) || !pmc->perf_event)</yellow>
			continue;

		/*
		 * A negative index indicates the event isn&#x27;t mapped to a
		 * physical counter in the host, e.g. due to contention.
		 */
<yellow>		hw_idx = pmc->perf_event->hw.idx;</yellow>
<yellow>		if (hw_idx != pmc->idx && hw_idx > -1)</yellow>
<yellow>			pmu->host_cross_mapped_mask |= BIT_ULL(hw_idx);</yellow>
	}
<yellow>}</yellow>

struct kvm_pmu_ops intel_pmu_ops __initdata = {
	.hw_event_available = intel_hw_event_available,
	.pmc_is_enabled = intel_pmc_is_enabled,
	.pmc_idx_to_pmc = intel_pmc_idx_to_pmc,
	.rdpmc_ecx_to_pmc = intel_rdpmc_ecx_to_pmc,
	.msr_idx_to_pmc = intel_msr_idx_to_pmc,
	.is_valid_rdpmc_ecx = intel_is_valid_rdpmc_ecx,
	.is_valid_msr = intel_is_valid_msr,
	.get_msr = intel_pmu_get_msr,
	.set_msr = intel_pmu_set_msr,
	.refresh = intel_pmu_refresh,
	.init = intel_pmu_init,
	.reset = intel_pmu_reset,
	.deliver_pmi = intel_pmu_deliver_pmi,
	.cleanup = intel_pmu_cleanup,
};


</code></pre></td></tr></table>
</div><script>const fileList = document.getElementById('file_list')
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/mmu/mmu.c.html">mmu.c 58.8%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/nested.c.html">nested.c 80.9%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/vmx/vmx.c.html">vmx.c 58.2%</li>`
fileList.innerHTML+=`<li><a href="/kvm_coverage/coverage/linux/arch/x86/kvm/x86.c.html">x86.c 50.7%</li>`
</script></body></html>